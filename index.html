<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="es" xml:lang="es">
<head>
<!-- 2026-01-20 mar 12:01 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Redes Neuronales Artificiales</title>
<meta name="author" content="Eduardo Alcaraz" />
<meta name="generator" content="Org Mode" />
<style type="text/css">
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="src/readtheorg_theme/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="src/readtheorg_theme/css/readtheorg.css"/>
<script type="text/javascript" src="src/lib/js/jquery.min.js"></script>
<script type="text/javascript" src="src/lib/js/bootstrap.min.js"></script>
<script type="text/javascript" src="src/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="src/readtheorg_theme/js/readtheorg.js"></script>
<style> #content{max-width:1800px;}</style>
<style>pre.src {background-color: #303030; color: #e5e5e5;}</style>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Redes Neuronales Artificiales</h1>
<div id="table-of-contents" role="doc-toc">
<h2>&Iacute;ndice</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org7015bef">Presentación del Curso</a>
<ul>
<li><a href="#org55bcfc6">Objetivo general</a></li>
</ul>
</li>
<li><a href="#orgd7913fc">Instalación de requerimientos</a>
<ul>
<li><a href="#org2908672">Verificación de Python</a></li>
<li><a href="#orgd19dad3">Creación del entorno virtual</a></li>
<li><a href="#org59856e4">Activación del entorno virtual</a>
<ul>
<li><a href="#org8d63460">GNU/Linux y macOS</a></li>
<li><a href="#orgd83f74e">Windows – Símbolo del sistema (CMD)</a></li>
<li><a href="#org930287a">Windows – PowerShell</a></li>
<li><a href="#org6a8d374">Windows – Git Bash</a></li>
</ul>
</li>
<li><a href="#org33d747b">Actualización del gestor de paquetes</a></li>
<li><a href="#org6585e96">Instalación de dependencias</a></li>
<li><a href="#org937286a">Verificación de la instalación</a></li>
<li><a href="#org5b8a8b6">Desactivación del entorno virtual</a></li>
</ul>
</li>
<li><a href="#org60db9a4">Manual de Entornos Virtuales en Python</a>
<ul>
<li><a href="#orga33fe24">Introducción</a></li>
<li><a href="#orge7daa4a">Flujo de Trabajo Básico</a>
<ul>
<li><a href="#org9c59411">1. Creación del entorno</a></li>
<li><a href="#orge6288f5">2. Activación</a></li>
</ul>
</li>
<li><a href="#org35a2d0a">Gestión de paquetes</a></li>
<li><a href="#org0d2e26d">El archivo de Requerimientos</a></li>
<li><a href="#org59b2408">Exportar dependencias</a></li>
<li><a href="#org6210df1">Instalar desde el archivo</a></li>
<li><a href="#org1fe0104">Tips de Limpieza</a></li>
<li><a href="#org905da8c">Entornos Virtuales Python (Edición Windows)</a></li>
<li><a href="#orgcbacff7">Requisitos Previos</a></li>
<li><a href="#org6df5084">Flujo de Trabajo en Windows</a>
<ul>
<li><a href="#org1ecaf66">1. Crear el Entorno Virtual</a></li>
<li><a href="#org2d2fd9a">2. El Paso Crítico: La Activación</a></li>
<li><a href="#orgb732b51">3. Confirmación</a></li>
</ul>
</li>
<li><a href="#orgd972244">Gestión de Librerías con PIP</a>
<ul>
<li><a href="#org263d455">Instalación de paquetes</a></li>
<li><a href="#org9c377df">Congelar dependencias (Compartir proyecto)</a></li>
<li><a href="#orgd16c0b9">Instalar desde un archivo recibido</a></li>
</ul>
</li>
<li><a href="#org2d128bc">Uso de Entornos en Emacs (Windows)</a>
<ul>
<li><a href="#org59b2eff">Instalación del paquete pyvenv</a></li>
<li><a href="#orgb30adea">Cómo activarlo dentro de Emacs</a></li>
</ul>
</li>
<li><a href="#org7867558">Solución de Problemas Comunes en Windows</a></li>
<li><a href="#orgdffce91">Desactivación y Limpieza</a></li>
<li><a href="#org8c3ad50">Agregar a jupyter notebook</a></li>
</ul>
</li>
<li><a href="#org9e59490">Introducción a la Inteligencia Artificial (IA)</a>
<ul>
<li><a href="#orge8abd65">Definición formal</a></li>
<li><a href="#orgc0d06be">IA débil vs IA fuerte</a></li>
</ul>
</li>
<li><a href="#org2ec9534">Machine Learning (Aprendizaje Automático)</a>
<ul>
<li><a href="#org538a1c5">Tipos de aprendizaje</a></li>
</ul>
</li>
<li><a href="#org1e9a825">Historia de las Redes Neuronales</a>
<ul>
<li><a href="#org41a039a">Contexto histórico</a></li>
<li><a href="#orgb7d7519">McCulloch y Pitts (1943)</a></li>
<li><a href="#orgc88900d">Perceptrón y optimismo inicial</a></li>
<li><a href="#org6abfff8">Críticas y estancamiento</a></li>
<li><a href="#orge9f6364">Renacimiento moderno</a></li>
</ul>
</li>
<li><a href="#orge1d3add">La Neurona Artificial</a>
<ul>
<li><a href="#orgce9ef15">Motivación</a></li>
<li><a href="#org735c33c">Componentes</a></li>
<li><a href="#org76dccc7">Modelo matemático</a></li>
</ul>
</li>
<li><a href="#org25a374b">El Perceptrón</a>
<ul>
<li><a href="#org4efaf09">Definición</a></li>
<li><a href="#orgdb2daa7">Interpretación geométrica</a></li>
<li><a href="#org07749df">Aprendizaje</a></li>
<li><a href="#org2e136e0">Limitaciones</a></li>
</ul>
</li>
<li><a href="#orgd2b29db">¿Qué es un problema linealmente separable?</a>
<ul>
<li><a href="#org1f2a355">Intuición básica</a></li>
</ul>
</li>
<li><a href="#orgcdb5810">Estructura de un Perceptrón Simple</a>
<ul>
<li><a href="#org19a7b23">Entradas (Inputs)</a></li>
<li><a href="#org95535b6">Pesos Sinápticos (Weights)</a></li>
<li><a href="#org5ba5f1e">Suma Ponderada (Weighted Sum)</a></li>
<li><a href="#org07f09c8">Sesgo (Bias)</a></li>
<li><a href="#org798af64">Función de Activación</a></li>
<li><a href="#orgb4270fd">Salida (Output)</a></li>
</ul>
</li>
<li><a href="#org28e2f60">Ejemplo Tacos</a>
<ul>
<li><a href="#org1b5db83">1. El Escenario: La Decisión de la Cena</a>
<ul>
<li><a href="#org99e45c3">Nuestras Entradas (Inputs)</a></li>
<li><a href="#org011770f">El Peso (Weight): La Importancia</a></li>
</ul>
</li>
<li><a href="#org439258c">2. ¿Cómo toma la decisión? (La Suma y el Sesgo)</a>
<ul>
<li><a href="#orgc337946">El Sesgo (Bias)</a></li>
</ul>
</li>
<li><a href="#orgcf36728">3. El Descenso del Gradiente: ¿Cómo "Aprende" la Neurona?</a>
<ul>
<li><a href="#org0263ebd">El Error: La Brújula</a></li>
<li><a href="#orgdd207dc">El Ajuste (La Regla Delta)</a></li>
</ul>
</li>
<li><a href="#org452c07a">Perceptrón ejemplo (Python)</a>
<ul>
<li><a href="#org1cbff08">Perceptrón para una compuerta "AND"</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgc08eba2">Codigo C python</a></li>
<li><a href="#org87de320">Redes Neuronales Multicapa (Multilayer Perceptron)</a>
<ul>
<li><a href="#orgf04393b">Introducción</a></li>
<li><a href="#orgfa9d43f">Modelo matemático</a>
<ul>
<li><a href="#orgd9d565a">Neurona individual</a></li>
<li><a href="#org3abb12f">Forma matricial de una capa</a></li>
</ul>
</li>
<li><a href="#org670f5f4">Funciones de activación</a>
<ul>
<li><a href="#org8ecb0d2">¿Para qué sirven las funciones de activación?</a></li>
</ul>
</li>
<li><a href="#orgae9e0eb">Clasificación de las funciones de activación</a>
<ul>
<li><a href="#orgbf074db">Funciones de activación clásicas</a></li>
<li><a href="#orgaf0135f">Funciones de activación modernas</a></li>
<li><a href="#orgff27281">Funciones de activación en la capa de salida</a></li>
<li><a href="#org6f63e25">Impacto en el entrenamiento</a></li>
</ul>
</li>
<li><a href="#orgf0b7f33">Propagación hacia adelante (Forward Propagation)</a></li>
<li><a href="#orge935b49">Funciones de pérdida</a>
<ul>
<li><a href="#org7a0dcbd">Error cuadrático medio (MSE)</a></li>
<li><a href="#org444ae6c">Entropía cruzada (Cross-Entropy)</a></li>
</ul>
</li>
<li><a href="#org8475fcb">Backpropagation</a></li>
<li><a href="#orgcfa3523">Descenso de gradiente</a></li>
</ul>
</li>
<li><a href="#orga982b53">Ejemplo Clasificación del dataset Iris con MLP</a>
<ul>
<li><a href="#org9025bea">Reproducibilidad del experimento</a></li>
<li><a href="#orga02ff76">Estratificación de clases</a></li>
<li><a href="#orgea407f1">Importancia en redes neuronales</a></li>
<li><a href="#org9e61ba4">Normalización de los datos</a></li>
<li><a href="#org6433828">Definir el Perceptrón Multicapa (MLP)</a>
<ul>
<li><a href="#org3730d02">Arquitectura de la red neuronal</a></li>
<li><a href="#orgb5fb5e7">Capa de entrada</a></li>
<li><a href="#org63eaffa">Capas ocultas</a></li>
<li><a href="#org22e573d">Función de activación</a></li>
<li><a href="#org0e249b3">Capa de salida</a></li>
<li><a href="#org828ab5a">Entrenamiento del modelo</a></li>
</ul>
</li>
<li><a href="#orge103146">Número de iteraciones</a></li>
<li><a href="#orge20e603">Reproducibilidad</a></li>
<li><a href="#org6e589ec">Entrenamiento del modelo</a>
<ul>
<li><a href="#org7fec327">Propagación hacia adelante (forward pass)</a></li>
<li><a href="#org28b16b6">Cálculo del error</a></li>
<li><a href="#orgcec415a">Backpropagation</a></li>
<li><a href="#org0baa450">Descenso de gradiente con Adam</a></li>
</ul>
</li>
<li><a href="#org121ba4e">Evaluación del modelo</a>
<ul>
<li><a href="#org43e7442">Métricas utilizadas</a></li>
</ul>
</li>
<li><a href="#org7842ef7">Predicción con una nueva flor</a></li>
<li><a href="#org6c53864">Interpretación matemática del MLP</a></li>
</ul>
</li>
<li><a href="#org301fabc">Ejemplo Clasificación de Cáncer de Mama con MLP</a>
<ul>
<li><a href="#org5462d3d">Importar librerías</a></li>
<li><a href="#org807d152">Cargar el dataset Breast Cancer</a>
<ul>
<li><a href="#org01fd31f">Importancia del dominio</a></li>
</ul>
</li>
<li><a href="#orgcb899b9">Partición del conjunto de datos</a>
<ul>
<li><a href="#orgfd2a90d">Significado de los parámetros</a></li>
<li><a href="#orgf8e5d55">Propósito de la partición</a></li>
</ul>
</li>
<li><a href="#org387d5ce">Normalización (Estandarización)</a>
<ul>
<li><a href="#org9d1cac6">Importante</a></li>
</ul>
</li>
<li><a href="#org1aff302">Definición del Perceptrón Multicapa (MLP)</a>
<ul>
<li><a href="#org5d317ee">Arquitectura del modelo</a></li>
<li><a href="#org7269d85">Función de activación ReLU</a></li>
<li><a href="#org3d3fc36">Optimizador Adam</a></li>
<li><a href="#orgf9cbc31">Número máximo de iteraciones</a></li>
<li><a href="#org394ef21">Reproducibilidad</a></li>
</ul>
</li>
<li><a href="#orgb9a0145">Entrenamiento del modelo</a>
<ul>
<li><a href="#orgfaf18f7">Proceso interno (visión conceptual)</a></li>
</ul>
</li>
<li><a href="#org145d8c8">Evaluación del modelo</a>
<ul>
<li><a href="#org629df3e">Métricas principales</a></li>
</ul>
</li>
<li><a href="#orgf110795">Interpretación matemática de la salida binaria</a></li>
<li><a href="#org8cf5958">Predicción de una nueva muestra</a>
<ul>
<li><a href="#orgb7cbcbf">Comentarios importantes</a></li>
</ul>
</li>
<li><a href="#orgd775baa">Resumen conceptual</a></li>
</ul>
</li>
<li><a href="#org65b2102">Redes Neuronales LSTM (Long Short-Term Memory)</a>
<ul>
<li><a href="#orgedd8f1f">Estructura general de una RNN</a>
<ul>
<li><a href="#org56bc796">Problema: vanishing y exploding gradients</a></li>
</ul>
</li>
<li><a href="#org733e479">Motivación de las LSTM</a></li>
<li><a href="#orgea859ac">Arquitectura interna de una celda LSTM</a>
<ul>
<li><a href="#org9a47d58">Ecuaciones de la LSTM</a></li>
<li><a href="#org35be279">Interpretación de las puertas</a></li>
</ul>
</li>
<li><a href="#orgac392d3">Intuición conceptual</a></li>
<li><a href="#org4934aae">Entrenamiento de una LSTM</a></li>
<li><a href="#org6d42adf">Tipos de tareas típicas con LSTM</a></li>
<li><a href="#org25992bb">Variantes y extensiones de LSTM</a></li>
<li><a href="#org54b1845">Implementación básica en Python (Keras)</a></li>
<li><a href="#org04b2937">Ventajas y limitaciones de LSTM</a>
<ul>
<li><a href="#orgf7b42e6">Ventajas</a></li>
<li><a href="#org6ed38ed">Limitaciones</a></li>
</ul>
</li>
<li><a href="#orgb90b09c">Conclusión</a></li>
</ul>
</li>
<li><a href="#org24358f0">Ejemplo Clasificación del dataset Iris con una red LSTM</a>
<ul>
<li><a href="#org98267e0">Importar librerías</a></li>
<li><a href="#org2347246">Cargar el dataset Iris</a></li>
<li><a href="#orgb69b738">Partición entrenamiento / prueba</a>
<ul>
<li><a href="#org70ba70b">Propósito de la partición</a></li>
<li><a href="#org28a6d1e">Parámetro test<sub>size</sub></a></li>
<li><a href="#org4596a3f">Reproducibilidad del experimento</a></li>
<li><a href="#org371bdbe">Estratificación de clases</a></li>
</ul>
</li>
<li><a href="#orgea57be5">Normalización de los datos</a></li>
<li><a href="#orgbb3f71f">Preparar los datos para LSTM</a></li>
<li><a href="#orgc5f326b">Definir la red LSTM</a>
<ul>
<li><a href="#org8c6df01">Arquitectura de la red</a></li>
</ul>
</li>
<li><a href="#org633b9e9">Compilación del modelo</a></li>
<li><a href="#org5871aca">Entrenamiento del modelo LSTM</a>
<ul>
<li><a href="#orgf67a0b1">Propagación hacia adelante (forward pass)</a></li>
<li><a href="#org33f7fe5">Cálculo del error</a></li>
<li><a href="#org6f85fbd">Backpropagation Through Time (BPTT)</a></li>
<li><a href="#orgd5fb814">Descenso de gradiente con Adam</a></li>
</ul>
</li>
<li><a href="#orga44f5b8">Evaluación del modelo</a>
<ul>
<li><a href="#orgc6f9d34">Métricas utilizadas</a></li>
</ul>
</li>
<li><a href="#orgcadf24e">Predicción con una nueva flor</a></li>
<li><a href="#orgfcd1968">Interpretación matemática básica de la LSTM</a></li>
</ul>
</li>
<li><a href="#org8a82987">Bibliografía  Deep Learning</a>
<ul>
<li><a href="#org3c076f8">Teórica y Académica&#xa0;&#xa0;&#xa0;<span class="tag"><span class="MATEMATICAS">MATEMATICAS</span>&#xa0;<span class="FUNDAMENTOS">FUNDAMENTOS</span></span></a>
<ul>
<li><a href="#org1a1484a">Deep Learning</a></li>
<li><a href="#org654aaee">Neural Networks and Deep Learning: A Textbook</a></li>
</ul>
</li>
<li><a href="#org913b264">Práctica y Programación&#xa0;&#xa0;&#xa0;<span class="tag"><span class="PYTHON">PYTHON</span>&#xa0;<span class="PYTORCH">PYTORCH</span>&#xa0;<span class="KERAS">KERAS</span></span></a>
<ul>
<li><a href="#org8bb1bdb">Deep Learning with Python (3rd Edition)</a></li>
<li><a href="#org69f9d04">Hands-On Machine Learning with Scikit-Learn and PyTorch</a></li>
</ul>
</li>
<li><a href="#orgc8123ac">Aprendizaje "Desde Cero"&#xa0;&#xa0;&#xa0;<span class="tag"><span class="NUMPY">NUMPY</span>&#xa0;<span class="LOGICA">LOGICA</span></span></a>
<ul>
<li><a href="#org1eefb2e">Grokking Deep Learning</a></li>
<li><a href="#orgdae682c">Neural Networks from Scratch (NNFS)</a></li>
</ul>
</li>
<li><a href="#org590c047">Tabla Comparativa para Elección Rápida</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-org7015bef" class="outline-2">
<h2 id="org7015bef">Presentación del Curso</h2>
<div class="outline-text-2" id="text-org7015bef">
</div>
<div id="outline-container-org55bcfc6" class="outline-3">
<h3 id="org55bcfc6">Objetivo general</h3>
<div class="outline-text-3" id="text-org55bcfc6">
<p>
Este curso tiene como objetivo proporcionar una
comprensión <b>profunda y progresiva</b> de las redes neuronales
artificiales, desde sus fundamentos teóricos hasta las principales
arquitecturas modernas utilizadas en la industria y la
investigación.
</p>


<ul class="org-ul">
<li>Fundamentos matemáticos y conceptuales</li>
<li>Funcionamiento interno de una red neuronal</li>
<li>Entrenamiento y optimización</li>
<li>Arquitecturas principales</li>
<li>Intuición práctica y casos de uso</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgd7913fc" class="outline-2">
<h2 id="orgd7913fc">Instalación de requerimientos</h2>
<div class="outline-text-2" id="text-orgd7913fc">
</div>
<div id="outline-container-org2908672" class="outline-3">
<h3 id="org2908672">Verificación de Python</h3>
<div class="outline-text-3" id="text-org2908672">
<p>
Antes de instalar las dependencias, verifique que el sistema cuente con Python
versión 3.9 o superior.
</p>

<div class="org-src-container">
<pre class="src src-bash">python --version
</pre>
</div>

<p>
En caso de que el comando anterior no esté disponible, intente:
</p>

<div class="org-src-container">
<pre class="src src-bash">python3 --version
</pre>
</div>
</div>
</div>
<div id="outline-container-orgd19dad3" class="outline-3">
<h3 id="orgd19dad3">Creación del entorno virtual</h3>
<div class="outline-text-3" id="text-orgd19dad3">
<p>
Se recomienda crear un entorno virtual para aislar las dependencias del proyecto
y evitar conflictos entre versiones de librerías.
</p>

<div class="org-src-container">
<pre class="src src-bash">python -m venv mlp_env
</pre>
</div>
</div>
</div>
<div id="outline-container-org59856e4" class="outline-3">
<h3 id="org59856e4">Activación del entorno virtual</h3>
<div class="outline-text-3" id="text-org59856e4">
<p>
La activación del entorno virtual depende del sistema operativo y del intérprete
de comandos utilizado.
</p>
</div>
<div id="outline-container-org8d63460" class="outline-4">
<h4 id="org8d63460">GNU/Linux y macOS</h4>
<div class="outline-text-4" id="text-org8d63460">
<div class="org-src-container">
<pre class="src src-bash"><span style="color: #483d8b;">source</span> mlp_env/bin/activate
</pre>
</div>
</div>
</div>
<div id="outline-container-orgd83f74e" class="outline-4">
<h4 id="orgd83f74e">Windows – Símbolo del sistema (CMD)</h4>
<div class="outline-text-4" id="text-orgd83f74e">
<p>
Si se utiliza el <b>Símbolo del sistema</b> (cmd.exe), ejecute:
</p>

<div class="org-src-container">
<pre class="src src-bat">mlp_env\Scripts\activate.bat
</pre>
</div>
</div>
</div>
<div id="outline-container-org930287a" class="outline-4">
<h4 id="org930287a">Windows – PowerShell</h4>
<div class="outline-text-4" id="text-org930287a">
<p>
Si se utiliza <b>Windows PowerShell</b>, ejecute:
</p>

<div class="org-src-container">
<pre class="src src-powershell">mlp_env\Scripts\Activate.ps1
</pre>
</div>

<p>
En caso de que la ejecución de scripts esté deshabilitada, habilítela
temporalmente con:
</p>

<div class="org-src-container">
<pre class="src src-powershell">Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process
</pre>
</div>

<p>
Posteriormente, vuelva a ejecutar el comando de activación.
</p>
</div>
</div>
<div id="outline-container-org6a8d374" class="outline-4">
<h4 id="org6a8d374">Windows – Git Bash</h4>
<div class="outline-text-4" id="text-org6a8d374">
<p>
Si se utiliza <b>Git Bash</b>, ejecute:
</p>

<div class="org-src-container">
<pre class="src src-bash"><span style="color: #483d8b;">source</span> mlp_env/Scripts/activate
</pre>
</div>

<p>
Una vez activado el entorno virtual, el nombre del entorno aparecerá entre
paréntesis al inicio de la línea de comandos.
</p>
</div>
</div>
</div>
<div id="outline-container-org33d747b" class="outline-3">
<h3 id="org33d747b">Actualización del gestor de paquetes</h3>
<div class="outline-text-3" id="text-org33d747b">
<p>
Antes de instalar las librerías, se recomienda actualizar el gestor de paquetes
<code>pip</code>.
</p>

<div class="org-src-container">
<pre class="src src-bash">pip install --upgrade pip
</pre>
</div>
</div>
</div>
<div id="outline-container-org6585e96" class="outline-3">
<h3 id="org6585e96">Instalación de dependencias</h3>
<div class="outline-text-3" id="text-org6585e96">
<p>
Ejecute el siguiente comando para instalar los requerimientos del módulo
Perceptrón Multicapa <b>feedforward</b>.
</p>

<div class="org-src-container">
<pre class="src src-bash">pip install numpy matplotlib scikit-learn pygame
</pre>
</div>
</div>
</div>
<div id="outline-container-org937286a" class="outline-3">
<h3 id="org937286a">Verificación de la instalación</h3>
<div class="outline-text-3" id="text-org937286a">
<p>
Para verificar que las dependencias se instalaron correctamente, ejecute Python
en modo interactivo:
</p>

<div class="org-src-container">
<pre class="src src-bash">python
</pre>
</div>

<p>
Posteriormente, importe las librerías:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy
<span style="color: #a020f0;">import</span> matplotlib
<span style="color: #a020f0;">import</span> sklearn
<span style="color: #a020f0;">import</span> pygame
<span style="color: #483d8b;">print</span><span style="color: #707183;">(</span><span style="color: #8b2252;">"Instalaci&#243;n de requerimientos completada correctamente"</span><span style="color: #707183;">)</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-org5b8a8b6" class="outline-3">
<h3 id="org5b8a8b6">Desactivación del entorno virtual</h3>
<div class="outline-text-3" id="text-org5b8a8b6">
<p>
Una vez finalizado el trabajo, el entorno virtual puede desactivarse con el
siguiente comando:
</p>

<div class="org-src-container">
<pre class="src src-bash">deactivate
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org60db9a4" class="outline-2">
<h2 id="org60db9a4">Manual de Entornos Virtuales en Python</h2>
<div class="outline-text-2" id="text-org60db9a4">
</div>
<div id="outline-container-orga33fe24" class="outline-3">
<h3 id="orga33fe24">Introducción</h3>
<div class="outline-text-3" id="text-orga33fe24">
<p>
El uso de entornos virtuales es esencial para mantener las
dependencias de tus proyectos aisladas. En este manual aprenderás a
gestionarlos usando el módulo estándar <code>venv</code>.
</p>
</div>
</div>
<div id="outline-container-orge7daa4a" class="outline-3">
<h3 id="orge7daa4a">Flujo de Trabajo Básico</h3>
<div class="outline-text-3" id="text-orge7daa4a">
</div>
<div id="outline-container-org9c59411" class="outline-4">
<h4 id="org9c59411">1. Creación del entorno</h4>
<div class="outline-text-4" id="text-org9c59411">
<p>
Para crear un entorno virtual, navega a la raíz de tu proyecto en la terminal (o dentro de un buffer de Emacs con <code>M-x shell</code>) y ejecuta:
</p>

<div class="org-src-container">
<pre class="src src-bash">python -m venv .venv
</pre>
</div>

<p>
<b>Nota:</b> El nombre <code>.venv</code> es una convención que hace que el directorio sea oculto en sistemas Unix.
</p>
</div>
</div>
<div id="outline-container-orge6288f5" class="outline-4">
<h4 id="orge6288f5">2. Activación</h4>
<div class="outline-text-4" id="text-orge6288f5">
<p>
La activación depende de tu sistema operativo:
</p>
</div>
<ul class="org-ul">
<li><a id="org4eef6a6"></a>En Windows (PowerShell)<br />
<div class="outline-text-5" id="text-org4eef6a6">
<div class="org-src-container">
<pre class="src src-powershell">.\.venv\Scripts\Activate.ps1
</pre>
</div>
</div>
</li>
<li><a id="org4f2404b"></a>En macOS / Linux<br />
<div class="outline-text-5" id="text-org4f2404b">
<div class="org-src-container">
<pre class="src src-bash"><span style="color: #483d8b;">source</span> .venv/bin/activate
</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div id="outline-container-org35a2d0a" class="outline-3">
<h3 id="org35a2d0a">Gestión de paquetes</h3>
<div class="outline-text-3" id="text-org35a2d0a">
<p>
Una vez activado (verás el prefijo <code>(.venv)</code> en tu prompt), puedes instalar librerías:
</p>

<div class="org-src-container">
<pre class="src src-bash">pip install requests pandas
</pre>
</div>
</div>
</div>
<div id="outline-container-org0d2e26d" class="outline-3">
<h3 id="org0d2e26d">El archivo de Requerimientos</h3>
<div class="outline-text-3" id="text-org0d2e26d">
<p>
Es fundamental para la reproducibilidad del proyecto.
</p>
</div>
</div>
<div id="outline-container-org59b2408" class="outline-3">
<h3 id="org59b2408">Exportar dependencias</h3>
<div class="outline-text-3" id="text-org59b2408">
<div class="org-src-container">
<pre class="src src-bash">pip freeze &gt; requirements.txt
</pre>
</div>
</div>
</div>
<div id="outline-container-org6210df1" class="outline-3">
<h3 id="org6210df1">Instalar desde el archivo</h3>
<div class="outline-text-3" id="text-org6210df1">
<div class="org-src-container">
<pre class="src src-bash">pip install -r requirements.txt
</pre>
</div>
</div>
</div>
<div id="outline-container-org1fe0104" class="outline-3">
<h3 id="org1fe0104">Tips de Limpieza</h3>
<div class="outline-text-3" id="text-org1fe0104">
<p>
Para salir del entorno virtual:
</p>
<div class="org-src-container">
<pre class="src src-bash">deactivate
</pre>
</div>

<p>
Para borrar el entorno, simplemente elimina la carpeta:
</p>
<div class="org-src-container">
<pre class="src src-bash">rm -rf .venv  <span style="color: #b22222;"># </span><span style="color: #b22222;">En Linux/macOS
</span>rmdir /s /q .venv  <span style="color: #b22222;"># </span><span style="color: #b22222;">En Windows</span>
</pre>
</div>

<p>
&#x2014;
</p>
<blockquote>
<p>
"Keep your global Python clean, keep your projects isolated."
</p>
</blockquote>
</div>
</div>
<div id="outline-container-org905da8c" class="outline-3">
<h3 id="org905da8c">Entornos Virtuales Python (Edición Windows)</h3>
</div>

<div id="outline-container-orgcbacff7" class="outline-3">
<h3 id="orgcbacff7">Requisitos Previos</h3>
<div class="outline-text-3" id="text-orgcbacff7">
<ol class="org-ol">
<li>Tener instalado Python (descargado de <a href="https://www.python.org/">python.org</a> o la Microsoft Store).</li>
<li>Durante la instalación, asegúrate de marcar la casilla: <b><b>"Add Python to PATH"</b></b>.</li>
</ol>
</div>
</div>
<div id="outline-container-org6df5084" class="outline-3">
<h3 id="org6df5084">Flujo de Trabajo en Windows</h3>
<div class="outline-text-3" id="text-org6df5084">
</div>
<div id="outline-container-org1ecaf66" class="outline-4">
<h4 id="org1ecaf66">1. Crear el Entorno Virtual</h4>
<div class="outline-text-4" id="text-org1ecaf66">
<p>
Abre tu terminal (PowerShell o CMD) en la carpeta de tu proyecto. El comando es el mismo para ambos:
</p>

<div class="org-src-container">
<pre class="src src-powershell">python -m venv venv
</pre>
</div>
</div>
</div>
<div id="outline-container-org2d2fd9a" class="outline-4">
<h4 id="org2d2fd9a">2. El Paso Crítico: La Activación</h4>
<div class="outline-text-4" id="text-org2d2fd9a">
<p>
En Windows, la activación depende de qué terminal estés usando.
</p>
</div>
<ul class="org-ul">
<li><a id="orgdf0f95f"></a>Opción A: PowerShell (Recomendado)<br />
<div class="outline-text-5" id="text-orgdf0f95f">
<p>
Si es la primera vez que usas scripts en Windows, podrías recibir un error de seguridad. Primero, ejecuta esto como administrador (solo una vez):
</p>
<div class="org-src-container">
<pre class="src src-powershell">Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
</pre>
</div>

<p>
Luego, para activar el entorno:
</p>
<div class="org-src-container">
<pre class="src src-powershell">.\venv\Scripts\Activate.ps1
</pre>
</div>
</div>
</li>
<li><a id="org165c9b5"></a>Opción B: Símbolo del Sistema (CMD)<br />
<div class="outline-text-5" id="text-org165c9b5">
<div class="org-src-container">
<pre class="src src-cmd">.\venv\Scripts\activate.bat
</pre>
</div>
</div>
</li>
</ul>
</div>
<div id="outline-container-orgb732b51" class="outline-4">
<h4 id="orgb732b51">3. Confirmación</h4>
<div class="outline-text-4" id="text-orgb732b51">
<p>
Sabrás que el entorno está activo porque el nombre <code>(venv)</code> aparecerá a la izquierda de la ruta en tu terminal:
#+example
(venv) C:\Proyectos\MiProyecto&gt;
#+example
</p>
</div>
</div>
</div>
<div id="outline-container-orgd972244" class="outline-3">
<h3 id="orgd972244">Gestión de Librerías con PIP</h3>
<div class="outline-text-3" id="text-orgd972244">
</div>
<div id="outline-container-org263d455" class="outline-4">
<h4 id="org263d455">Instalación de paquetes</h4>
<div class="outline-text-4" id="text-org263d455">
<p>
Una vez activo el entorno, instala lo que necesites:
</p>
<div class="org-src-container">
<pre class="src src-powershell">pip install pandas requests openpyxl
</pre>
</div>
</div>
</div>
<div id="outline-container-org9c377df" class="outline-4">
<h4 id="org9c377df">Congelar dependencias (Compartir proyecto)</h4>
<div class="outline-text-4" id="text-org9c377df">
<p>
Para que otros participantes tengan exactamente lo mismo que tú:
</p>
<div class="org-src-container">
<pre class="src src-powershell">pip freeze &gt; requirements.txt
</pre>
</div>
</div>
</div>
<div id="outline-container-orgd16c0b9" class="outline-4">
<h4 id="orgd16c0b9">Instalar desde un archivo recibido</h4>
<div class="outline-text-4" id="text-orgd16c0b9">
<p>
Si un compañero te pasa su <code>requirements.txt</code>:
</p>
<div class="org-src-container">
<pre class="src src-powershell">pip install -r requirements.txt
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org2d128bc" class="outline-3">
<h3 id="org2d128bc">Uso de Entornos en Emacs (Windows)</h3>
<div class="outline-text-3" id="text-org2d128bc">
<p>
Para que Emacs en Windows gestione bien el entorno, añade esto a tu archivo de configuración (<code>init.el</code> o <code>.emacs</code>):
</p>
</div>
<div id="outline-container-org59b2eff" class="outline-4">
<h4 id="org59b2eff">Instalación del paquete pyvenv</h4>
<div class="outline-text-4" id="text-org59b2eff">
<div class="org-src-container">
<pre class="src src-elisp"><span style="color: #707183;">(</span><span style="color: #a020f0;">use-package</span> pyvenv
  <span style="color: #483d8b;">:ensure</span> t
  <span style="color: #483d8b;">:config</span>
  <span style="color: #7388d6;">(</span>pyvenv-mode 1<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-orgb30adea" class="outline-4">
<h4 id="orgb30adea">Cómo activarlo dentro de Emacs</h4>
<div class="outline-text-4" id="text-orgb30adea">
<ol class="org-ol">
<li>Presiona <code>M-x pyvenv-activate</code>.</li>
<li>Emacs te pedirá la ruta. Navega hasta la carpeta <code>venv</code> de tu proyecto.</li>
<li>Al seleccionarla, Emacs usará ese intérprete de Python para todos los scripts que ejecutes.</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org7867558" class="outline-3">
<h3 id="org7867558">Solución de Problemas Comunes en Windows</h3>
<div class="outline-text-3" id="text-org7867558">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">Error / Problema</td>
<td class="org-left">Solución</td>
</tr>

<tr>
<td class="org-left">:---</td>
<td class="org-left">:---</td>
</tr>

<tr>
<td class="org-left">"python" no se reconoce</td>
<td class="org-left">Reinstala Python y marca "Add to PATH" o usa el comando `py`.</td>
</tr>

<tr>
<td class="org-left">Error de "Execution Policy"</td>
<td class="org-left">Ejecuta `Set-ExecutionPolicy RemoteSigned -Scope CurrentUser`.</td>
</tr>

<tr>
<td class="org-left">No aparece el (venv)</td>
<td class="org-left">Asegúrate de usar el comando de activación correcto para tu terminal (.ps1 vs .bat).</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-orgdffce91" class="outline-3">
<h3 id="orgdffce91">Desactivación y Limpieza</h3>
<div class="outline-text-3" id="text-orgdffce91">
<p>
Para salir del entorno:
</p>
<div class="org-src-container">
<pre class="src src-powershell">deactivate
</pre>
</div>

<p>
Si quieres borrar el entorno por completo (para empezar de cero):
</p>
<div class="org-src-container">
<pre class="src src-powershell">rmdir /s /q venv
</pre>
</div>

<p>
&#x2014;
</p>
<div class="IMPORTANT" id="org851d8d4">
<p>
<b>Recordatorio:</b> Nunca incluyas la carpeta <code>venv</code> en tus archivos compartidos o en tu repositorio de Git. Solo comparte el código y el archivo <code>requirements.txt</code>.
</p>

</div>
</div>
</div>
<div id="outline-container-org8c3ad50" class="outline-3">
<h3 id="org8c3ad50">Agregar a jupyter notebook</h3>
<div class="outline-text-3" id="text-org8c3ad50">
<div class="org-src-container">
<pre class="src src-shell">pip install ipykernel
python -m ipykernel install --user --name=redes --display-name=<span style="color: #8b2252;">"redes"</span>
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org9e59490" class="outline-2">
<h2 id="org9e59490">Introducción a la Inteligencia Artificial (IA)</h2>
<div class="outline-text-2" id="text-org9e59490">
</div>
<div id="outline-container-orge8abd65" class="outline-3">
<h3 id="orge8abd65">Definición formal</h3>
<div class="outline-text-3" id="text-orge8abd65">
<p>
La <b>Inteligencia Artificial</b> es el área de la computación que estudia
  cómo construir sistemas capaces de realizar tareas que, si fueran
  realizadas por humanos, requerirían inteligencia.
</p>

<p>
Esto incluye capacidades como:
</p>

<ul class="org-ul">
<li>Aprender de la experiencia</li>
<li>Razonar</li>
<li>Reconocer patrones</li>
<li>Tomar decisiones bajo incertidumbre</li>
</ul>

<p>
La IA no implica necesariamente consciencia; se centra en <b>comportamiento inteligente observable</b>.
</p>
</div>
</div>
<div id="outline-container-orgc0d06be" class="outline-3">
<h3 id="orgc0d06be">IA débil vs IA fuerte</h3>
<div class="outline-text-3" id="text-orgc0d06be">
<ul class="org-ul">
<li><b>IA débil</b>: sistemas especializados en una tarea concreta (la IA actual)</li>
<li><b>IA fuerte</b>: inteligencia general comparable a la humana (teórica)</li>
</ul>

<p>
Las redes neuronales pertenecen claramente a la IA débil.
</p>
</div>
</div>
</div>
<div id="outline-container-org2ec9534" class="outline-2">
<h2 id="org2ec9534">Machine Learning (Aprendizaje Automático)</h2>
<div class="outline-text-2" id="text-org2ec9534">
<p>
<b>* Relación entre IA y ML
El *Machine Learning (ML)</b> es una subdisciplina de la IA. Mientras la IA es el objetivo general, el ML es una de las herramientas principales para alcanzarlo.
</p>

<p>
Idea clave:
</p>
<blockquote>
<p>
En lugar de programar reglas, el sistema aprende las reglas a partir de datos.
</p>
</blockquote>
</div>
<div id="outline-container-org538a1c5" class="outline-3">
<h3 id="org538a1c5">Tipos de aprendizaje</h3>
<div class="outline-text-3" id="text-org538a1c5">
<ul class="org-ul">
<li><b>Supervisado</b>: datos con etiqueta (clasificación, regresión)</li>
<li><b>No supervisado</b>: datos sin etiqueta (clustering, reducción de dimensión)</li>
<li><b>Por refuerzo</b>: aprendizaje mediante recompensa y castigo</li>
</ul>

<p>
Las redes neuronales pueden adaptarse a los tres esquemas.
</p>
</div>
</div>
</div>
<div id="outline-container-org1e9a825" class="outline-2">
<h2 id="org1e9a825">Historia de las Redes Neuronales</h2>
<div class="outline-text-2" id="text-org1e9a825">
</div>
<div id="outline-container-org41a039a" class="outline-3">
<h3 id="org41a039a">Contexto histórico</h3>
<div class="outline-text-3" id="text-org41a039a">
<p>
Desde mediados del siglo XX, científicos han intentado entender si la inteligencia podía ser replicada mediante modelos matemáticos.
</p>
</div>
</div>
<div id="outline-container-orgb7d7519" class="outline-3">
<h3 id="orgb7d7519">McCulloch y Pitts (1943)</h3>
<div class="outline-text-3" id="text-orgb7d7519">
<p>
Propusieron la primera neurona artificial basada en lógica
booleana. Demostraron que redes de estas neuronas podían computar
cualquier función lógica.
</p>

<p>
Este trabajo sentó las bases teóricas de las redes neuronales.
</p>
</div>
</div>
<div id="outline-container-orgc88900d" class="outline-3">
<h3 id="orgc88900d">Perceptrón y optimismo inicial</h3>
<div class="outline-text-3" id="text-orgc88900d">
<p>
En los años 50 y 60, el perceptrón generó grandes expectativas al ser uno de los primeros sistemas que <b>aprendía</b> automáticamente.
</p>
</div>
</div>
<div id="outline-container-org6abfff8" class="outline-3">
<h3 id="org6abfff8">Críticas y estancamiento</h3>
<div class="outline-text-3" id="text-org6abfff8">
<p>
El libro <b>Perceptrons</b> (Minsky &amp; Papert, 1969) demostró limitaciones severas del perceptrón, provocando una fuerte caída en el interés por las redes neuronales.
</p>
</div>
</div>
<div id="outline-container-orge9f6364" class="outline-3">
<h3 id="orge9f6364">Renacimiento moderno</h3>
<div class="outline-text-3" id="text-orge9f6364">
<p>
Con mayor poder computacional, grandes bases de datos y mejores algoritmos, las redes neuronales resurgen como <b>Deep Learning</b>.
</p>
</div>
</div>
</div>
<div id="outline-container-orge1d3add" class="outline-2">
<h2 id="orge1d3add">La Neurona Artificial</h2>
<div class="outline-text-2" id="text-orge1d3add">
</div>
<div id="outline-container-orgce9ef15" class="outline-3">
<h3 id="orgce9ef15">Motivación</h3>
<div class="outline-text-3" id="text-orgce9ef15">
<p>
Una red neuronal artificial es un modelo matemático inspirado en la
organización y conectividad de las neuronas biológicas, que abstrae
su funcionamiento esencial para construir sistemas capaces de
aprender representaciones a partir de datos mediante el ajuste de
parámetros.
</p>
</div>
</div>
<div id="outline-container-org735c33c" class="outline-3">
<h3 id="org735c33c">Componentes</h3>
<div class="outline-text-3" id="text-org735c33c">
<ul class="org-ul">
<li>Entradas: variables numéricas</li>
<li>Pesos: importancia relativa de cada entrada</li>
<li>Bias: ajuste del umbral</li>
<li>Activación: decisión final</li>
</ul>
</div>
</div>
<div id="outline-container-org76dccc7" class="outline-3">
<h3 id="org76dccc7">Modelo matemático</h3>
<div class="outline-text-3" id="text-org76dccc7">
<div class="org-src-container">
<pre class="src src-text">z = w&#183;x + b
a = f(z)
</pre>
</div>

<p>
Este modelo es la unidad básica de todas las redes neuronales modernas.
</p>
</div>
</div>
</div>
<div id="outline-container-org25a374b" class="outline-2">
<h2 id="org25a374b">El Perceptrón</h2>
<div class="outline-text-2" id="text-org25a374b">
</div>
<div id="outline-container-org4efaf09" class="outline-3">
<h3 id="org4efaf09">Definición</h3>
<div class="outline-text-3" id="text-org4efaf09">
<p>
El perceptrón es una neurona artificial entrenable utilizada para clasificación binaria.
</p>
</div>
</div>
<div id="outline-container-orgdb2daa7" class="outline-3">
<h3 id="orgdb2daa7">Interpretación geométrica</h3>
<div class="outline-text-3" id="text-orgdb2daa7">
<p>
El perceptrón aprende una <b>frontera de decisión lineal</b> que separa los datos en dos clases.
</p>
</div>
</div>
<div id="outline-container-org07749df" class="outline-3">
<h3 id="org07749df">Aprendizaje</h3>
<div class="outline-text-3" id="text-org07749df">
<p>
Cuando el perceptrón se equivoca, ajusta sus pesos para reducir el error.
</p>
</div>
</div>
<div id="outline-container-org2e136e0" class="outline-3">
<h3 id="org2e136e0">Limitaciones</h3>
<div class="outline-text-3" id="text-org2e136e0">
<p>
No puede aprender relaciones no lineales, como XOR.
</p>
</div>
</div>
</div>
<div id="outline-container-orgd2b29db" class="outline-2">
<h2 id="orgd2b29db">¿Qué es un problema linealmente separable?</h2>
<div class="outline-text-2" id="text-orgd2b29db">
</div>
<div id="outline-container-org1f2a355" class="outline-3">
<h3 id="org1f2a355">Intuición básica</h3>
<div class="outline-text-3" id="text-org1f2a355">
<p>
Un problema es linealmente separable si puedes separar las clases usando una línea recta (en 2D),
un plano (en 3D), o en general un hiperplano.
Si existe una sola frontera recta que divide perfectamente las clases, el problema es linealmente separable.
</p>
</div>
</div>
</div>
<div id="outline-container-orgcdb5810" class="outline-2">
<h2 id="orgcdb5810">Estructura de un Perceptrón Simple</h2>
<div class="outline-text-2" id="text-orgcdb5810">
</div>
<div id="outline-container-org19a7b23" class="outline-3">
<h3 id="org19a7b23">Entradas (Inputs)</h3>
<div class="outline-text-3" id="text-org19a7b23">
<ul class="org-ul">
<li>Representadas como \(x_1, x_2, ..., x_n\).</li>
<li>Son los datos brutos o características que recibe el modelo.</li>
</ul>
</div>
</div>
<div id="outline-container-org95535b6" class="outline-3">
<h3 id="org95535b6">Pesos Sinápticos (Weights)</h3>
<div class="outline-text-3" id="text-org95535b6">
<ul class="org-ul">
<li>Representados como \(w_1, w_2, ..., w_n\).</li>
<li>Determinan la importancia o influencia de cada entrada en el resultado final.</li>
</ul>
</div>
</div>
<div id="outline-container-org5ba5f1e" class="outline-3">
<h3 id="org5ba5f1e">Suma Ponderada (Weighted Sum)</h3>
<div class="outline-text-3" id="text-org5ba5f1e">
<ul class="org-ul">
<li>Es la combinación lineal de las entradas y los pesos.</li>
<li>La fórmula matemática es:
\[z = \sum_{i=1}^{n} w_i x_i + b\]</li>
</ul>
</div>
</div>
<div id="outline-container-org07f09c8" class="outline-3">
<h3 id="org07f09c8">Sesgo (Bias)</h3>
<div class="outline-text-3" id="text-org07f09c8">
<ul class="org-ul">
<li>Representado generalmente como \(b\) (o \(w_0\)).</li>
<li>Permite desplazar la función de activación hacia la izquierda o derecha para ajustar mejor los datos.</li>
</ul>
</div>
</div>
<div id="outline-container-org798af64" class="outline-3">
<h3 id="org798af64">Función de Activación</h3>
<div class="outline-text-3" id="text-org798af64">
<ul class="org-ul">
<li>Decide si la neurona debe "dispararse" (activarse) o no.</li>
<li>En el perceptrón original de Rosenblatt, se utiliza la <b>Función Escalón</b> (Heaviside):
<ul class="org-ul">
<li>\(f(z) = 1\) si \(z > 0\)</li>
<li>\(f(z) = 0\) en caso contrario.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgb4270fd" class="outline-3">
<h3 id="orgb4270fd">Salida (Output)</h3>
<div class="outline-text-3" id="text-orgb4270fd">
<ul class="org-ul">
<li>Es el resultado final del procesamiento (\(\hat{y}\)).</li>
<li>En un perceptrón simple, suele ser un valor binario (0 o 1).</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org28e2f60" class="outline-2">
<h2 id="org28e2f60">Ejemplo Tacos</h2>
<div class="outline-text-2" id="text-org28e2f60">
</div>
<div id="outline-container-org1b5db83" class="outline-3">
<h3 id="org1b5db83">1. El Escenario: La Decisión de la Cena</h3>
<div class="outline-text-3" id="text-org1b5db83">
<p>
Para entender cómo aprende una IA, vamos a usar un ejemplo de la vida real. Queremos que nuestra neurona artificial aprenda cuándo estamos "Satisfechos" (1) o "Inconformes" (0).
</p>
</div>
<div id="outline-container-org99e45c3" class="outline-4">
<h4 id="org99e45c3">Nuestras Entradas (Inputs)</h4>
<div class="outline-text-4" id="text-org99e45c3">
<ul class="org-ul">
<li>\(x_0\): ¿Hay Tacos? (1 = Sí, 0 = No)</li>
<li>\(x_1\): ¿Hay Refresco? (1 = Sí, 0 = No)</li>
</ul>
</div>
</div>
<div id="outline-container-org011770f" class="outline-4">
<h4 id="org011770f">El Peso (Weight): La Importancia</h4>
<div class="outline-text-4" id="text-org011770f">
<p>
No todo nos hace igual de felices. Los <b><b>Pesos</b></b> (\(w_0, w_1\)) representan qué tanto nos importa cada ingrediente. 
</p>
<ul class="org-ul">
<li>Si el peso del taco es alto, el taco es fundamental para nuestra felicidad.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org439258c" class="outline-3">
<h3 id="org439258c">2. ¿Cómo toma la decisión? (La Suma y el Sesgo)</h3>
<div class="outline-text-3" id="text-org439258c">
<p>
La neurona hace un cálculo matemático simple:
\[net = (x_0 \cdot w_0) + (x_1 \cdot w_1) - \text{bias}\]
</p>
</div>
<div id="outline-container-orgc337946" class="outline-4">
<h4 id="orgc337946">El Sesgo (Bias)</h4>
<div class="outline-text-4" id="text-orgc337946">
<p>
El <b><b>Bias</b></b> es nuestro "nivel de exigencia". Si el Bias es muy alto, necesitaremos mucha comida (pesos altos) para poder llegar a estar satisfechos.
</p>
</div>
</div>
</div>
<div id="outline-container-orgcf36728" class="outline-3">
<h3 id="orgcf36728">3. El Descenso del Gradiente: ¿Cómo "Aprende" la Neurona?</h3>
<div class="outline-text-3" id="text-orgcf36728">
<p>
Cuando la neurona se equivoca, necesita ajustar sus pesos. Este proceso se llama <b><b>Descenso del Gradiente</b></b>.
</p>
</div>
<div id="outline-container-org0263ebd" class="outline-4">
<h4 id="org0263ebd">El Error: La Brújula</h4>
<div class="outline-text-4" id="text-org0263ebd">
<p>
Si esperábamos estar felices (<code>target = 1</code>) pero la neurona dice que estamos tristes (<code>net = 0</code>), tenemos un <b><b>Error de 1</b></b>.
\[Error = Target - Net\]
</p>
</div>
</div>
<div id="outline-container-orgdd207dc" class="outline-4">
<h4 id="orgdd207dc">El Ajuste (La Regla Delta)</h4>
<div class="outline-text-4" id="text-orgdd207dc">
<p>
Para corregir el error, cambiamos los pesos usando esta lógica:
</p>
<ol class="org-ol">
<li>Miramos el <b><b>Error</b></b>.</li>
<li>Miramos la <b><b>Entrada</b></b> (¿Quién tuvo la culpa? Si no había tacos, el taco no pudo causar el error).</li>
<li>Aplicamos el <b><b>Learning Rate (K)</b></b>: Que es qué tan rápido queremos aprender.</li>
</ol>

<p>
\[Nuevo\_Peso = Peso\_Actual + (K \cdot Error \cdot Entrada)\]
</p>


<p>
Imagina que \(K = 0.1\) y el Peso inicial del taco es \(0.5\).
</p>
<ol class="org-ol">
<li><b><b>Situación:</b></b> Hay tacos (\(x_0=1\)), pero la red dice "Triste" (\(net=0\)). Nosotros queríamos "Feliz" (\(target=1\)).</li>
<li><b><b>Cálculo del Error:</b></b> \(1 - 0 = 1\).</li>
<li><b><b>Ajuste del Peso:</b></b>
<ul class="org-ul">
<li>\(Delta = 0.1 \cdot 1 \cdot 1 = 0.1\)</li>
<li>\(Nuevo\_Peso\_Taco = 0.5 + 0.1 = 0.6\)</li>
</ul></li>
</ol>

<p>
<b><b>Resultado:</b></b> La próxima vez que haya tacos, la neurona estará un poco más cerca de hacernos felices. ¡Eso es el aprendizaje!
</p>

<p>
Puntos importantes
</p>

<ul class="org-ul">
<li>El <b><b>Gradiente</b></b> nos dice en qué dirección mover los pesos para que el error sea cero.</li>
<li>El <b><b>Learning Rate</b></b> controla qué tan grandes son los pasos que damos hacia esa solución.</li>
<li>Si repetimos este proceso miles de veces (Épocas), la neurona encontrará los pesos perfectos para nuestra felicidad.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org452c07a" class="outline-3">
<h3 id="org452c07a">Perceptrón ejemplo (Python)</h3>
<div class="outline-text-3" id="text-org452c07a">
<ol class="org-ol">
<li>Recibe entradas (\(x_1, x_2, \dots\)).</li>
<li>Las multiplica por pesos (\(w_1, w_2, \dots\)).</li>
<li>Suma un sesgo (\(b\)).</li>
<li>Aplica una función de activación (ej. Escalón).</li>
</ol>
</div>
<div id="outline-container-org1cbff08" class="outline-4">
<h4 id="org1cbff08">Perceptrón para una compuerta "AND"</h4>
<div class="outline-text-4" id="text-org1cbff08">
<p>
Este código simula una neurona que solo se activa si ambas entradas son 1.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">def</span> <span style="color: #0000ff;">perceptron_and</span><span style="color: #707183;">(</span>x1, x2<span style="color: #707183;">)</span>:
<span style="background-color: #f2f2f2;"> </span>   <span style="color: #b22222;"># </span><span style="color: #b22222;">1. Definimos los Pesos y el Sesgo (Bias)
</span><span style="background-color: #f2f2f2;"> </span>   <span style="color: #b22222;"># </span><span style="color: #b22222;">Estos valores normalmente se aprenden, aqu&#237; los asignamos manualmente
</span><span style="background-color: #f2f2f2;"> </span>   <span style="color: #a0522d;">w1</span>, <span style="color: #a0522d;">w2</span> = 0.5, 0.5
<span style="background-color: #f2f2f2;"> </span>   <span style="color: #a0522d;">bias</span> = -0.7

<span style="background-color: #f2f2f2;"> </span>   <span style="color: #b22222;"># </span><span style="color: #b22222;">2. Suma ponderada: (x1 * w1) + (x2 * w2) + bias
</span><span style="background-color: #f2f2f2;"> </span>   <span style="color: #a0522d;">suma</span> = <span style="color: #707183;">(</span>x1 * w1<span style="color: #707183;">)</span> + <span style="color: #707183;">(</span>x2 * w2<span style="color: #707183;">)</span> + bias

<span style="background-color: #f2f2f2;"> </span>   <span style="color: #b22222;"># </span><span style="color: #b22222;">3. Funci&#243;n de activaci&#243;n (Escal&#243;n de Heaviside)
</span><span style="background-color: #f2f2f2;"> </span>   <span style="color: #b22222;"># </span><span style="color: #b22222;">Si la suma es mayor a 0, la neurona se dispara (1)
</span><span style="background-color: #f2f2f2;"> </span>   <span style="color: #a020f0;">if</span> suma &gt; 0:
<span style="background-color: #f2f2f2;"> </span>   <span style="background-color: #f2f2f2;"> </span>   <span style="color: #a020f0;">return</span> 1
<span style="background-color: #f2f2f2;"> </span>   <span style="color: #a020f0;">else</span>:
<span style="background-color: #f2f2f2;"> </span>   <span style="background-color: #f2f2f2;"> </span>   <span style="color: #a020f0;">return</span> 0

<span style="color: #b22222;"># </span><span style="color: #b22222;">Prueba de la tabla de verdad AND
</span><span style="color: #a0522d;">entradas</span> = <span style="color: #707183;">[</span><span style="color: #7388d6;">(</span>0, 0<span style="color: #7388d6;">)</span>, <span style="color: #7388d6;">(</span>0, 1<span style="color: #7388d6;">)</span>, <span style="color: #7388d6;">(</span>1, 0<span style="color: #7388d6;">)</span>, <span style="color: #7388d6;">(</span>1, 1<span style="color: #7388d6;">)</span><span style="color: #707183;">]</span>

<span style="color: #483d8b;">print</span><span style="color: #707183;">(</span><span style="color: #8b2252;">"Entrada | Salida"</span><span style="color: #707183;">)</span>
<span style="color: #483d8b;">print</span><span style="color: #707183;">(</span><span style="color: #8b2252;">"--------|-------"</span><span style="color: #707183;">)</span>
<span style="color: #a020f0;">for</span> e <span style="color: #a020f0;">in</span> entradas:
<span style="background-color: #f2f2f2;"> </span>   <span style="color: #a0522d;">resultado</span> = perceptron_and<span style="color: #707183;">(</span>e<span style="color: #7388d6;">[</span>0<span style="color: #7388d6;">]</span>, e<span style="color: #7388d6;">[</span>1<span style="color: #7388d6;">]</span><span style="color: #707183;">)</span>
<span style="background-color: #f2f2f2;"> </span>   <span style="color: #483d8b;">print</span><span style="color: #707183;">(</span>f<span style="color: #8b2252;">" </span>{e}<span style="color: #8b2252;">  |   </span>{resultado}<span style="color: #8b2252;">"</span><span style="color: #707183;">)</span>
</pre>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-orgc08eba2" class="outline-2">
<h2 id="orgc08eba2">Codigo C python</h2>
<div class="outline-text-2" id="text-orgc08eba2">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> math
<span style="color: #a020f0;">import</span> random

<span style="color: #a0522d;">EPOCAS</span> = 300000
<span style="color: #a0522d;">K</span> = 0.03  <span style="color: #b22222;"># </span><span style="color: #b22222;">tasa de aprendizaje
</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">Pesos y bias globales (como en el c&#243;digo C)
</span><span style="color: #a0522d;">Pesos</span> = <span style="color: #707183;">[</span>0.0, 0.0<span style="color: #707183;">]</span>
<span style="color: #a0522d;">bias</span> = 0.5
<span style="color: #a0522d;">Error</span> = 0.0


<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">sigmoide</span><span style="color: #707183;">(</span>s: <span style="color: #483d8b;">float</span><span style="color: #707183;">)</span> -&gt; <span style="color: #483d8b;">float</span>:
<span style="background-color: #f2f2f2;"> </span>   <span style="color: #b22222;"># </span><span style="color: #b22222;">Versi&#243;n correcta de la sigmoide log&#237;stica:
</span><span style="background-color: #f2f2f2;"> </span>   <span style="color: #b22222;"># </span><span style="color: #b22222;">1 / (1 + e^{-s})
</span><span style="background-color: #f2f2f2;"> </span>   <span style="color: #a020f0;">return</span> 1.0 / <span style="color: #707183;">(</span>1.0 + math.exp<span style="color: #7388d6;">(</span>-s<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>


<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">pesos_initNt</span><span style="color: #707183;">()</span>:
<span style="background-color: #f2f2f2;"> </span>   <span style="color: #8b2252;">"""Inicializa los pesos de forma aleatoria en [0,1)."""</span>
<span style="background-color: #f2f2f2;"> </span>   <span style="color: #a020f0;">global</span> Pesos
<span style="background-color: #f2f2f2;"> </span>   <span style="color: #a0522d;">Pesos</span> = <span style="color: #707183;">[</span>random.random<span style="color: #7388d6;">()</span> <span style="color: #a020f0;">for</span> _ <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">range</span><span style="color: #7388d6;">(</span>2<span style="color: #7388d6;">)</span><span style="color: #707183;">]</span>


<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">EntNt</span><span style="color: #707183;">(</span>x0: <span style="color: #483d8b;">float</span>, x1: <span style="color: #483d8b;">float</span>, target: <span style="color: #483d8b;">float</span><span style="color: #707183;">)</span> -&gt; <span style="color: #483d8b;">float</span>:
<span style="background-color: #f2f2f2;"> </span>   <span style="color: #8b2252;">"""
</span><span style="color: #8b2252; background-color: #f2f2f2;"> </span><span style="color: #8b2252;">   Funci&#243;n de entrenamiento del perceptr&#243;n (una iteraci&#243;n para un patr&#243;n).
</span><span style="color: #8b2252; background-color: #f2f2f2;"> </span><span style="color: #8b2252;">   Modifica Pesos y bias de forma global.
</span><span style="color: #8b2252; background-color: #f2f2f2;"> </span><span style="color: #8b2252;">   """</span>
<span style="background-color: #f2f2f2;"> </span>   <span style="color: #a020f0;">global</span> Pesos, bias, Error

<span style="background-color: #f2f2f2;"> </span>   <span style="color: #b22222;"># </span><span style="color: #b22222;">net = w0*x0 + w1*x1 - bias
</span><span style="background-color: #f2f2f2;"> </span>   <span style="color: #a0522d;">net</span> = Pesos<span style="color: #707183;">[</span>0<span style="color: #707183;">]</span> * x0 + Pesos<span style="color: #707183;">[</span>1<span style="color: #707183;">]</span> * x1 - bias
<span style="background-color: #f2f2f2;"> </span>   <span style="color: #a0522d;">net</span> = sigmoide<span style="color: #707183;">(</span>net<span style="color: #707183;">)</span>

<span style="background-color: #f2f2f2;"> </span>   <span style="color: #a0522d;">Error</span> = target - net

<span style="background-color: #f2f2f2;"> </span>   <span style="color: #b22222;"># </span><span style="color: #b22222;">Actualizaci&#243;n de bias (se asume entrada de bias = 1)
</span><span style="background-color: #f2f2f2;"> </span>   <span style="color: #a0522d;">bias</span> -= K * Error

<span style="background-color: #f2f2f2;"> </span>   <span style="color: #b22222;"># </span><span style="color: #b22222;">Variaci&#243;n de los pesos sin&#225;pticos
</span><span style="background-color: #f2f2f2;"> </span>   <span style="color: #a0522d;">delta0</span> = K * Error * x0
<span style="background-color: #f2f2f2;"> </span>   <span style="color: #a0522d;">delta1</span> = K * Error * x1

<span style="background-color: #f2f2f2;"> </span>   <span style="color: #b22222;"># </span><span style="color: #b22222;">Ajuste de pesos
</span><span style="background-color: #f2f2f2;"> </span>   <span style="color: #a0522d;">Pesos</span><span style="color: #707183;">[</span>0<span style="color: #707183;">]</span> += delta0
<span style="background-color: #f2f2f2;"> </span>   <span style="color: #a0522d;">Pesos</span><span style="color: #707183;">[</span>1<span style="color: #707183;">]</span> += delta1

<span style="background-color: #f2f2f2;"> </span>   <span style="color: #a020f0;">return</span> net


<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">InitNt</span><span style="color: #707183;">(</span>x0: <span style="color: #483d8b;">float</span>, x1: <span style="color: #483d8b;">float</span><span style="color: #707183;">)</span> -&gt; <span style="color: #483d8b;">float</span>:
<span style="background-color: #f2f2f2;"> </span>   <span style="color: #8b2252;">"""
</span><span style="color: #8b2252; background-color: #f2f2f2;"> </span><span style="color: #8b2252;">   Funci&#243;n que usa pesos fijos (de ejemplo) para calcular la salida.
</span><span style="color: #8b2252; background-color: #f2f2f2;"> </span><span style="color: #8b2252;">   Equivalente a la parte comentada en C.
</span><span style="color: #8b2252; background-color: #f2f2f2;"> </span><span style="color: #8b2252;">   """</span>
<span style="background-color: #f2f2f2;"> </span>   <span style="color: #b22222;"># </span><span style="color: #b22222;">net = 70.934807*x0 + 93.935219*x1 - 187.886169;
</span><span style="background-color: #f2f2f2;"> </span>   <span style="color: #a0522d;">net</span> = 70.934807 * x0 + 93.935219 * x1 - 187.886169
<span style="background-color: #f2f2f2;"> </span>   <span style="color: #a0522d;">net</span> = sigmoide<span style="color: #707183;">(</span>net<span style="color: #707183;">)</span>
<span style="background-color: #f2f2f2;"> </span>   <span style="color: #a020f0;">return</span> net


<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">main</span><span style="color: #707183;">()</span>:
<span style="background-color: #f2f2f2;"> </span>   <span style="color: #a020f0;">global</span> Pesos, bias, Error

<span style="background-color: #f2f2f2;"> </span>   pesos_initNt<span style="color: #707183;">()</span>

<span style="background-color: #f2f2f2;"> </span>   <span style="color: #a020f0;">for</span> i <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">range</span><span style="color: #707183;">(</span>EPOCAS<span style="color: #707183;">)</span>:
<span style="background-color: #f2f2f2;"> </span>   <span style="background-color: #f2f2f2;"> </span>   <span style="color: #483d8b;">print</span><span style="color: #707183;">(</span><span style="color: #8b2252;">"------------------------"</span><span style="color: #707183;">)</span>
<span style="background-color: #f2f2f2;"> </span>   <span style="background-color: #f2f2f2;"> </span>   <span style="color: #483d8b;">print</span><span style="color: #707183;">(</span>f<span style="color: #8b2252;">"Salida Entrenamiento Epoca </span>{i}<span style="color: #8b2252;">"</span><span style="color: #707183;">)</span>

<span style="background-color: #f2f2f2;"> </span>   <span style="background-color: #f2f2f2;"> </span>   <span style="color: #a0522d;">apr</span> = EntNt<span style="color: #707183;">(</span>1, 1, 0<span style="color: #707183;">)</span>
<span style="background-color: #f2f2f2;"> </span>   <span style="background-color: #f2f2f2;"> </span>   <span style="color: #483d8b;">print</span><span style="color: #707183;">(</span>f<span style="color: #8b2252;">"1,1 = </span>{apr:.6f}<span style="color: #8b2252;">"</span><span style="color: #707183;">)</span>

<span style="background-color: #f2f2f2;"> </span>   <span style="background-color: #f2f2f2;"> </span>   <span style="color: #a0522d;">apr</span> = EntNt<span style="color: #707183;">(</span>1, 0, 1<span style="color: #707183;">)</span>
<span style="background-color: #f2f2f2;"> </span>   <span style="background-color: #f2f2f2;"> </span>   <span style="color: #483d8b;">print</span><span style="color: #707183;">(</span>f<span style="color: #8b2252;">"1,0 = </span>{apr:.6f}<span style="color: #8b2252;">"</span><span style="color: #707183;">)</span>

<span style="background-color: #f2f2f2;"> </span>   <span style="background-color: #f2f2f2;"> </span>   <span style="color: #a0522d;">apr</span> = EntNt<span style="color: #707183;">(</span>0, 1, 1<span style="color: #707183;">)</span>
<span style="background-color: #f2f2f2;"> </span>   <span style="background-color: #f2f2f2;"> </span>   <span style="color: #483d8b;">print</span><span style="color: #707183;">(</span>f<span style="color: #8b2252;">"0,1 = </span>{apr:.6f}<span style="color: #8b2252;">"</span><span style="color: #707183;">)</span>

<span style="background-color: #f2f2f2;"> </span>   <span style="background-color: #f2f2f2;"> </span>   <span style="color: #a0522d;">apr</span> = EntNt<span style="color: #707183;">(</span>0, 0, 0<span style="color: #707183;">)</span>
<span style="background-color: #f2f2f2;"> </span>   <span style="background-color: #f2f2f2;"> </span>   <span style="color: #483d8b;">print</span><span style="color: #707183;">(</span>f<span style="color: #8b2252;">"0,0 = </span>{apr:.6f}<span style="color: #8b2252;">"</span><span style="color: #707183;">)</span>

<span style="background-color: #f2f2f2;"> </span>   <span style="background-color: #f2f2f2;"> </span>   <span style="color: #483d8b;">print</span><span style="color: #707183;">(</span><span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Pesos de cada epoca"</span><span style="color: #707183;">)</span>
<span style="background-color: #f2f2f2;"> </span>   <span style="background-color: #f2f2f2;"> </span>   <span style="color: #483d8b;">print</span><span style="color: #707183;">(</span>f<span style="color: #8b2252;">"Peso 0 = </span>{Pesos[0]:.6f}<span style="color: #8b2252;">"</span><span style="color: #707183;">)</span>
<span style="background-color: #f2f2f2;"> </span>   <span style="background-color: #f2f2f2;"> </span>   <span style="color: #483d8b;">print</span><span style="color: #707183;">(</span>f<span style="color: #8b2252;">"Peso 1 = </span>{Pesos[1]:.6f}<span style="color: #8b2252;">"</span><span style="color: #707183;">)</span>
<span style="background-color: #f2f2f2;"> </span>   <span style="background-color: #f2f2f2;"> </span>   <span style="color: #483d8b;">print</span><span style="color: #707183;">(</span>f<span style="color: #8b2252;">"Bias   = </span>{bias:.6f}<span style="color: #8b2252;">"</span><span style="color: #707183;">)</span>
<span style="background-color: #f2f2f2;"> </span>   <span style="background-color: #f2f2f2;"> </span>   <span style="color: #483d8b;">print</span><span style="color: #707183;">(</span>f<span style="color: #8b2252;">"Error  = </span>{Error:.6f}<span style="color: #8b2252;">"</span><span style="color: #707183;">)</span>
<span style="background-color: #f2f2f2;"> </span>   <span style="background-color: #f2f2f2;"> </span>   <span style="color: #483d8b;">print</span><span style="color: #707183;">(</span><span style="color: #8b2252;">"------------------------"</span><span style="color: #707183;">)</span>

<span style="background-color: #f2f2f2;"> </span>   <span style="color: #b22222;"># </span><span style="color: #b22222;">Si quieres probar con los pesos fijos del comentario de C:
</span><span style="background-color: #f2f2f2;"> </span>   <span style="color: #b22222;"># </span><span style="color: #b22222;">print("Resultados con InitNt:")
</span><span style="background-color: #f2f2f2;"> </span>   <span style="color: #b22222;"># </span><span style="color: #b22222;">print("1,1 =", InitNt(1, 1))
</span><span style="background-color: #f2f2f2;"> </span>   <span style="color: #b22222;"># </span><span style="color: #b22222;">print("1,0 =", InitNt(1, 0))
</span><span style="background-color: #f2f2f2;"> </span>   <span style="color: #b22222;"># </span><span style="color: #b22222;">print("0,1 =", InitNt(0, 1))
</span><span style="background-color: #f2f2f2;"> </span>   <span style="color: #b22222;"># </span><span style="color: #b22222;">print("0,0 =", InitNt(0, 0))
</span>

<span style="color: #a020f0;">if</span> <span style="color: #483d8b;">__name__</span> == <span style="color: #8b2252;">"__main__"</span>:
<span style="background-color: #f2f2f2;"> </span>   main<span style="color: #707183;">()</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-org87de320" class="outline-2">
<h2 id="org87de320">Redes Neuronales Multicapa (Multilayer Perceptron)</h2>
<div class="outline-text-2" id="text-org87de320">
</div>
<div id="outline-container-orgf04393b" class="outline-3">
<h3 id="orgf04393b">Introducción</h3>
<div class="outline-text-3" id="text-orgf04393b">
<p>
Una <b>red neuronal multicapa</b> o <b>Perceptrón Multicapa (MLP)</b> es un modelo
de aprendizaje supervisado que extiende al perceptrón simple mediante
la inclusión de <b>una o más capas ocultas</b>. Estas capas permiten modelar
<b>relaciones no lineales complejas</b> entre las variables de entrada y la salida.
</p>

<p>
Los MLP constituyen la base conceptual del <b>Deep Learning</b> moderno:
</p>

<ul class="org-ul">
<li>El perceptrón simple solo puede resolver problemas <b>linealmente separables</b>.</li>
<li>Muchos problemas reales (visión, texto, señales) son <b>no lineales</b>.</li>
<li>Agregar capas ocultas + funciones de activación no lineales permite
aproximar funciones mucho más complejas (teorema del aproximador universal).</li>
</ul>

<p>
Ejemplos clásicos (con 2 entradas):
</p>

<ul class="org-ul">
<li>AND  → ✓ separable linealmente</li>
<li>OR   → ✓ separable linealmente</li>
<li>XOR  → ✗ <b>no</b> es separable linealmente</li>
</ul>

<p>
El problema XOR demuestra la necesidad de introducir:
</p>

<ul class="org-ul">
<li><b>Capas ocultas</b> (capacidad de representar interacciones no triviales),</li>
<li><b>Funciones de activación no lineales</b>.</li>
</ul>

<p>
Un MLP típico está compuesto por:
</p>

<ol class="org-ol">
<li><b>Capa de entrada</b>  
Recibe el vector de características \(\mathbf{x}\).</li>

<li><b>Capas ocultas</b>  
Realizan transformaciones lineales + no lineales sobre las
representaciones intermedias.</li>

<li><b>Capa de salida</b>  
Produce la predicción final (clase, probabilidad, valor continuo, etc.).</li>
</ol>

<p>
Estructura conceptual:
</p>

<pre class="example" id="org6ae7235">
x → Capa Oculta 1 → Capa Oculta 2 → … → Capa de salida → ŷ
</pre>

<p>
Cada “flecha” representa una transformación lineal (multiplicación por
matriz de pesos + suma de bias) seguida de una función de activación.
</p>
</div>
</div>
<div id="outline-container-orgfa9d43f" class="outline-3">
<h3 id="orgfa9d43f">Modelo matemático</h3>
<div class="outline-text-3" id="text-orgfa9d43f">
</div>
<div id="outline-container-orgd9d565a" class="outline-4">
<h4 id="orgd9d565a">Neurona individual</h4>
<div class="outline-text-4" id="text-orgd9d565a">
<p>
Cada neurona recibe un vector de entrada \(\mathbf{x} \in \mathbb{R}^n\) y calcula:
</p>

<p>
\[
z = \sum_{i=1}^{n} w_i x_i + b
\]
</p>

<p>
\[
a = f(z)
\]
</p>

<p>
Donde:
</p>

<ul class="org-ul">
<li>\(w_i\): pesos sinápticos de la neurona,</li>
<li>\(b\): término de sesgo (bias),</li>
<li>\(f(\cdot)\): función de activación,</li>
<li>\(z\): combinación lineal de las entradas,</li>
<li>\(a\): salida (activación) de la neurona.</li>
</ul>

<p>
En forma vectorial para una neurona:
</p>

<p>
\[
z = \mathbf{w}^\top \mathbf{x} + b
\]
</p>

<p>
\[
a = f(z)
\]
</p>
</div>
</div>
<div id="outline-container-org3abb12f" class="outline-4">
<h4 id="org3abb12f">Forma matricial de una capa</h4>
<div class="outline-text-4" id="text-org3abb12f">
<p>
En una capa con múltiples neuronas, se usa notación matricial:
</p>

<ul class="org-ul">
<li>\(\mathbf{a}^{(l-1)}\): vector de activaciones de la capa anterior,</li>
<li>\(\mathbf{W}^{(l)}\): matriz de pesos de la capa \(l\),</li>
<li>\(\mathbf{b}^{(l)}\): vector de bias de la capa \(l\),</li>
<li>\(\mathbf{z}^{(l)}\): preactivaciones de la capa \(l\),</li>
<li>\(\mathbf{a}^{(l)}\): activaciones de la capa \(l\).</li>
</ul>

<p>
Cálculo:
</p>

<p>
\[
\mathbf{z}^{(l)} = \mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}
\]
</p>

<p>
\[
\mathbf{a}^{(l)} = f(\mathbf{z}^{(l)})
\]
</p>

<p>
donde \(f\) se aplica componente a componente.
</p>

<p>
Repitiendo este proceso desde la capa de entrada hasta la salida,
obtenemos la predicción del MLP.
</p>
</div>
</div>
</div>
<div id="outline-container-org670f5f4" class="outline-3">
<h3 id="org670f5f4">Funciones de activación</h3>
<div class="outline-text-3" id="text-org670f5f4">
<p>
Las funciones de activación son componentes fundamentales en las redes
neuronales multicapa. Su función principal es introducir <b>no linealidad</b>
en el modelo.
</p>

<p>
Sin funciones de activación no lineales:
</p>

<ul class="org-ul">
<li>cada capa sería solo una transformación lineal,</li>
<li>y la composición de múltiples transformaciones lineales es <b>otra</b>
transformación lineal,</li>
<li>por lo tanto, una “red profunda” sin no linealidad se comporta como
un <b>modelo lineal simple</b>, independientemente del número de capas.</li>
</ul>
</div>
<div id="outline-container-org8ecb0d2" class="outline-4">
<h4 id="org8ecb0d2">¿Para qué sirven las funciones de activación?</h4>
<div class="outline-text-4" id="text-org8ecb0d2">
<p>
Las funciones de activación permiten:
</p>

<ul class="org-ul">
<li>Introducir <b>no linealidad</b> en la red.</li>
<li>Modelar relaciones complejas entre variables de entrada y salida.</li>
<li>Aprender fronteras de decisión no lineales en problemas de clasificación.</li>
<li>Aproximar cualquier función continua bajo ciertas condiciones
(teorema del aproximador universal).</li>
</ul>

<p>
Recordemos que una neurona realiza:
</p>

<p>
\[
z = \sum_{i=1}^{n} w_i x_i + b
\]
</p>

<p>
\[
a = f(z)
\]
</p>

<p>
Donde \(f\) es la función de activación.
</p>
</div>
</div>
</div>
<div id="outline-container-orgae9e0eb" class="outline-3">
<h3 id="orgae9e0eb">Clasificación de las funciones de activación</h3>
<div class="outline-text-3" id="text-orgae9e0eb">
<p>
Las funciones de activación pueden clasificarse según:
</p>

<ul class="org-ul">
<li>su forma (escalón, sigmoide, lineal por partes, etc.),</li>
<li>su rango de salida,</li>
<li>su uso (capas ocultas vs capa de salida),</li>
<li>su comportamiento para valores grandes de \(|z|\).</li>
</ul>
</div>
<div id="outline-container-orgbf074db" class="outline-4">
<h4 id="orgbf074db">Funciones de activación clásicas</h4>
<div class="outline-text-4" id="text-orgbf074db">
</div>
<ul class="org-ul">
<li><a id="orga0ffe04"></a>Función escalón (Step Function)<br />
<div class="outline-text-5" id="text-orga0ffe04">
<p>
\[
f(z) =
</p>
\begin{cases}
1 & \text{si } z \ge 0 \\
0 & \text{si } z < 0
\end{cases}
<p>
\]
</p>

<p>
Características:
</p>

<ul class="org-ul">
<li>Usada en el perceptrón simple original.</li>
<li>No es continua ni derivable (no adecuada para descenso de gradiente).</li>
<li>Genera salidas binarias (0/1).</li>
</ul>

<p>
Uso:
</p>

<ul class="org-ul">
<li>Modelos teóricos e históricos.</li>
<li>Introducción al concepto de neurona como “disparar / no disparar”.</li>
<li>No se utiliza en redes multicapa modernas para entrenamiento con
backpropagation.</li>
</ul>
</div>
</li>
<li><a id="orgaf1c19c"></a>Sigmoide logística<br />
<div class="outline-text-5" id="text-orgaf1c19c">
<p>
\[
f(z) = \frac{1}{1 + e^{-z}}
\]
</p>

<p>
Características:
</p>

<ul class="org-ul">
<li>Salida en el intervalo \((0, 1)\).</li>
<li>Puede interpretarse como una probabilidad.</li>
<li>Es suave y derivable en todo \(\mathbb{R}\).</li>
</ul>

<p>
Ventajas:
</p>

<ul class="org-ul">
<li>Interpretación probabilística directa.</li>
<li>Históricamente muy usada en redes neuronales tempranas.</li>
</ul>

<p>
Desventajas:
</p>

<ul class="org-ul">
<li>Para \(|z|\) grandes, la función se <b>satura</b> cerca de 0 o 1:
<ul class="org-ul">
<li>la derivada es muy pequeña,</li>
<li>aparece el problema de <b>vanishing gradient</b>,</li>
<li>el entrenamiento de redes profundas se vuelve lento o ineficaz.</li>
</ul></li>
</ul>

<p>
Uso típico actual:
</p>

<ul class="org-ul">
<li>Capa de salida en algunos modelos de clasificación binaria (aunque
en deep learning moderno también se usan otras combinaciones).</li>
</ul>
</div>
</li>
<li><a id="org8aaf227"></a>Tangente hiperbólica (tanh)<br />
<div class="outline-text-5" id="text-org8aaf227">
<p>
\[
f(z) = \tanh(z)
\]
</p>

<p>
Características:
</p>

<ul class="org-ul">
<li>Salida en el intervalo \((-1, 1)\).</li>
<li>Es simétrica alrededor de 0 (centrada en cero).</li>
</ul>

<p>
Ventajas:
</p>

<ul class="org-ul">
<li>En comparación con la sigmoide logística, su salida centrada en cero:
<ul class="org-ul">
<li>puede favorecer una convergencia algo más rápida,</li>
<li>reduce ciertos sesgos en las activaciones.</li>
</ul></li>
</ul>

<p>
Desventajas:
</p>

<ul class="org-ul">
<li>También se satura para \(|z|\) grandes.</li>
<li>Sigue presentando <b>vanishing gradient</b> en redes muy profundas.</li>
</ul>

<p>
Uso clásico:
</p>

<ul class="org-ul">
<li>Capas ocultas en redes no demasiado profundas, antes de la adopción
masiva de ReLU.</li>
</ul>
</div>
</li>
</ul>
</div>
<div id="outline-container-orgaf0135f" class="outline-4">
<h4 id="orgaf0135f">Funciones de activación modernas</h4>
<div class="outline-text-4" id="text-orgaf0135f">
</div>
<ul class="org-ul">
<li><a id="org0fccafc"></a>ReLU (Rectified Linear Unit)<br />
<div class="outline-text-5" id="text-org0fccafc">
<p>
\[
f(z) = \max(0, z)
\]
</p>

<p>
Características:
</p>

<ul class="org-ul">
<li>Función lineal por partes:
<ul class="org-ul">
<li>para \(z < 0\): salida 0,</li>
<li>para \(z \ge 0\): salida \(z\).</li>
</ul></li>
<li>Muy sencilla y eficiente de calcular.</li>
</ul>

<p>
Ventajas:
</p>

<ul class="org-ul">
<li>No se satura para valores positivos grandes (derivada constante 1).</li>
<li>Reduce considerablemente el problema del <b>vanishing gradient</b>.</li>
<li>Ha permitido entrenar redes profundas con muchas capas.</li>
</ul>

<p>
Desventajas:
</p>

<ul class="org-ul">
<li><b>Dead neurons</b>: si una neurona recibe valores de \(z\) siempre
negativos, su salida es siempre 0 y el gradiente puede quedar en 0
→ deja de aprender.</li>
</ul>

<p>
Uso típico:
</p>

<ul class="org-ul">
<li>Función estándar en <b>capas ocultas</b> de MLP y CNN modernos.</li>
</ul>
</div>
</li>
<li><a id="orgeabbe46"></a>Leaky ReLU<br />
<div class="outline-text-5" id="text-orgeabbe46">
<p>
\[
f(z) =
</p>
\begin{cases}
z & \text{si } z \ge 0 \\
\alpha z & \text{si } z < 0
\end{cases}
<p>
\]
</p>

<p>
donde \(\alpha\) es un pequeño número positivo (por ejemplo, 0.01).
</p>

<p>
Características:
</p>

<ul class="org-ul">
<li>Variante de ReLU que, en lugar de “apagar” totalmente la neurona para
\(z < 0\), permite un pequeño gradiente negativo.</li>
</ul>

<p>
Ventajas:
</p>

<ul class="org-ul">
<li>Reduce el problema de neuronas muertas (<b>dead ReLU</b>).</li>
<li>Mantiene muchas ventajas de ReLU.</li>
</ul>

<p>
Uso típico:
</p>

<ul class="org-ul">
<li>Alternativa a ReLU cuando se observa que muchas neuronas quedan
inactivas de manera permanente.</li>
</ul>
</div>
</li>
<li><a id="org3da542b"></a>ELU (Exponential Linear Unit)<br />
<div class="outline-text-5" id="text-org3da542b">
<p>
\[
f(z) =
</p>
\begin{cases}
z & \text{si } z \ge 0 \\
\alpha (e^{z} - 1) & \text{si } z < 0
\end{cases}
<p>
\]
</p>

<p>
Características:
</p>

<ul class="org-ul">
<li>Para \(z \ge 0\): igual que ReLU.</li>
<li>Para \(z < 0\): decae de forma exponencial, con salida negativa acotada.</li>
</ul>

<p>
Ventajas:
</p>

<ul class="org-ul">
<li>Puede acelerar la convergencia en ciertos casos.</li>
<li>Las salidas negativas ayudan a centrar la activación alrededor de 0,
lo que puede mejorar la propagación del gradiente.</li>
</ul>

<p>
Uso típico:
</p>

<ul class="org-ul">
<li>Capas ocultas, en algunos modelos donde se ha observado mejor
rendimiento que con ReLU estándar.</li>
</ul>
</div>
</li>
</ul>
</div>
<div id="outline-container-orgff27281" class="outline-4">
<h4 id="orgff27281">Funciones de activación en la capa de salida</h4>
<div class="outline-text-4" id="text-orgff27281">
<p>
La función de activación de la <b>capa de salida</b> depende del tipo de
problema:
</p>
</div>
<ul class="org-ul">
<li><a id="org9cd33f5"></a>Clasificación binaria<br />
<div class="outline-text-5" id="text-org9cd33f5">
<p>
Se usa típicamente una función <b>sigmoide</b> para obtener una probabilidad
en \((0,1)\):
</p>

<p>
\[
\hat{y} = \sigma(z) \in (0,1)
\]
</p>

<p>
donde \(\hat{y}\) se interpreta como la probabilidad de clase “1”.
</p>
</div>
</li>
<li><a id="org0bbc1a9"></a>Clasificación multiclase (mutuamente excluyentes)<br />
<div class="outline-text-5" id="text-org0bbc1a9">
<p>
Se usa la función <b>softmax</b>:
</p>

<p>
\[
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}
\]
</p>

<ul class="org-ul">
<li>Convierte un vector de puntajes \(\mathbf{z}\) en un vector de
probabilidades que:
<ul class="org-ul">
<li>son positivas,</li>
<li>y suman 1.</li>
</ul></li>
</ul>
</div>
</li>
<li><a id="orgdd7d8f2"></a>Regresión<br />
<div class="outline-text-5" id="text-orgdd7d8f2">
<ul class="org-ul">
<li>Se usa generalmente una función de activación <b>lineal</b> en la salida:
\[
    f(z) = z
  \]</li>
<li>La red puede entonces producir valores reales sin restricción de
rango (o con restricciones adicionales impuestas por el preprocesado).</li>
</ul>
</div>
</li>
</ul>
</div>
<div id="outline-container-org6f63e25" class="outline-4">
<h4 id="org6f63e25">Impacto en el entrenamiento</h4>
<div class="outline-text-4" id="text-org6f63e25">
<p>
La elección de la función de activación afecta:
</p>

<ul class="org-ul">
<li>La <b>velocidad de convergencia</b>.</li>
<li>La <b>estabilidad</b> del entrenamiento.</li>
<li>El <b>flujo del gradiente</b> a través de las capas.</li>
<li>La capacidad de representar ciertas distribuciones de salida.</li>
</ul>

<p>
Una elección inadecuada:
</p>

<ul class="org-ul">
<li>puede causar vanishing/exploding gradient,</li>
<li>puede impedir que la red aprenda adecuadamente,</li>
<li>o requerir tiempos de entrenamiento excesivos.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgf0b7f33" class="outline-3">
<h3 id="orgf0b7f33">Propagación hacia adelante (Forward Propagation)</h3>
<div class="outline-text-3" id="text-orgf0b7f33">
<p>
La <b>propagación hacia adelante</b> es el proceso mediante el cual la red
calcula su salida a partir de una entrada dada.
</p>

<p>
Pasos generales:
</p>

<ol class="org-ol">
<li>Se recibe el vector de entrada \(\mathbf{x}\).</li>
<li>Se calcula sucesivamente:
<ul class="org-ul">
<li>\(\mathbf{z}^{(1)}\), \(\mathbf{a}^{(1)}\) (primera capa oculta),</li>
<li>\(\mathbf{z}^{(2)}\), \(\mathbf{a}^{(2)}\), etc.</li>
</ul></li>
<li>Se obtiene la activación de la capa de salida \(\mathbf{a}^{(L)}\),
que corresponde a la predicción \(\hat{\mathbf{y}}\).</li>
</ol>

<p>
Podemos verlo como una composición de funciones:
</p>

<p>
\[
\mathbf{a}^{(L)} = f^{(L)}\left( \mathbf{W}^{(L)} \, f^{(L-1)}\left( \dots f^{(1)}\left(\mathbf{W}^{(1)} \mathbf{x} + \mathbf{b}^{(1)}\right) \dots \right) + \mathbf{b}^{(L)} \right)
\]
</p>

<p>
o, de forma más compacta:
</p>

<p>
\[
\hat{\mathbf{y}} = F(\mathbf{x}; \theta)
\]
</p>

<p>
donde \(\theta\) denota el conjunto de todos los pesos y bias de la red.
</p>
</div>
</div>
<div id="outline-container-orge935b49" class="outline-3">
<h3 id="orge935b49">Funciones de pérdida</h3>
<div class="outline-text-3" id="text-orge935b49">
<p>
Las funciones de pérdida (o <b>funciones de coste</b>) miden el error entre:
</p>

<ul class="org-ul">
<li>la predicción del modelo \(\hat{y}\) (o \(\hat{\mathbf{y}}\)),</li>
<li>y el valor real \(y\) (o \(\mathbf{y}\)).</li>
</ul>

<p>
El objetivo del entrenamiento es encontrar parámetros \(\theta\) que
<b>minimicen</b> la pérdida promedio sobre el conjunto de entrenamiento.
</p>
</div>
<div id="outline-container-org7a0dcbd" class="outline-4">
<h4 id="org7a0dcbd">Error cuadrático medio (MSE)</h4>
<div class="outline-text-4" id="text-org7a0dcbd">
<p>
Común en problemas de regresión:
</p>

<p>
\[
\mathcal{L}_{\text{MSE}} = \frac{1}{n} \sum_{k=1}^{n} (y_k - \hat{y}_k)^2
\]
</p>

<p>
donde:
</p>

<ul class="org-ul">
<li>\(n\) es el número de muestras,</li>
<li>\(y_k\) es el valor real de la muestra \(k\),</li>
<li>\(\hat{y}_k\) es la predicción del modelo.</li>
</ul>
</div>
</div>
<div id="outline-container-org444ae6c" class="outline-4">
<h4 id="org444ae6c">Entropía cruzada (Cross-Entropy)</h4>
<div class="outline-text-4" id="text-org444ae6c">
<p>
Muy utilizada en clasificación.
</p>

<ul class="org-ul">
<li>Para clasificación binaria:</li>
</ul>

<p>
\[
\mathcal{L}_{\text{binaria}} = - \frac{1}{n} \sum_{k=1}^{n} \left[ y_k \log(\hat{y}_k) + (1 - y_k)\log(1 - \hat{y}_k) \right]
\]
</p>

<ul class="org-ul">
<li>Para clasificación multiclase (one-hot \(\mathbf{y}\) y softmax \(\hat{\mathbf{y}}\)):</li>
</ul>

<p>
\[
\mathcal{L}_{\text{multiclase}} = - \frac{1}{n} \sum_{k=1}^{n} \sum_{c=1}^{C} y_{k,c} \log(\hat{y}_{k,c})
\]
</p>

<p>
donde:
</p>

<ul class="org-ul">
<li>\(C\) es el número de clases,</li>
<li>\(y_{k,c}\) es 1 si la muestra \(k\) pertenece a la clase \(c\), y 0 en caso contrario,</li>
<li>\(\hat{y}_{k,c}\) es la probabilidad predicha para la clase \(c\).</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org8475fcb" class="outline-3">
<h3 id="org8475fcb">Backpropagation</h3>
<div class="outline-text-3" id="text-org8475fcb">
<p>
<b>Backpropagation</b> es el algoritmo fundamental para entrenar MLP. Se
basa en la aplicación sistemática de la <b>regla de la cadena</b> del cálculo
diferencial para propagar el error desde la capa de salida hacia las
capas anteriores.
</p>

<p>
Para un peso genérico \(w\), la derivada de la pérdida se expresa como:
</p>

<p>
\[
\frac{\partial \mathcal{L}}{\partial w} =
\frac{\partial \mathcal{L}}{\partial a}
\cdot \frac{\partial a}{\partial z}
\cdot \frac{\partial z}{\partial w}
\]
</p>

<p>
donde:
</p>

<ul class="org-ul">
<li>\(\frac{\partial \mathcal{L}}{\partial a}\): cuánto cambia la pérdida
si cambia la activación \(a\),</li>
<li>\(\frac{\partial a}{\partial z}\): deriva de la función de activación,</li>
<li>\(\frac{\partial z}{\partial w}\): relación lineal entre \(z\) y el
peso \(w\).</li>
</ul>

<p>
El algoritmo:
</p>

<ol class="org-ol">
<li>Calcula un <b>forward pass</b> (de entrada a salida).</li>
<li>Calcula la pérdida \(\mathcal{L}\).</li>
<li>Propaga gradientes hacia atrás usando la regla de la cadena.</li>
<li>Obtiene \(\frac{\partial \mathcal{L}}{\partial w}\) y
\(\frac{\partial \mathcal{L}}{\partial b}\) para todos los pesos y
bias de la red.</li>
<li>Usa estos gradientes para actualizar los parámetros.</li>
</ol>
</div>
</div>
<div id="outline-container-orgcfa3523" class="outline-3">
<h3 id="orgcfa3523">Descenso de gradiente</h3>
<div class="outline-text-3" id="text-orgcfa3523">
<p>
El <b>descenso de gradiente</b> (gradient descent) es la regla de
actualización básica para minimizar la función de pérdida.
</p>

<p>
Para un peso \(w\):
</p>

<p>
\[
w := w - \eta \, \frac{\partial \mathcal{L}}{\partial w}
\]
</p>

<p>
donde:
</p>

<ul class="org-ul">
<li>\(\eta\) es la <b>tasa de aprendizaje</b> (learning rate),</li>
<li>\(\frac{\partial \mathcal{L}}{\partial w}\) es el gradiente de la
pérdida respecto a \(w\).</li>
</ul>

<p>
Interpretación:
</p>

<ul class="org-ul">
<li>Nos movemos en la dirección <b>opuesta</b> al gradiente (descenso),</li>
<li>La magnitud del paso está controlada por \(\eta\).</li>
</ul>

<p>
En la práctica:
</p>

<ul class="org-ul">
<li>Se usan variantes como:
<ul class="org-ul">
<li><b>Stochastic Gradient Descent (SGD)</b>,</li>
<li>SGD con <b>momentum</b>,</li>
<li><b>Adam</b>, <b>RMSProp</b>, etc.</li>
</ul></li>
<li>Estas variantes mejoran:
<ul class="org-ul">
<li>la velocidad de convergencia,</li>
<li>la estabilidad numérica,</li>
<li>la capacidad de escapar de mínimos locales poco profundos.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orga982b53" class="outline-2">
<h2 id="orga982b53">Ejemplo Clasificación del dataset Iris con MLP</h2>
<div class="outline-text-2" id="text-orga982b53">
<p>
Este apartado muestra un ejemplo completo en Python usando el dataset
clásico Iris y una red neuronal feedforward (Perceptrón Multicapa)
usando la librería scikit-learn.
</p>

<p>
El objetivo es clasificar flores Iris en tres clases:
</p>

<ul class="org-ul">
<li>Setosa</li>
<li>Versicolor</li>
<li>Virginica</li>
</ul>


<p>
Importar librerías
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">from</span> sklearn.datasets <span style="color: #a020f0;">import</span> load_iris 
<span style="color: #a020f0;">from</span> sklearn.model_selection <span style="color: #a020f0;">import</span> train_test_split 
<span style="color: #a020f0;">from</span> sklearn.preprocessing <span style="color: #a020f0;">import</span> StandardScaler 
<span style="color: #a020f0;">from</span> sklearn.neural_network <span style="color: #a020f0;">import</span> MLPClassifier 
<span style="color: #a020f0;">from</span> sklearn.metrics <span style="color: #a020f0;">import</span> accuracy_score, classification_report
</pre>
</div>

<p>
Cargar el dataset Iris
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">iris</span> = load_iris<span style="color: #707183;">()</span>
<span style="color: #a0522d;">X</span> = iris.data <span style="color: #b22222;"># </span><span style="color: #b22222;">Caracter&#237;sticas (150 x 4) y = iris.target # Clases (150,)
</span><span style="color: #483d8b;">print</span><span style="color: #707183;">(</span>X.shape<span style="color: #707183;">)</span> <span style="color: #483d8b;">print</span><span style="color: #707183;">(</span>y.shape<span style="color: #707183;">)</span>
</pre>
</div>

<p>
Las 4 características son:
</p>

<ul class="org-ul">
<li>Largo del sépalo</li>
<li>Ancho del sépalo</li>
<li>Largo del pétalo</li>
<li>Ancho del pétalo</li>
</ul>

<p>
Las clases están codificadas como:
</p>

<ul class="org-ul">
<li>0 → Setosa</li>
<li>1 → Versicolor</li>
<li>2 → Virginica</li>
</ul>

<p>
El siguiente fragmento de código realiza la partición del conjunto de
datos original en dos subconjuntos disjuntos: uno destinado al
entrenamiento del modelo y otro a su evaluación.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">X_train</span>, <span style="color: #a0522d;">X_test</span>, <span style="color: #a0522d;">y_train</span>, <span style="color: #a0522d;">y_test</span> = train_test_split<span style="color: #707183;">(</span> X, y, test_size=0.2, random_state=42, stratify=y <span style="color: #707183;">)</span>
</pre>
</div>

<p>
Propósito de la partición:
</p>

<p>
En aprendizaje automático supervisado, es fundamental evaluar la
capacidad de generalización de un modelo. Para ello, los datos
disponibles se dividen en:
</p>

<ul class="org-ul">
<li><b>Conjunto de entrenamiento</b>: utilizado para ajustar los parámetros del modelo (pesos y sesgos).</li>
<li><b>Conjunto de prueba</b>: utilizado exclusivamente para medir el desempeño del modelo sobre datos no vistos durante el entrenamiento.</li>
</ul>

<p>
Esta separación permite detectar fenómenos como overfitting y
underfitting, y proporciona una estimación más realista del
rendimiento esperado en producción.
</p>

<p>
<b>El parámetro</b>:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">test_size</span>=0.2
</pre>
</div>
<p>
indica que el 20 % del conjunto total de datos se reserva para el
conjunto de prueba, mientras que el 80 % restante se utiliza para
entrenamiento. Esta proporción es común en la práctica y representa un
compromiso razonable entre:
</p>

<ul class="org-ul">
<li>disponer de suficientes datos para entrenar el modelo, y</li>
<li>contar con una muestra representativa para evaluar su generalización.</li>
</ul>
</div>
<div id="outline-container-org9025bea" class="outline-3">
<h3 id="org9025bea">Reproducibilidad del experimento</h3>
<div class="outline-text-3" id="text-org9025bea">
<p>
El parámetro:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">random_state</span>=42
</pre>
</div>

<p>
fija la semilla del generador de números aleatorios utilizado durante
la partición.  Esto garantiza que la división de los datos sea
reproducible, es decir, que múltiples ejecuciones del mismo código
produzcan exactamente la misma separación entre entrenamiento y
prueba.
</p>

<p>
La reproducibilidad es un requisito esencial en entornos científicos y
académicos, ya que permite validar y comparar resultados de manera
consistente.
</p>
</div>
</div>
<div id="outline-container-orga02ff76" class="outline-3">
<h3 id="orga02ff76">Estratificación de clases</h3>
<div class="outline-text-3" id="text-orga02ff76">
<p>
El parámetro:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">stratify</span>=y
</pre>
</div>

<p>
indica que la división debe realizarse de forma estratificada,
conservando la proporción original de cada clase en ambos
subconjuntos.
</p>

<p>
Esto es especialmente importante en problemas de clasificación, ya que
una división aleatoria sin estratificación podría provocar:
</p>

<ul class="org-ul">
<li>conjuntos de entrenamiento o prueba con clases desbalanceadas,</li>
<li>métricas de desempeño engañosas,</li>
<li>modelos que no aprendan adecuadamente clases minoritarias.</li>
</ul>

<p>
Mediante la estratificación, se asegura que tanto el conjunto de
entrenamiento como el de prueba sean representativos de la
distribución real de las clases.
</p>
</div>
</div>
<div id="outline-container-orgea407f1" class="outline-3">
<h3 id="orgea407f1">Importancia en redes neuronales</h3>
<div class="outline-text-3" id="text-orgea407f1">
<p>
En el contexto de redes neuronales (incluyendo MLP):
</p>

<ul class="org-ul">
<li>el entrenamiento ajusta los pesos minimizando una función de pérdida
sobre X<sub>train</sub>,</li>

<li>la evaluación sobre X<sub>test</sub> permite medir la capacidad del modelo
para generalizar,</li>

<li>una mala partición puede conducir a conclusiones erróneas sobre el
desempeño del modelo.</li>
</ul>

<p>
Por esta razón, la división adecuada del conjunto de datos constituye
un paso crítico previo al entrenamiento y no debe considerarse un
detalle menor de implementación.
</p>
</div>
</div>
<div id="outline-container-org9e61ba4" class="outline-3">
<h3 id="org9e61ba4">Normalización de los datos</h3>
<div class="outline-text-3" id="text-org9e61ba4">
<p>
La normalización de los datos de entrada es un paso crítico en el
entrenamiento de redes neuronales, ya que influye directamente en la
estabilidad numérica, la velocidad de convergencia y la eficacia del
aprendizaje.
</p>

<p>
En una red neuronal, los pesos se ajustan mediante métodos de
optimización basados en gradientes. Si las variables de entrada
presentan escalas muy diferentes, el proceso de entrenamiento puede
volverse ineficiente o inestable.
</p>

<p>
En muchos conjuntos de datos, las características pueden tener rangos muy distintos. Por ejemplo:
</p>

<ul class="org-ul">
<li>una variable puede tomar valores entre 0 y 1,</li>
<li>otra entre 0 y 10⁶,</li>
<li>otra puede incluir valores negativos.</li>
</ul>

<p>
Cuando estos datos se introducen directamente en una red neuronal:
</p>

<ul class="org-ul">
<li>los gradientes asociados a variables de gran escala dominan la actualización de los pesos,</li>
<li>las funciones de activación pueden saturarse,</li>
<li>el descenso por gradiente se vuelve lento o errático.</li>
</ul>

<p>
La normalización busca homogeneizar la escala de las entradas,
permitiendo que todas las características contribuyan de manera
equilibrada al aprendizaje.
</p>


<p>
Una de las técnicas más utilizadas es la estandarización, implementada
en scikit-learn mediante la clase StandardScaler.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">scaler</span> = StandardScaler<span style="color: #707183;">()</span>
<span style="color: #a0522d;">X_train</span> = scaler.fit_transform<span style="color: #707183;">(</span>X_train<span style="color: #707183;">)</span> <span style="color: #a0522d;">X_test</span> = scaler.transform<span style="color: #707183;">(</span>X_test<span style="color: #707183;">)</span>
</pre>
</div>

<p>
Este procedimiento transforma cada característica \(x\) según la siguiente expresión:
</p>

<p>
\[
x' = \frac{x - \mu}{\sigma}
\]
</p>

<p>
donde:
</p>

<ul class="org-ul">
<li>\( \mu \) es la media de la característica,</li>
<li>\( \sigma \) es la desviación estándar de la característica.</li>
</ul>

<p>
Como resultado de esta transformación, cada variable queda con:
</p>

<ul class="org-ul">
<li>media aproximadamente igual a 0,</li>
<li>desviación estándar igual a 1.</li>
</ul>
</div>
</div>
<div id="outline-container-org6433828" class="outline-3">
<h3 id="org6433828">Definir el Perceptrón Multicapa (MLP)</h3>
<div class="outline-text-3" id="text-org6433828">
<p>
El siguiente fragmento de código define un <b>Perceptrón Multicapa</b> (Multilayer Perceptron, MLP)
para un problema de clasificación multiclase, utilizando la implementación provista por
la biblioteca <b>scikit-learn</b>.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">mlp</span> = MLPClassifier<span style="color: #707183;">(</span>
<span style="background-color: #f2f2f2;"> </span>   hidden_layer_sizes=<span style="color: #7388d6;">(</span>10, 10<span style="color: #7388d6;">)</span>,
<span style="background-color: #f2f2f2;"> </span>   activation=<span style="color: #8b2252;">'relu'</span>,
<span style="background-color: #f2f2f2;"> </span>   solver=<span style="color: #8b2252;">'adam'</span>,
<span style="background-color: #f2f2f2;"> </span>   learning_rate_init=0.001,
<span style="background-color: #f2f2f2;"> </span>   max_iter=2000,
<span style="background-color: #f2f2f2;"> </span>   random_state=42
<span style="color: #707183;">)</span>
</pre>
</div>

<p>
Este modelo corresponde a una red neuronal <b>feedforward completamente conectada</b>,
entrenada mediante <b>backpropagation</b> y optimizada usando métodos de descenso por gradiente.
</p>
</div>
<div id="outline-container-org3730d02" class="outline-4">
<h4 id="org3730d02">Arquitectura de la red neuronal</h4>
<div class="outline-text-4" id="text-org3730d02">
<p>
La arquitectura del MLP queda determinada por el número de características de entrada,
las capas ocultas definidas explícitamente y el número de neuronas en la capa de salida.
</p>
</div>
</div>
<div id="outline-container-orgb5fb5e7" class="outline-4">
<h4 id="orgb5fb5e7">Capa de entrada</h4>
<div class="outline-text-4" id="text-orgb5fb5e7">
<ul class="org-ul">
<li>La capa de entrada está compuesta por <b>4 neuronas</b>.</li>
<li>Cada neurona representa una característica del vector de entrada.</li>
<li>Esta capa no aplica transformaciones, únicamente propaga los valores hacia la primera capa oculta.</li>
</ul>

<p>
Formalmente, el vector de entrada se expresa como:
</p>

<p>
\[
\mathbf{x} = (x_1, x_2, x_3, x_4)
\]
</p>
</div>
</div>
<div id="outline-container-org63eaffa" class="outline-4">
<h4 id="org63eaffa">Capas ocultas</h4>
<div class="outline-text-4" id="text-org63eaffa">
<p>
El parámetro:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">hidden_layer_sizes</span>=<span style="color: #707183;">(</span>10, 10<span style="color: #707183;">)</span>
</pre>
</div>

<p>
define <b>dos capas ocultas</b>, cada una con <b>10 neuronas</b>.
</p>

<p>
Características principales:
</p>

<ul class="org-ul">
<li>Son capas <b>densas (fully connected)</b>.</li>
<li>Cada neurona recibe como entrada la salida de <b>todas</b> las neuronas de la capa anterior.</li>
<li>Permiten aprender representaciones no lineales del espacio de características.</li>
</ul>

<p>
Cada capa oculta implementa la transformación:
</p>

<p>
\[
\mathbf{h}^{(l)} = f\left( \mathbf{W}^{(l)} \mathbf{h}^{(l-1)} + \mathbf{b}^{(l)} \right)
\]
</p>

<p>
donde:
</p>

<ul class="org-ul">
<li>\( \mathbf{W}^{(l)} \) es la matriz de pesos,</li>
<li>\( \mathbf{b}^{(l)} \) es el vector de sesgos,</li>
<li>\( f(\cdot) \) es la función de activación.</li>
</ul>
</div>
</div>
<div id="outline-container-org22e573d" class="outline-4">
<h4 id="org22e573d">Función de activación</h4>
<div class="outline-text-4" id="text-org22e573d">
<p>
El parámetro:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">activation</span>=<span style="color: #8b2252;">'relu'</span>
</pre>
</div>

<p>
indica el uso de la función <b>ReLU (Rectified Linear Unit)</b>, definida como:
</p>

<p>
\[
\text{ReLU}(z) = \max(0, z)
\]
</p>

<p>
Esta función se utiliza porque:
</p>

<ul class="org-ul">
<li>introduce no linealidad,</li>
<li>reduce el problema del <b>vanishing gradient</b>,</li>
<li>mejora la estabilidad del entrenamiento,</li>
<li>es computacionalmente eficiente.</li>
</ul>
</div>
</div>
<div id="outline-container-org0e249b3" class="outline-4">
<h4 id="org0e249b3">Capa de salida</h4>
<div class="outline-text-4" id="text-org0e249b3">
<ul class="org-ul">
<li>La capa de salida está compuesta por <b>3 neuronas</b>.</li>
<li>Cada neurona corresponde a una de las clases del problema.</li>
<li>Internamente, el modelo utiliza una función <b>softmax</b> para obtener probabilidades.</li>
</ul>

<p>
La clase predicha se obtiene mediante la operación <b>argmax</b> sobre las salidas.
</p>
</div>
</div>
<div id="outline-container-org828ab5a" class="outline-4">
<h4 id="org828ab5a">Entrenamiento del modelo</h4>
<div class="outline-text-4" id="text-org828ab5a">
<p>
El entrenamiento se realiza mediante el optimizador <b>Adam</b>:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">solver</span>=<span style="color: #8b2252;">'adam'</span>
</pre>
</div>

<p>
Adam combina:
</p>
<ul class="org-ul">
<li>momentum</li>
<li>tasas de aprendizaje adaptativas</li>
</ul>

<p>
lo que permite una convergencia más rápida y estable en redes con múltiples capas.
</p>

<p>
La tasa de aprendizaje inicial se define como:
</p>

<p>
\[
\eta = 0.001
\]
</p>

<p>
controlando el tamaño de los pasos durante la actualización de los pesos.
</p>
</div>
</div>
</div>
<div id="outline-container-orge103146" class="outline-3">
<h3 id="orge103146">Número de iteraciones</h3>
<div class="outline-text-3" id="text-orge103146">
<p>
El parámetro:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">max_iter</span>=2000
</pre>
</div>

<p>
establece el número máximo de épocas de entrenamiento.
Una época corresponde a un pase completo sobre el conjunto de entrenamiento.
</p>

<p>
Un número elevado de épocas favorece la convergencia, aunque incrementa el riesgo de <b>overfitting</b>.
</p>
</div>
</div>
<div id="outline-container-orge20e603" class="outline-3">
<h3 id="orge20e603">Reproducibilidad</h3>
<div class="outline-text-3" id="text-orge20e603">
<p>
El parámetro:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">random_state</span>=42
</pre>
</div>

<p>
fija la semilla del generador de números aleatorios, garantizando resultados reproducibles
en la inicialización de los pesos y el proceso de entrenamiento.
</p>
</div>
</div>
<div id="outline-container-org6e589ec" class="outline-3">
<h3 id="org6e589ec">Entrenamiento del modelo</h3>
<div class="outline-text-3" id="text-org6e589ec">
<p>
El entrenamiento deñ Perceptrón Multicapa (MLP) consiste en ajustar
los pesos y sesgos de la red para minimizar el error de predicción
sobre los datos de entrenamiento.
</p>

<div class="org-src-container">
<pre class="src src-python">mlp.fit<span style="color: #707183;">(</span>X_train, y_train<span style="color: #707183;">)</span>
</pre>
</div>

<p>
Durante este proceso ocurren los siguientes pasos fundamentales:
</p>
</div>
<div id="outline-container-org7fec327" class="outline-4">
<h4 id="org7fec327">Propagación hacia adelante (forward pass)</h4>
<div class="outline-text-4" id="text-org7fec327">
<p>
Cada muestra de entrada x atraviesa la red capa por capa.
</p>

<p>
En cada neurona se calcula una combinación lineal y se aplica una función de activación.
</p>

<p>
Se obtiene una salida y que representa la predicción del modelo.
</p>
</div>
</div>
<div id="outline-container-org28b16b6" class="outline-4">
<h4 id="org28b16b6">Cálculo del error</h4>
<div class="outline-text-4" id="text-org28b16b6">
<p>
La salida predicha se compara con la etiqueta real 𝑦.
Se utiliza una función de pérdida (por ejemplo, log-loss para clasificación multiclase).
</p>
</div>
</div>
<div id="outline-container-orgcec415a" class="outline-4">
<h4 id="orgcec415a">Backpropagation</h4>
<div class="outline-text-4" id="text-orgcec415a">
<p>
El error se propaga desde la capa de salida hacia las capas
anteriores. Se calculan los gradientes del error respecto a cada peso
y sesgo. Se aplica la regla de la cadena del cálculo diferencial.
</p>
</div>
</div>
<div id="outline-container-org0baa450" class="outline-4">
<h4 id="org0baa450">Descenso de gradiente con Adam</h4>
<div class="outline-text-4" id="text-org0baa450">
<ul class="org-ul">
<li>Adam combina Momentum y RMSProp.</li>
<li>Ajusta automáticamente la tasa de aprendizaje para cada parámetro.</li>
<li>Proporciona convergencia rápida y estable.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org121ba4e" class="outline-3">
<h3 id="org121ba4e">Evaluación del modelo</h3>
<div class="outline-text-3" id="text-org121ba4e">
<p>
Una vez entrenado el modelo, se evalúa su desempeño usando datos no vistos durante el entrenamiento.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">y_pred</span> = mlp.predict<span style="color: #707183;">(</span>X_test<span style="color: #707183;">)</span>
<span style="color: #483d8b;">print</span><span style="color: #707183;">(</span><span style="color: #8b2252;">"Accuracy:"</span>, accuracy_score<span style="color: #7388d6;">(</span>y_test, y_pred<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span> <span style="color: #483d8b;">print</span><span style="color: #707183;">(</span><span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Reporte de clasificaci&#243;n:</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">"</span><span style="color: #707183;">)</span> <span style="color: #483d8b;">print</span><span style="color: #707183;">(</span>classification_report<span style="color: #7388d6;">(</span>y_test, y_pred, target_names=iris.target_names<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>
</div>
<div id="outline-container-org43e7442" class="outline-4">
<h4 id="org43e7442">Métricas utilizadas</h4>
<div class="outline-text-4" id="text-org43e7442">
<ul class="org-ul">
<li>Accuracy: proporción total de predicciones correctas.</li>
<li>Precision: qué tan confiables son las predicciones positivas.</li>
<li>Recall: capacidad del modelo para encontrar todos los ejemplos de una clase.</li>
<li>F1-score: balance entre precisión y recall.</li>
</ul>

<p>
En el dataset Iris, el accuracy suele estar en el rango del 95% al
100%, debido a su baja dimensionalidad y buena separación entre
clases.
</p>
</div>
</div>
</div>
<div id="outline-container-org7842ef7" class="outline-3">
<h3 id="org7842ef7">Predicción con una nueva flor</h3>
<div class="outline-text-3" id="text-org7842ef7">
<p>
El modelo entrenado puede utilizarse para predecir nuevas muestras.
</p>

<div class="org-src-container">
<pre class="src src-python">
<span style="color: #b22222;">#</span><span style="color: #b22222;">[largo_sepalo, ancho_sepalo, largo_petalo, ancho_petalo]
</span>
<span style="color: #a0522d;">flor_nueva</span> = np.array<span style="color: #707183;">(</span><span style="color: #7388d6;">[</span><span style="color: #909183;">[</span>5.1, 3.5, 1.4, 0.2<span style="color: #909183;">]</span><span style="color: #7388d6;">]</span><span style="color: #707183;">)</span>
<span style="color: #b22222;">#</span><span style="color: #b22222;">Normalizaci&#243;n con el mismo scaler del entrenamiento
</span><span style="color: #a0522d;">flor_nueva</span> = scaler.transform<span style="color: #707183;">(</span>flor_nueva<span style="color: #707183;">)</span>
<span style="color: #a0522d;">prediccion</span> = mlp.predict<span style="color: #707183;">(</span>flor_nueva<span style="color: #707183;">)</span> <span style="color: #483d8b;">print</span><span style="color: #707183;">(</span><span style="color: #8b2252;">"Clase predicha:"</span>, iris.target_names<span style="color: #7388d6;">[</span>prediccion<span style="color: #909183;">[</span>0<span style="color: #909183;">]</span><span style="color: #7388d6;">]</span><span style="color: #707183;">)</span>
</pre>
</div>

<p>
Es fundamental aplicar la misma normalización usada en el entrenamiento para mantener coherencia en las escalas.
</p>
</div>
</div>
<div id="outline-container-org6c53864" class="outline-3">
<h3 id="org6c53864">Interpretación matemática del MLP</h3>
<div class="outline-text-3" id="text-org6c53864">
<p>
Cada neurona de la red realiza el siguiente cálculo:
</p>

<pre class="example" id="orgfcdd098">
z = w · x + b a = f(z)
</pre>

<p>
Donde: 𝑤 es el vector de pesos 𝑥 es el vector de entradas 𝑏 es el sesgo (bias) 𝑓
es la función de activación (ReLU en las capas ocultas). La capa de salida utiliza la función Softmax:
</p>

<div class="latex" id="org6aeed83">
<p>
\text{softmax}(z<sub>i</sub>) = \frac{e<sup>z<sub>i</sub></sup>}{&sum;<sub>j</sub> e<sup>z<sub>j</sub></sup>}
</p>

</div>

<p>
Esto permite interpretar las salidas como probabilidades para cada clase.
</p>
</div>
</div>
</div>
<div id="outline-container-org301fabc" class="outline-2">
<h2 id="org301fabc">Ejemplo Clasificación de Cáncer de Mama con MLP</h2>
<div class="outline-text-2" id="text-org301fabc">
<p>
Este apartado muestra un ejemplo completo en Python usando el dataset
<b>Breast Cancer Wisconsin (Diagnostic)</b> y un <b>Perceptrón Multicapa (MLP)</b>
usando la librería scikit-learn.
</p>

<p>
El objetivo es clasificar tumores en dos categorías:
</p>

<ul class="org-ul">
<li>0 → Maligno</li>
<li>1 → Benigno</li>
</ul>

<p>
Este problema es un caso típico de <b><b>clasificación binaria</b></b> en el
ámbito médico, donde es crucial:
</p>

<ul class="org-ul">
<li>minimizar falsos negativos (tumor maligno clasificado como benigno),</li>
<li>mantener una buena precisión general del modelo.</li>
</ul>
</div>
<div id="outline-container-org5462d3d" class="outline-3">
<h3 id="org5462d3d">Importar librerías</h3>
<div class="outline-text-3" id="text-org5462d3d">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">from</span> sklearn.datasets <span style="color: #a020f0;">import</span> load_breast_cancer
<span style="color: #a020f0;">from</span> sklearn.model_selection <span style="color: #a020f0;">import</span> train_test_split
<span style="color: #a020f0;">from</span> sklearn.preprocessing <span style="color: #a020f0;">import</span> StandardScaler
<span style="color: #a020f0;">from</span> sklearn.neural_network <span style="color: #a020f0;">import</span> MLPClassifier
<span style="color: #a020f0;">from</span> sklearn.metrics <span style="color: #a020f0;">import</span> accuracy_score, classification_report
</pre>
</div>

<p>
En este bloque:
</p>

<ul class="org-ul">
<li><code>load_breast_cancer</code>: carga el dataset de cáncer de mama.</li>
<li><code>train_test_split</code>: divide en entrenamiento y prueba.</li>
<li><code>StandardScaler</code>: aplica estandarización a las características.</li>
<li><code>MLPClassifier</code>: implementa un Perceptrón Multicapa (MLP).</li>
<li><code>accuracy_score</code> y <code>classification_report</code>: permiten evaluar el modelo.</li>
</ul>
</div>
</div>
<div id="outline-container-org807d152" class="outline-3">
<h3 id="org807d152">Cargar el dataset Breast Cancer</h3>
<div class="outline-text-3" id="text-org807d152">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">data</span> = load_breast_cancer<span style="color: #707183;">()</span>
<span style="color: #a0522d;">X</span> = data.data      <span style="color: #b22222;"># </span><span style="color: #b22222;">Caracter&#237;sticas (569 x 30)
</span><span style="color: #a0522d;">y</span> = data.target    <span style="color: #b22222;"># </span><span style="color: #b22222;">Etiquetas (569,)
</span>
<span style="color: #483d8b;">print</span><span style="color: #707183;">(</span><span style="color: #8b2252;">"Dimensiones de X:"</span>, X.shape<span style="color: #707183;">)</span>
<span style="color: #483d8b;">print</span><span style="color: #707183;">(</span><span style="color: #8b2252;">"Clases num&#233;ricas:"</span>, np.unique<span style="color: #7388d6;">(</span>y<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #483d8b;">print</span><span style="color: #707183;">(</span><span style="color: #8b2252;">"Nombres de clases:"</span>, data.target_names<span style="color: #707183;">)</span>
</pre>
</div>

<p>
Este dataset contiene:
</p>

<ul class="org-ul">
<li>569 muestras (pacientes).</li>
<li>30 características numéricas que describen propiedades de los tumores
(radio, textura, perímetro, área, suavidad, compacidad, etc.).</li>
<li>2 clases:
<ul class="org-ul">
<li>0 → malignant (maligno)</li>
<li>1 → benign (benigno)</li>
</ul></li>
</ul>

<p>
Cada muestra corresponde a una imagen de tejido mamario analizada
digitalmente. Las 30 características se derivan de medidas estadísticas
de la forma, textura y estructura del tumor.
</p>
</div>
<div id="outline-container-org01fd31f" class="outline-4">
<h4 id="org01fd31f">Importancia del dominio</h4>
<div class="outline-text-4" id="text-org01fd31f">
<p>
En aplicaciones médicas, la interpretación de las predicciones es muy
sensible:
</p>

<ul class="org-ul">
<li>un <b>falso negativo</b> (maligno etiquetado como benigno) puede retrasar
un diagnóstico crítico;</li>
<li>un <b>falso positivo</b> (benigno etiquetado como maligno) puede generar
ansiedad y procedimientos innecesarios.</li>
</ul>

<p>
Por ello, además de la <b>accuracy</b>, suelen analizarse:
</p>

<ul class="org-ul">
<li>sensibilidad (<b>recall</b>) para la clase “maligno”,</li>
<li>especificidad,</li>
<li>matriz de confusión, etc.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgcb899b9" class="outline-3">
<h3 id="orgcb899b9">Partición del conjunto de datos</h3>
<div class="outline-text-3" id="text-orgcb899b9">
<p>
Dividimos el dataset en entrenamiento y prueba:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">X_train</span>, <span style="color: #a0522d;">X_test</span>, <span style="color: #a0522d;">y_train</span>, <span style="color: #a0522d;">y_test</span> = train_test_split<span style="color: #707183;">(</span>
<span style="background-color: #f2f2f2;"> </span>   X,
<span style="background-color: #f2f2f2;"> </span>   y,
<span style="background-color: #f2f2f2;"> </span>   test_size=0.3,
<span style="background-color: #f2f2f2;"> </span>   random_state=42,
<span style="background-color: #f2f2f2;"> </span>   stratify=y
<span style="color: #707183;">)</span>
</pre>
</div>
</div>
<div id="outline-container-orgfd2a90d" class="outline-4">
<h4 id="orgfd2a90d">Significado de los parámetros</h4>
<div class="outline-text-4" id="text-orgfd2a90d">
<ul class="org-ul">
<li><code>test_size=0.3</code>:
<ul class="org-ul">
<li>30 % de los datos se reservan para el <b>conjunto de prueba</b>.</li>
<li>70 % se usan para entrenar el modelo.</li>
<li>En este caso:
<ul class="org-ul">
<li>Entrenamiento: ≈ 398 muestras.</li>
<li>Prueba: ≈ 171 muestras.</li>
</ul></li>
</ul></li>

<li><code>random_state=42</code>:
<ul class="org-ul">
<li>Fija la semilla del generador de números aleatorios.</li>
<li>Permite que la partición sea reproducible: el mismo código genera
siempre la misma división.</li>
</ul></li>

<li><code>stratify=y</code>:
<ul class="org-ul">
<li>Asegura que la proporción de clases (maligno/benigno) sea similar
tanto en entrenamiento como en prueba.</li>
<li>Esto es crucial en problemas médicos, donde el balance de clases
puede influir fuertemente en las métricas.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgf8e5d55" class="outline-4">
<h4 id="orgf8e5d55">Propósito de la partición</h4>
<div class="outline-text-4" id="text-orgf8e5d55">
<p>
En aprendizaje automático supervisado:
</p>

<ul class="org-ul">
<li>El <b>conjunto de entrenamiento</b> se usa para ajustar los parámetros
(pesos y sesgos) del modelo.</li>
<li>El <b>conjunto de prueba</b> se usa <b>solo</b> para evaluar la capacidad de
generalización sobre datos no vistos.</li>
</ul>

<p>
Esta separación es esencial para:
</p>

<ul class="org-ul">
<li>detectar <b>overfitting</b> (el modelo memoriza el entrenamiento pero
generaliza mal),</li>
<li>obtener una estimación realista del rendimiento.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org387d5ce" class="outline-3">
<h3 id="org387d5ce">Normalización (Estandarización)</h3>
<div class="outline-text-3" id="text-org387d5ce">
<p>
Las características tienen escalas muy diferentes (áreas grandes,
texturas pequeñas, etc.). Esto puede causar:
</p>

<ul class="org-ul">
<li>problemas numéricos,</li>
<li>gradientes desbalanceados,</li>
<li>convergencia lenta o inestable.</li>
</ul>

<p>
Por ello, aplicamos estandarización:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">scaler</span> = StandardScaler<span style="color: #707183;">()</span>
<span style="color: #a0522d;">X_train</span> = scaler.fit_transform<span style="color: #707183;">(</span>X_train<span style="color: #707183;">)</span>
<span style="color: #a0522d;">X_test</span> = scaler.transform<span style="color: #707183;">(</span>X_test<span style="color: #707183;">)</span>
</pre>
</div>

<p>
Este procedimiento transforma cada característica \(x\) según:
</p>

<div class="latex" id="orgf3a852d">
<p>
x' = \frac{x - \mu}{\sigma}
</p>

</div>

<p>
donde:
</p>

<ul class="org-ul">
<li>\(\mu\) es la media de la característica calculada sobre el conjunto
de entrenamiento,</li>
<li>\(\sigma\) es la desviación estándar de dicha característica.</li>
</ul>

<p>
Después de la transformación:
</p>

<ul class="org-ul">
<li>cada variable tiene media aproximadamente 0,</li>
<li>y desviación estándar aproximadamente 1.</li>
</ul>
</div>
<div id="outline-container-org9d1cac6" class="outline-4">
<h4 id="org9d1cac6">Importante</h4>
<div class="outline-text-4" id="text-org9d1cac6">
<ul class="org-ul">
<li>Solo se llama a <code>fit_transform</code> en <code>X_train</code>:
<ul class="org-ul">
<li><code>fit</code> calcula \(\mu\) y \(\sigma\) usando <b>solo entrenamiento</b>.</li>
<li><code>transform</code> aplica la transformación.</li>
</ul></li>
<li>Para <code>X_test</code> se usa únicamente <code>transform</code>:
<ul class="org-ul">
<li>se reescalan los datos de prueba con los mismos parámetros
(\(\mu\) y \(\sigma\)) obtenidos del entrenamiento.</li>
</ul></li>
<li>Esto evita <b>filtrar información del conjunto de prueba</b> hacia el
proceso de entrenamiento (data leakage).</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org1aff302" class="outline-3">
<h3 id="org1aff302">Definición del Perceptrón Multicapa (MLP)</h3>
<div class="outline-text-3" id="text-org1aff302">
<p>
Definimos un MLP para clasificación binaria:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">mlp</span> = MLPClassifier<span style="color: #707183;">(</span>
<span style="background-color: #f2f2f2;"> </span>   hidden_layer_sizes=<span style="color: #7388d6;">(</span>30, 15<span style="color: #7388d6;">)</span>,
<span style="background-color: #f2f2f2;"> </span>   activation=<span style="color: #8b2252;">'relu'</span>,
<span style="background-color: #f2f2f2;"> </span>   solver=<span style="color: #8b2252;">'adam'</span>,
<span style="background-color: #f2f2f2;"> </span>   max_iter=1000,
<span style="background-color: #f2f2f2;"> </span>   random_state=42
<span style="color: #707183;">)</span>
</pre>
</div>
</div>
<div id="outline-container-org5d317ee" class="outline-4">
<h4 id="org5d317ee">Arquitectura del modelo</h4>
<div class="outline-text-4" id="text-org5d317ee">
<p>
El MLP es una red neuronal <b>feedforward</b> completamente conectada.
</p>

<ul class="org-ul">
<li><b>Capa de entrada</b>:
<ul class="org-ul">
<li>Tiene tantas neuronas como características de entrada: 30.</li>
<li>Cada componente del vector \(\mathbf{x} \in \mathbb{R}^{30}\)
representa una característica del tumor.</li>
</ul></li>

<li><p>
<b>Capas ocultas</b>:
</p>
<ul class="org-ul">
<li>Primer capa oculta: 30 neuronas.</li>
<li>Segunda capa oculta: 15 neuronas.</li>
<li><p>
Se especifican con el parámetro:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">hidden_layer_sizes</span> = <span style="color: #707183;">(</span>30, 15<span style="color: #707183;">)</span>
</pre>
</div></li>
</ul>

<p>
Cada neurona en una capa oculta:
</p>
<ul class="org-ul">
<li>recibe como entrada la salida de todas las neuronas de la capa
anterior (capas densas),</li>
<li>aplica una transformación lineal seguida de una función de
activación no lineal.</li>
</ul></li>

<li><b>Capa de salida</b>:
<ul class="org-ul">
<li>Para clasificación binaria, internamente el MLP de scikit-learn
puede usar:
<ul class="org-ul">
<li>1 neurona con activación logística (sigmoide),</li>
<li>o una representación equivalente a 2 clases.</li>
</ul></li>
<li>La salida se interpreta como una probabilidad de pertenecer a la
clase “1” (benigno).</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org7269d85" class="outline-4">
<h4 id="org7269d85">Función de activación ReLU</h4>
<div class="outline-text-4" id="text-org7269d85">
<p>
El parámetro:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">activation</span> = <span style="color: #8b2252;">'relu'</span>
</pre>
</div>

<p>
indica el uso de la función <b>ReLU (Rectified Linear Unit)</b> en las capas
ocultas:
</p>

<div class="latex" id="orgc4ee903">
<p>
\text{ReLU}(z) = max(0, z)
</p>

</div>

<p>
Ventajas de ReLU:
</p>

<ul class="org-ul">
<li>Introduce no linealidad (permite aproximar funciones no lineales).</li>
<li>Reduce el problema del <b>vanishing gradient</b> en comparación con
funciones como sigmoide o tanh.</li>
<li>Es simple y eficiente de calcular.</li>
</ul>
</div>
</div>
<div id="outline-container-org3d3fc36" class="outline-4">
<h4 id="org3d3fc36">Optimizador Adam</h4>
<div class="outline-text-4" id="text-org3d3fc36">
<p>
El parámetro:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">solver</span> = <span style="color: #8b2252;">'adam'</span>
</pre>
</div>

<p>
selecciona el optimizador <b>Adam</b> (Adaptive Moment Estimation), que:
</p>

<ul class="org-ul">
<li>combina ideas de Momentum y RMSProp,</li>
<li>mantiene promedios móviles de:
<ul class="org-ul">
<li>los gradientes,</li>
<li>los gradientes al cuadrado,</li>
</ul></li>
<li>adapta la tasa de aprendizaje para cada parámetro.</li>
</ul>

<p>
Esto suele producir:
</p>

<ul class="org-ul">
<li>convergencia rápida,</li>
<li>estabilidad numérica,</li>
<li>buen desempeño sin necesidad de mucha sintonía manual.</li>
</ul>
</div>
</div>
<div id="outline-container-orgf9cbc31" class="outline-4">
<h4 id="orgf9cbc31">Número máximo de iteraciones</h4>
<div class="outline-text-4" id="text-orgf9cbc31">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">max_iter</span> = 1000
</pre>
</div>

<ul class="org-ul">
<li>Indica el número máximo de épocas (o iteraciones sobre los datos)
para entrenar la red.</li>
<li>Un valor alto permite que el modelo converja, aunque si es excesivo
puede aumentar el riesgo de <b>overfitting</b> (la red se ajusta demasiado
al entrenamiento).</li>
</ul>
</div>
</div>
<div id="outline-container-org394ef21" class="outline-4">
<h4 id="org394ef21">Reproducibilidad</h4>
<div class="outline-text-4" id="text-org394ef21">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">random_state</span> = 42
</pre>
</div>

<ul class="org-ul">
<li>Fija la semilla para la inicialización de pesos y el muestreo interno.</li>
<li>Permite reproducir exactamente los mismos resultados entre ejecuciones.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgb9a0145" class="outline-3">
<h3 id="orgb9a0145">Entrenamiento del modelo</h3>
<div class="outline-text-3" id="text-orgb9a0145">
<p>
Entrenamos el MLP ajustando pesos y sesgos para minimizar el error de
clasificación en el conjunto de entrenamiento:
</p>

<div class="org-src-container">
<pre class="src src-python">mlp.fit<span style="color: #707183;">(</span>X_train, y_train<span style="color: #707183;">)</span>
</pre>
</div>
</div>
<div id="outline-container-orgfaf18f7" class="outline-4">
<h4 id="orgfaf18f7">Proceso interno (visión conceptual)</h4>
<div class="outline-text-4" id="text-orgfaf18f7">
<ol class="org-ol">
<li><b>Propagación hacia adelante</b>:
<ul class="org-ul">
<li>Cada muestra \(\mathbf{x}\) pasa por las capas:
\[
       \mathbf{h}^{(1)} = f_1(W^{(1)} \mathbf{x} + \mathbf{b}^{(1)})
     \]
\[
       \mathbf{h}^{(2)} = f_2(W^{(2)} \mathbf{h}^{(1)} + \mathbf{b}^{(2)})
     \]
\[
       \hat{y} = \sigma(W^{(3)} \mathbf{h}^{(2)} + \mathbf{b}^{(3)})
     \]</li>
<li>\(\hat{y}\) es la probabilidad estimada de que el tumor sea benigno
(clase 1).</li>
</ul></li>

<li><b>Cálculo del error</b>:
<ul class="org-ul">
<li>La salida \(\hat{y}\) se compara con la etiqueta real \(y \in \{0, 1\}\).</li>
<li>En problemas binarios, se usa típicamente la <b>entropía cruzada
binaria</b> (o log-loss):
\[
       L(y, \hat{y}) = -\left[y \log(\hat{y}) + (1 - y)\log(1 - \hat{y})\right]
     \]</li>
</ul></li>

<li><b>Backpropagation</b>:
<ul class="org-ul">
<li>Se calculan las derivadas parciales de la pérdida con respecto a
todos los pesos y sesgos mediante la regla de la cadena.</li>
<li>Se propaga el error desde la capa de salida hacia las capas
ocultas.</li>
</ul></li>

<li><b>Actualización de pesos</b>:
<ul class="org-ul">
<li>El optimizador Adam actualiza cada parámetro en la dirección que
reduce la pérdida:
\[
       \theta \leftarrow \theta - \eta \cdot \hat{g}
     \]
donde \(\hat{g}\) es una versión adaptada del gradiente, y
\(\eta\) la tasa de aprendizaje efectiva.</li>
</ul></li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org145d8c8" class="outline-3">
<h3 id="org145d8c8">Evaluación del modelo</h3>
<div class="outline-text-3" id="text-org145d8c8">
<p>
Una vez entrenado, evaluamos el modelo sobre el conjunto de prueba:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Predicci&#243;n
</span><span style="color: #a0522d;">y_pred</span> = mlp.predict<span style="color: #707183;">(</span>X_test<span style="color: #707183;">)</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">M&#233;tricas
</span><span style="color: #483d8b;">print</span><span style="color: #707183;">(</span><span style="color: #8b2252;">"Accuracy:"</span>, accuracy_score<span style="color: #7388d6;">(</span>y_test, y_pred<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #483d8b;">print</span><span style="color: #707183;">(</span><span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Reporte de clasificaci&#243;n:</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">"</span><span style="color: #707183;">)</span>
<span style="color: #483d8b;">print</span><span style="color: #707183;">(</span>classification_report<span style="color: #7388d6;">(</span>y_test, y_pred, target_names=data.target_names<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>
</div>
<div id="outline-container-org629df3e" class="outline-4">
<h4 id="org629df3e">Métricas principales</h4>
<div class="outline-text-4" id="text-org629df3e">
<ul class="org-ul">
<li><b>Accuracy</b>:
<ul class="org-ul">
<li>Proporción de predicciones correctas:
\[
      \text{Accuracy} = \frac{\text{número de aciertos}}{\text{número total de muestras}}
    \]</li>
</ul></li>
<li><b>Precision</b>:
<ul class="org-ul">
<li>De todos los ejemplos predichos como positivos, qué fracción son
realmente positivos.</li>
<li>Importante para evitar falsos positivos.</li>
</ul></li>

<li><b>Recall (Sensibilidad)</b>:
<ul class="org-ul">
<li>De todos los ejemplos realmente positivos, qué fracción se detecta
correctamente.</li>
<li>En diagnóstico médico, el recall para la clase “maligno” es crítico
(minimizar falsos negativos).</li>
</ul></li>

<li><b>F1-score</b>:
<ul class="org-ul">
<li>Media armónica de precision y recall:
\[
      F1 = 2 \cdot \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}
    \]</li>
<li>Útil cuando hay cierto desbalance entre clases.</li>
</ul></li>
</ul>

<p>
El dataset Breast Cancer suele producir <b>accuracy</b> muy altas (por
encima de 95 %) con modelos bien entrenados, debido a que las
características están relativamente bien separadas.
</p>
</div>
</div>
</div>
<div id="outline-container-orgf110795" class="outline-3">
<h3 id="orgf110795">Interpretación matemática de la salida binaria</h3>
<div class="outline-text-3" id="text-orgf110795">
<p>
En este modelo binario, la capa de salida puede verse como una neurona
con activación <b>sigmoide (logística)</b>:
</p>

<div class="latex" id="orge581e6e">
<p>
&sigma;(z) = \frac{1}{1 + e<sup>-z</sup>}
</p>

</div>

<p>
donde:
</p>

<ul class="org-ul">
<li>\(z = \mathbf{w} \cdot \mathbf{h} + b\),</li>
<li>\(\mathbf{h}\) es el vector de activaciones de la última capa oculta.</li>
</ul>

<p>
La salida \(\sigma(z)\) se interpreta como:
</p>

<ul class="org-ul">
<li>\(\sigma(z) \approx 1\): alta probabilidad de clase 1 (benigno),</li>
<li>\(\sigma(z) \approx 0\): alta probabilidad de clase 0 (maligno).</li>
</ul>

<p>
En scikit-learn, la predicción de clase se realiza típicamente como:
</p>

<ul class="org-ul">
<li>Si \(\sigma(z) \geq 0.5\) → clase 1 (benigno).</li>
<li>Si \(\sigma(z) < 0.5\) → clase 0 (maligno).</li>
</ul>

<p>
Este umbral puede modificarse en contextos médicos, por ejemplo:
</p>

<ul class="org-ul">
<li>usar un umbral menor que 0.5 para aumentar la sensibilidad a tumores
malignos (aceptando más falsos positivos).</li>
</ul>
</div>
</div>
<div id="outline-container-org8cf5958" class="outline-3">
<h3 id="org8cf5958">Predicción de una nueva muestra</h3>
<div class="outline-text-3" id="text-org8cf5958">
<p>
Podemos usar el modelo entrenado para predecir el diagnóstico de un
nuevo paciente.
</p>

<p>
En este ejemplo, tomamos la primera muestra del set de prueba (ya
normalizada) y realizamos la predicción:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Ejemplo con la primera fila del set de prueba
</span><span style="color: #a0522d;">nuevo_paciente</span> = X_test<span style="color: #707183;">[</span>0:1<span style="color: #707183;">]</span>     <span style="color: #b22222;"># </span><span style="color: #b22222;">forma (1, 30)
</span><span style="color: #a0522d;">pred</span> = mlp.predict<span style="color: #707183;">(</span>nuevo_paciente<span style="color: #707183;">)</span>
<span style="color: #483d8b;">print</span><span style="color: #707183;">(</span><span style="color: #8b2252;">"Resultado del diagn&#243;stico:"</span>, data.target_names<span style="color: #7388d6;">[</span>pred<span style="color: #909183;">[</span>0<span style="color: #909183;">]</span><span style="color: #7388d6;">]</span><span style="color: #707183;">)</span>
</pre>
</div>
</div>
<div id="outline-container-orgb7cbcbf" class="outline-4">
<h4 id="orgb7cbcbf">Comentarios importantes</h4>
<div class="outline-text-4" id="text-orgb7cbcbf">
<ul class="org-ul">
<li>Es fundamental que la nueva muestra haya sido procesada con el mismo:

<ol class="org-ol">
<li>Conjunto de características (mismas columnas y orden).</li>
<li>Mismo procedimiento de normalización (mismo <code>scaler</code>, con las
medias y desviaciones aprendidas en el entrenamiento).</li>
</ol></li>

<li>En la práctica, se suele:

<ul class="org-ul">
<li>guardar el modelo entrenado (por ejemplo con <code>joblib</code>),</li>
<li>guardar también el objeto <code>StandardScaler</code>,</li>
<li>aplicar ambos de manera coherente a los nuevos datos.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgd775baa" class="outline-3">
<h3 id="orgd775baa">Resumen conceptual</h3>
<div class="outline-text-3" id="text-orgd775baa">
<p>
Este ejemplo ilustra varios conceptos clave en el uso de redes
neuronales (MLP) para clasificación binaria en un contexto médico:
</p>

<ul class="org-ul">
<li>Importancia de:
<ul class="org-ul">
<li>Partición entrenamiento/prueba estratificada.</li>
<li>Normalización de características.</li>
<li>Arquitectura de red (capas y neuronas).</li>
<li>Función de activación (ReLU).</li>
<li>Optimizador (Adam) y número de iteraciones.</li>
</ul></li>

<li>Interpretación de la salida:
<ul class="org-ul">
<li>Probabilidad generada por una función sigmoide.</li>
<li>Decisión de clase basada en un umbral.</li>
</ul></li>

<li>Evaluación:
<ul class="org-ul">
<li>No basta con accuracy: hay que considerar precision, recall y F1,
especialmente para la clase de interés (por ejemplo, “maligno”).</li>
</ul></li>
</ul>

<p>
Este mismo flujo (carga, partición, normalización, definición de modelo,
entrenamiento, evaluación, predicción) puede adaptarse a otros modelos
(árboles, SVM, LSTM, etc.) y a otros datasets de clasificación binaria
o multiclase.
</p>
</div>
</div>
</div>
<div id="outline-container-org65b2102" class="outline-2">
<h2 id="org65b2102">Redes Neuronales LSTM (Long Short-Term Memory)</h2>
<div class="outline-text-2" id="text-org65b2102">
<p>
Las redes neuronales LSTM (<b>Long Short-Term Memory</b>) son un tipo especial
de <b><b>Red Neuronal Recurrente (RNN)</b></b> diseñada para trabajar con datos
<b><b>secuenciales</b></b> y, en particular, para <b><b>recordar dependencias a largo
plazo</b></b>.
</p>

<p>
Se utilizan cuando el <b>orden</b> de los datos importa:
</p>

<ul class="org-ul">
<li>Texto (oraciones, párrafos, documentos).</li>
<li>Series temporales (precios, temperatura, señales).</li>
<li>Audio y voz.</li>
<li>Secuencias de eventos (logs, clics, acciones de usuario, etc.).</li>
</ul>

<p>
Su principal aporte es resolver (o al menos mitigar fuertemente) el
problema del <b><b>desvanecimiento del gradiente (vanishing gradient)</b></b> que
afecta a las RNN simples cuando manejan secuencias largas.
</p>
</div>
<div id="outline-container-orgedd8f1f" class="outline-3">
<h3 id="orgedd8f1f">Estructura general de una RNN</h3>
<div class="outline-text-3" id="text-orgedd8f1f">
<p>
En una RNN “simple”, en cada paso temporal \(t\):
</p>

<ul class="org-ul">
<li>Se recibe una entrada \(x_t\).</li>
<li>Se tiene un estado oculto anterior \(h_{t-1}\).</li>
<li>Se actualiza el estado oculto actual \(h_t\) con:</li>
</ul>

<div class="latex" id="org2a15dfc">
<p>
h<sub>t</sub> = &phi;(W<sub>xh</sub> x<sub>t</sub> + W<sub>hh</sub> h<sub>t-1</sub> + b<sub>h</sub>)
</p>

</div>

<p>
donde:
</p>

<ul class="org-ul">
<li>\(W_{xh}\) y \(W_{hh}\) son matrices de pesos,</li>
<li>\(b_h\) es el sesgo,</li>
<li>\(\phi\) es una función de activación (típicamente <code>tanh</code> o <code>ReLU</code>).</li>
</ul>

<p>
El estado final (o todos los estados) se usan para hacer predicciones.
</p>
</div>
<div id="outline-container-org56bc796" class="outline-4">
<h4 id="org56bc796">Problema: vanishing y exploding gradients</h4>
<div class="outline-text-4" id="text-org56bc796">
<p>
Durante el entrenamiento, se aplica <b>Backpropagation Through Time (BPTT)</b>,
es decir, se retropropagan los gradientes a través de todos los pasos
temporales.
</p>

<p>
En secuencias largas:
</p>

<ul class="org-ul">
<li>Los gradientes pueden volverse <b><b>muy pequeños</b></b> → la red “olvida” las
dependencias lejanas (problema de <b>vanishing gradient</b>).</li>
<li>O volverse <b><b>muy grandes</b></b> → inestabilidad numérica (problema de
<b>exploding gradient</b>).</li>
</ul>

<p>
Consecuencia: las RNN simples tienen dificultades para <b><b>aprender
relaciones de largo plazo</b></b>, por ejemplo, la dependencia entre una
palabra al principio de una oración y otra al final.
</p>
</div>
</div>
</div>
<div id="outline-container-org733e479" class="outline-3">
<h3 id="org733e479">Motivación de las LSTM</h3>
<div class="outline-text-3" id="text-org733e479">
<p>
Las LSTM fueron propuestas para permitir que la red:
</p>

<ul class="org-ul">
<li><b><b>Mantenga información relevante durante muchos pasos temporales</b></b>.</li>
<li><b><b>Controle qué recordar y qué olvidar</b></b>.</li>
<li>Facilite el flujo de gradiente a través del tiempo, reduciendo el
problema del desvanecimiento.</li>
</ul>

<p>
La idea principal es introducir una <b><b>celda de memoria</b></b> y un sistema de
<b><b>puertas (gates)</b></b> que regulan el flujo de información.
</p>
</div>
</div>
<div id="outline-container-orgea859ac" class="outline-3">
<h3 id="orgea859ac">Arquitectura interna de una celda LSTM</h3>
<div class="outline-text-3" id="text-orgea859ac">
<p>
En lugar de solo tener el estado oculto \(h_t\), la LSTM mantiene:
</p>

<ul class="org-ul">
<li>un <b><b>estado de memoria</b></b> \(c_t\) (a veces llamado <b>cell state</b>),</li>
<li>un <b><b>estado oculto</b></b> \(h_t\) (la “salida” en ese paso).</li>
</ul>

<p>
En cada paso temporal \(t\), la LSTM procesa:
</p>

<ul class="org-ul">
<li>la entrada actual \(x_t\),</li>
<li>el estado oculto anterior \(h_{t-1}\),</li>
<li>el estado de memoria anterior \(c_{t-1}\).</li>
</ul>

<p>
Y calcula:
</p>

<ul class="org-ul">
<li>una <b>puerta de olvido</b> \(f_t\),</li>
<li>una <b>puerta de entrada</b> \(i_t\),</li>
<li>una <b>candidata de memoria</b> \(\tilde{c}_t\),</li>
<li>una <b>puerta de salida</b> \(o_t\),</li>
<li>el nuevo estado de memoria \(c_t\),</li>
<li>el nuevo estado oculto \(h_t\).</li>
</ul>
</div>
<div id="outline-container-org9a47d58" class="outline-4">
<h4 id="org9a47d58">Ecuaciones de la LSTM</h4>
<div class="outline-text-4" id="text-org9a47d58">
<p>
Típicamente, las ecuaciones de una LSTM (versión “estándar”) son:
</p>

<div class="latex" id="org741dea7">
\begin{aligned}
f_t &= \sigma(W_f [h_{t-1}, x_t] + b_f) \\
i_t &= \sigma(W_i [h_{t-1}, x_t] + b_i) \\
\tilde{c}_t &= \tanh(W_c [h_{t-1}, x_t] + b_c) \\
c_t &= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t \\
o_t &= \sigma(W_o [h_{t-1}, x_t] + b_o) \\
h_t &= o_t \odot \tanh(c_t)
\end{aligned}

</div>

<p>
donde:
</p>

<ul class="org-ul">
<li>\(\sigma\) es la función sigmoide,</li>
<li>\(\tanh\) es la tangente hiperbólica,</li>
<li>\(\odot\) es el producto elemento a elemento,</li>
<li>\([h_{t-1}, x_t]\) indica la concatenación de vectores.</li>
</ul>
</div>
</div>
<div id="outline-container-org35be279" class="outline-4">
<h4 id="org35be279">Interpretación de las puertas</h4>
<div class="outline-text-4" id="text-org35be279">
<ul class="org-ul">
<li>\(f_t\) (<b>forget gate</b> o puerta de olvido):  
<ul class="org-ul">
<li>Toma valores en \([0,1]\) (por la sigmoide).</li>
<li>Decide cuánto de la memoria anterior \(c_{t-1}\) se conserva.</li>
<li>Si \(f_t \approx 1\) → se mantiene casi toda la memoria anterior.</li>
<li>Si \(f_t \approx 0\) → se olvida casi todo.</li>
</ul></li>

<li>\(i_t\) (<b>input gate</b> o puerta de entrada):  
<ul class="org-ul">
<li>Controla cuánta de la nueva información candidata \(\tilde{c}_t\) se
incorpora a la memoria.</li>
<li>Valores cercanos a 1 permiten “escribir” nueva información; valores
cercanos a 0 bloquean escritura.</li>
</ul></li>

<li>\(\tilde{c}_t\) (candidata de memoria):  
<ul class="org-ul">
<li>Es el nuevo contenido potencial para la memoria.</li>
<li>Se combina con \(i_t\) para actualizar \(c_t\).</li>
</ul></li>

<li>\(c_t\) (<b>cell state</b>):  
<ul class="org-ul">
<li>Es la memoria principal de la LSTM.</li>
<li>Se actualiza como:
\[
      c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t
    \]</li>
<li>Puede transportar información durante muchos pasos, ya que el
gradiente puede fluir a lo largo de \(c_t\) con menos atenuación.</li>
</ul></li>

<li>\(o_t\) (<b>output gate</b> o puerta de salida):  
<ul class="org-ul">
<li>Controla qué parte de la memoria \(c_t\) se expone como salida \(h_t\).</li>
</ul></li>

<li>\(h_t\) (estado oculto / salida):  
<ul class="org-ul">
<li>Es la “representación” que se usa para producir salidas en cada
paso, o para alimentar capas posteriores.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgac392d3" class="outline-3">
<h3 id="orgac392d3">Intuición conceptual</h3>
<div class="outline-text-3" id="text-orgac392d3">
<p>
Una forma intuitiva de entender una LSTM es verla como una <b><b>celda de
memoria con una “llave de escritura”, una “llave de borrado” y una
“llave de lectura”</b></b>:
</p>

<ul class="org-ul">
<li><b>Olvido</b>: la red decide qué partes de la memoria vieja ya no son
relevantes y las borra parcialmente (puerta \(f_t\)).</li>
<li><b>Escritura</b>: la red decide qué nueva información vale la pena guardar
(puerta \(i_t\) y candidata \(\tilde{c}_t\)).</li>
<li><b>Lectura</b>: la red decide qué parte de la memoria compartir como salida
(puerta \(o_t\)).</li>
</ul>

<p>
Esto permite que la red conserve información importante durante muchos
pasos temporales, por ejemplo:
</p>

<ul class="org-ul">
<li>el tema general de una oración,</li>
<li>la tendencia de una serie temporal,</li>
<li>el contexto de una conversación, etc.</li>
</ul>
</div>
</div>
<div id="outline-container-org4934aae" class="outline-3">
<h3 id="org4934aae">Entrenamiento de una LSTM</h3>
<div class="outline-text-3" id="text-org4934aae">
<p>
El entrenamiento de una LSTM sigue el mismo esquema general que otras
redes neuronales:
</p>

<ol class="org-ol">
<li><b><b>Forward pass</b></b>:
<ul class="org-ul">
<li>La secuencia completa \((x_1, x_2, \dots, x_T)\) se ingresa paso a
paso.</li>
<li>En cada paso se actualizan \(c_t\) y \(h_t\).</li>
<li>Se produce una salida (por ejemplo, en cada paso o solo al final).</li>
</ul></li>

<li><b><b>Cálculo de la pérdida</b></b>:
<ul class="org-ul">
<li>Se compara la salida de la red con la etiqueta real.</li>
<li>Se usa una función de pérdida acorde al problema:
<ul class="org-ul">
<li>Clasificación: entropía cruzada (multiclase o binaria).</li>
<li>Regresión: error cuadrático medio (MSE), etc.</li>
</ul></li>
</ul></li>

<li><b><b>Backpropagation Through Time (BPTT)</b></b>:
<ul class="org-ul">
<li>Se retropropaga el gradiente desde el último paso hacia los
primeros.</li>
<li>Se actualizan los pesos de todas las puertas y conexiones.</li>
</ul></li>

<li><b><b>Actualización de parámetros</b></b>:
<ul class="org-ul">
<li>Se utilizan optimizadores como SGD, Adam, RMSprop.</li>
<li>En la práctica, Adam y RMSprop son muy populares en LSTM.</li>
</ul></li>
</ol>

<p>
Gracias a la estructura de memoria, los gradientes a través de \(c_t\)
tienden a <b><b>no desaparecer tan rápido</b></b> como en una RNN simple.
</p>
</div>
</div>
<div id="outline-container-org6d42adf" class="outline-3">
<h3 id="org6d42adf">Tipos de tareas típicas con LSTM</h3>
<div class="outline-text-3" id="text-org6d42adf">
<p>
Las LSTM se aplican a muchas tareas de secuencia:
</p>

<ul class="org-ul">
<li><b>Modelado de lenguaje</b>:
<ul class="org-ul">
<li>Predecir la siguiente palabra de una oración.</li>
<li>Asignar probabilidad a una secuencia de palabras.</li>
</ul></li>

<li><b>Traducción automática</b>:
<ul class="org-ul">
<li>Modelos encoder–decoder (LSTM para codificar una oración, otra LSTM
para decodificar en otro idioma).</li>
</ul></li>

<li><b>Etiquetado secuencial</b>:
<ul class="org-ul">
<li>Etiquetado de partes del habla (POS tagging).</li>
<li>Reconocimiento de entidades nombradas (NER).</li>
</ul></li>

<li><b>Series temporales</b>:
<ul class="org-ul">
<li>Predicción de valores futuros (ej. precios de acciones).</li>
<li>Detección de anomalías en señales.</li>
</ul></li>

<li><b>Procesamiento de audio / voz</b>:
<ul class="org-ul">
<li>Reconocimiento de voz.</li>
<li>Síntesis de voz (en combinación con otras arquitecturas).</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org25992bb" class="outline-3">
<h3 id="org25992bb">Variantes y extensiones de LSTM</h3>
<div class="outline-text-3" id="text-org25992bb">
<p>
A partir de la LSTM estándar, han surgido múltiples variantes:
</p>

<ul class="org-ul">
<li><b>LSTM bidireccional (Bidirectional LSTM)</b>:
<ul class="org-ul">
<li>Procesa la secuencia en dos direcciones:
<ul class="org-ul">
<li>de izquierda a derecha (forward),</li>
<li>de derecha a izquierda (backward).</li>
</ul></li>
<li>Se concatena la información de ambos sentidos.</li>
<li>Útil cuando se tiene acceso a la secuencia completa (texto, frases).</li>
</ul></li>

<li><b>Pilado de LSTMs (Stacked LSTM)</b>:
<ul class="org-ul">
<li>Varias capas LSTM apiladas una sobre otra.</li>
<li>Las salidas de una capa sirven de entradas a la siguiente.</li>
<li>Permite aprender representaciones más complejas.</li>
</ul></li>

<li><b>LSTM con atención (Attention)</b>:
<ul class="org-ul">
<li>Combinación de LSTM con mecanismos de atención.</li>
<li>El modelo decide a qué partes de la secuencia “prestar más atención”
al hacer una predicción.</li>
</ul></li>

<li><b>GRU (Gated Recurrent Unit)</b>:
<ul class="org-ul">
<li>Variante simplificada de LSTM (menos puertas, menos parámetros).</li>
<li>Suele tener rendimiento comparable en muchos problemas.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org54b1845" class="outline-3">
<h3 id="org54b1845">Implementación básica en Python (Keras)</h3>
<div class="outline-text-3" id="text-org54b1845">
<p>
A modo de ejemplo muy simple (no ligado a un dataset específico):
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> tensorflow <span style="color: #a020f0;">as</span> tf
<span style="color: #a020f0;">from</span> tensorflow.keras.models <span style="color: #a020f0;">import</span> Sequential
<span style="color: #a020f0;">from</span> tensorflow.keras.layers <span style="color: #a020f0;">import</span> LSTM, Dense

<span style="color: #b22222;"># </span><span style="color: #b22222;">Supongamos una secuencia de longitud 10 con 5 features por timestep
</span><span style="color: #a0522d;">timesteps</span> = 10
<span style="color: #a0522d;">features</span> = 5
<span style="color: #a0522d;">num_samples</span> = 100
<span style="color: #a0522d;">num_classes</span> = 3

<span style="color: #b22222;"># </span><span style="color: #b22222;">Datos aleatorios de ejemplo (solo para ilustrar la forma)
</span><span style="color: #a0522d;">X</span> = np.random.randn<span style="color: #707183;">(</span>num_samples, timesteps, features<span style="color: #707183;">)</span>
<span style="color: #a0522d;">y</span> = np.random.randint<span style="color: #707183;">(</span>0, num_classes, size=<span style="color: #7388d6;">(</span>num_samples,<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">One-hot encoding
</span><span style="color: #a0522d;">y_onehot</span> = tf.keras.utils.to_categorical<span style="color: #707183;">(</span>y, num_classes=num_classes<span style="color: #707183;">)</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Definir modelo LSTM
</span><span style="color: #a0522d;">model</span> = Sequential<span style="color: #707183;">(</span><span style="color: #7388d6;">[</span>
<span style="background-color: #f2f2f2;"> </span>   LSTM<span style="color: #909183;">(</span>32, input_shape=<span style="color: #709870;">(</span>timesteps, features<span style="color: #709870;">)</span><span style="color: #909183;">)</span>,
<span style="background-color: #f2f2f2;"> </span>   Dense<span style="color: #909183;">(</span>num_classes, activation=<span style="color: #8b2252;">'softmax'</span><span style="color: #909183;">)</span>
<span style="color: #7388d6;">]</span><span style="color: #707183;">)</span>

model.<span style="color: #483d8b;">compile</span><span style="color: #707183;">(</span>
<span style="background-color: #f2f2f2;"> </span>   loss=<span style="color: #8b2252;">'categorical_crossentropy'</span>,
<span style="background-color: #f2f2f2;"> </span>   optimizer=<span style="color: #8b2252;">'adam'</span>,
<span style="background-color: #f2f2f2;"> </span>   metrics=<span style="color: #7388d6;">[</span><span style="color: #8b2252;">'accuracy'</span><span style="color: #7388d6;">]</span>
<span style="color: #707183;">)</span>

model.summary<span style="color: #707183;">()</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Entrenamiento de ejemplo
</span>model.fit<span style="color: #707183;">(</span>X, y_onehot, epochs=10, batch_size=16<span style="color: #707183;">)</span>
</pre>
</div>

<p>
En este ejemplo:
</p>

<ul class="org-ul">
<li>La entrada tiene forma <code>(num_samples, timesteps, features)</code>.</li>
<li>La LSTM devuelve un vector (el último estado oculto).</li>
<li>La capa <code>Dense</code> con softmax produce una distribución de probabilidad
sobre las clases.</li>
</ul>
</div>
</div>
<div id="outline-container-org04b2937" class="outline-3">
<h3 id="org04b2937">Ventajas y limitaciones de LSTM</h3>
<div class="outline-text-3" id="text-org04b2937">
</div>
<div id="outline-container-orgf7b42e6" class="outline-4">
<h4 id="orgf7b42e6">Ventajas</h4>
<div class="outline-text-4" id="text-orgf7b42e6">
<ul class="org-ul">
<li>Manejan mejor dependencias de largo plazo que las RNN simples.</li>
<li>Funcionan bien en problemas donde el orden y el contexto importan.</li>
<li>Son muy flexibles y pueden combinarse con otras arquitecturas
(convolucionales, atención, etc.).</li>
</ul>
</div>
</div>
<div id="outline-container-org6ed38ed" class="outline-4">
<h4 id="org6ed38ed">Limitaciones</h4>
<div class="outline-text-4" id="text-org6ed38ed">
<ul class="org-ul">
<li>Entrenamiento relativamente costoso (más parámetros que RNN simple).</li>
<li>Difíciles de paralelizar completamente debido a su naturaleza
secuencial.</li>
<li>En tareas muy complejas y con grandes volúmenes de datos, han sido en
muchos casos superadas por arquitecturas basadas en <b>transformers</b>.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgb90b09c" class="outline-3">
<h3 id="orgb90b09c">Conclusión</h3>
<div class="outline-text-3" id="text-orgb90b09c">
<p>
Las LSTM representan un paso fundamental en la historia de las redes
neuronales para secuencias:
</p>

<ul class="org-ul">
<li>resolvieron en gran medida el problema del desvanecimiento del
gradiente en RNN,</li>
<li>permitieron avances significativos en NLP, series temporales y voz,</li>
<li>siguen siendo una herramienta muy útil y educativa para entender cómo
las redes pueden “recordar” información a lo largo del tiempo.</li>
</ul>

<p>
Aun cuando hoy en día los <b>transformers</b> dominen muchas aplicaciones de
NLP, las LSTM siguen siendo:
</p>

<ul class="org-ul">
<li>una excelente base conceptual,</li>
<li>útiles en problemas con secuencias pequeñas o recursos limitados,</li>
<li>y una parte importante de la “caja de herramientas” de cualquier
persona que trabaja con aprendizaje profundo y datos secuenciales.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org24358f0" class="outline-2">
<h2 id="org24358f0">Ejemplo Clasificación del dataset Iris con una red LSTM</h2>
<div class="outline-text-2" id="text-org24358f0">
<p>
Este apartado muestra un ejemplo completo en Python usando el dataset
clásico Iris y una red neuronal recurrente del tipo <b>LSTM</b> (Long Short-Term Memory)
usando la librería <b>Keras</b> (TensorFlow).
</p>

<p>
El objetivo es clasificar flores Iris en tres clases:
</p>

<ul class="org-ul">
<li>Setosa</li>
<li>Versicolor</li>
<li>Virginica</li>
</ul>

<p>
A diferencia del MLP (Perceptrón Multicapa), la LSTM está diseñada
para procesar datos <b>secuenciales</b>. En este ejemplo, forzamos la
estructura secuencial tratando las 4 características de Iris como una
secuencia de 4 pasos temporales.
</p>
</div>
<div id="outline-container-org98267e0" class="outline-3">
<h3 id="org98267e0">Importar librerías</h3>
<div class="outline-text-3" id="text-org98267e0">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np

<span style="color: #a020f0;">from</span> sklearn.datasets <span style="color: #a020f0;">import</span> load_iris
<span style="color: #a020f0;">from</span> sklearn.model_selection <span style="color: #a020f0;">import</span> train_test_split
<span style="color: #a020f0;">from</span> sklearn.preprocessing <span style="color: #a020f0;">import</span> StandardScaler
<span style="color: #a020f0;">from</span> sklearn.metrics <span style="color: #a020f0;">import</span> accuracy_score, classification_report

<span style="color: #a020f0;">import</span> tensorflow <span style="color: #a020f0;">as</span> tf
<span style="color: #a020f0;">from</span> tensorflow.keras.models <span style="color: #a020f0;">import</span> Sequential
<span style="color: #a020f0;">from</span> tensorflow.keras.layers <span style="color: #a020f0;">import</span> LSTM, Dense
<span style="color: #a020f0;">from</span> tensorflow.keras.utils <span style="color: #a020f0;">import</span> to_categorical
</pre>
</div>
</div>
</div>
<div id="outline-container-org2347246" class="outline-3">
<h3 id="org2347246">Cargar el dataset Iris</h3>
<div class="outline-text-3" id="text-org2347246">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">iris</span> = load_iris<span style="color: #707183;">()</span>
<span style="color: #a0522d;">X</span> = iris.data      <span style="color: #b22222;"># </span><span style="color: #b22222;">Caracter&#237;sticas (150 x 4)
</span><span style="color: #a0522d;">y</span> = iris.target    <span style="color: #b22222;"># </span><span style="color: #b22222;">Clases (150,)
</span>
<span style="color: #483d8b;">print</span><span style="color: #707183;">(</span>X.shape<span style="color: #707183;">)</span>     <span style="color: #b22222;"># </span><span style="color: #b22222;">(150, 4)
</span><span style="color: #483d8b;">print</span><span style="color: #707183;">(</span>y.shape<span style="color: #707183;">)</span>     <span style="color: #b22222;"># </span><span style="color: #b22222;">(150,)</span>
</pre>
</div>

<p>
Las 4 características son:
</p>

<ul class="org-ul">
<li>Largo del sépalo</li>
<li>Ancho del sépalo</li>
<li>Largo del pétalo</li>
<li>Ancho del pétalo</li>
</ul>

<p>
Las clases están codificadas como:
</p>

<ul class="org-ul">
<li>0 → Setosa</li>
<li>1 → Versicolor</li>
<li>2 → Virginica</li>
</ul>
</div>
</div>
<div id="outline-container-orgb69b738" class="outline-3">
<h3 id="orgb69b738">Partición entrenamiento / prueba</h3>
<div class="outline-text-3" id="text-orgb69b738">
<p>
El siguiente fragmento de código realiza la partición del conjunto de
datos original en dos subconjuntos disjuntos: uno destinado al
entrenamiento del modelo y otro a su evaluación.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">X_train</span>, <span style="color: #a0522d;">X_test</span>, <span style="color: #a0522d;">y_train</span>, <span style="color: #a0522d;">y_test</span> = train_test_split<span style="color: #707183;">(</span>
<span style="background-color: #f2f2f2;"> </span>   X,
<span style="background-color: #f2f2f2;"> </span>   y,
<span style="background-color: #f2f2f2;"> </span>   test_size=0.2,
<span style="background-color: #f2f2f2;"> </span>   random_state=42,
<span style="background-color: #f2f2f2;"> </span>   stratify=y
<span style="color: #707183;">)</span>
</pre>
</div>
</div>
<div id="outline-container-org70ba70b" class="outline-4">
<h4 id="org70ba70b">Propósito de la partición</h4>
<div class="outline-text-4" id="text-org70ba70b">
<p>
En aprendizaje automático supervisado, es fundamental evaluar la
capacidad de generalización de un modelo. Para ello, los datos
disponibles se dividen en:
</p>

<ul class="org-ul">
<li><b>Conjunto de entrenamiento</b>: utilizado para ajustar los parámetros del modelo (pesos y sesgos).</li>
<li><b>Conjunto de prueba</b>: utilizado exclusivamente para medir el desempeño del modelo sobre datos no vistos durante el entrenamiento.</li>
</ul>

<p>
Esta separación permite detectar fenómenos como overfitting y
underfitting, y proporciona una estimación más realista del
rendimiento esperado en producción.
</p>
</div>
</div>
<div id="outline-container-org28a6d1e" class="outline-4">
<h4 id="org28a6d1e">Parámetro test<sub>size</sub></h4>
<div class="outline-text-4" id="text-org28a6d1e">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">test_size</span> = 0.2
</pre>
</div>

<p>
indica que el 20 % del conjunto total de datos se reserva para el
conjunto de prueba, mientras que el 80 % restante se utiliza para
entrenamiento.
</p>
</div>
</div>
<div id="outline-container-org4596a3f" class="outline-4">
<h4 id="org4596a3f">Reproducibilidad del experimento</h4>
<div class="outline-text-4" id="text-org4596a3f">
<p>
El parámetro:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">random_state</span> = 42
</pre>
</div>

<p>
fija la semilla del generador de números aleatorios utilizado durante
la partición. Esto garantiza que la división de los datos sea
reproducible.
</p>
</div>
</div>
<div id="outline-container-org371bdbe" class="outline-4">
<h4 id="org371bdbe">Estratificación de clases</h4>
<div class="outline-text-4" id="text-org371bdbe">
<p>
El parámetro:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">stratify</span> = y
</pre>
</div>

<p>
indica que la división debe realizarse de forma estratificada,
conservando la proporción original de cada clase en ambos
subconjuntos.
</p>
</div>
</div>
</div>
<div id="outline-container-orgea57be5" class="outline-3">
<h3 id="orgea57be5">Normalización de los datos</h3>
<div class="outline-text-3" id="text-orgea57be5">
<p>
La normalización de los datos de entrada es un paso crítico en el
entrenamiento de redes neuronales (incluyendo LSTM), ya que influye
directamente en la estabilidad numérica, la velocidad de convergencia
y la eficacia del aprendizaje.
</p>

<p>
Usaremos estandarización con <code>StandardScaler</code>:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">scaler</span> = StandardScaler<span style="color: #707183;">()</span>
<span style="color: #a0522d;">X_train</span> = scaler.fit_transform<span style="color: #707183;">(</span>X_train<span style="color: #707183;">)</span>
<span style="color: #a0522d;">X_test</span> = scaler.transform<span style="color: #707183;">(</span>X_test<span style="color: #707183;">)</span>
</pre>
</div>

<p>
Cada característica \(x\) se transforma según:
</p>

<div class="latex" id="org1ea0600">
<p>
x' = \frac{x - \mu}{\sigma}
</p>

</div>

<p>
donde:
</p>

<ul class="org-ul">
<li>\( \mu \) es la media de la característica,</li>
<li>\( \sigma \) es la desviación estándar.</li>
</ul>

<p>
El resultado es:
</p>

<ul class="org-ul">
<li>media aproximadamente 0,</li>
<li>desviación estándar aproximadamente 1.</li>
</ul>
</div>
</div>
<div id="outline-container-orgbb3f71f" class="outline-3">
<h3 id="orgbb3f71f">Preparar los datos para LSTM</h3>
<div class="outline-text-3" id="text-orgbb3f71f">
<p>
Las LSTM en Keras esperan entradas con forma:
</p>

<ul class="org-ul">
<li>\((n\_muestras, n\_timesteps, n\_features)\)</li>
</ul>

<p>
En Iris, tenemos 4 características. Una forma simple de usar LSTM es:
</p>

<ul class="org-ul">
<li>considerar una secuencia de longitud 4 (<code>timesteps=4</code>),</li>
<li>con 1 característica por paso (<code>features=1</code>).</li>
</ul>

<p>
Esto implica reestructurar cada muestra de forma (4,) a (4, 1):
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Reshape para LSTM: (n_muestras, timesteps=4, features=1)
</span><span style="color: #a0522d;">X_train_lstm</span> = X_train.reshape<span style="color: #707183;">(</span><span style="color: #7388d6;">(</span>X_train.shape<span style="color: #909183;">[</span>0<span style="color: #909183;">]</span>, 4, 1<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #a0522d;">X_test_lstm</span> = X_test.reshape<span style="color: #707183;">(</span><span style="color: #7388d6;">(</span>X_test.shape<span style="color: #909183;">[</span>0<span style="color: #909183;">]</span>, 4, 1<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>

<span style="color: #483d8b;">print</span><span style="color: #707183;">(</span>X_train_lstm.shape<span style="color: #707183;">)</span>  <span style="color: #b22222;"># </span><span style="color: #b22222;">(120, 4, 1)
</span><span style="color: #483d8b;">print</span><span style="color: #707183;">(</span>X_test_lstm.shape<span style="color: #707183;">)</span>   <span style="color: #b22222;"># </span><span style="color: #b22222;">(30, 4, 1)</span>
</pre>
</div>

<p>
Además, para clasificación multiclase con Keras, se suele usar
one-hot encoding de las etiquetas:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">num_classes</span> = <span style="color: #483d8b;">len</span><span style="color: #707183;">(</span>np.unique<span style="color: #7388d6;">(</span>y<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #a0522d;">y_train_cat</span> = to_categorical<span style="color: #707183;">(</span>y_train, num_classes=num_classes<span style="color: #707183;">)</span>
<span style="color: #a0522d;">y_test_cat</span> = to_categorical<span style="color: #707183;">(</span>y_test, num_classes=num_classes<span style="color: #707183;">)</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-orgc5f326b" class="outline-3">
<h3 id="orgc5f326b">Definir la red LSTM</h3>
<div class="outline-text-3" id="text-orgc5f326b">
<p>
Definimos un modelo secuencial con una capa LSTM y una capa densa de
salida con softmax:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">model</span> = Sequential<span style="color: #707183;">(</span><span style="color: #7388d6;">[</span>
<span style="background-color: #f2f2f2;"> </span>   LSTM<span style="color: #909183;">(</span>
<span style="background-color: #f2f2f2;"> </span>   <span style="background-color: #f2f2f2;"> </span>   units=16,           <span style="color: #b22222;"># </span><span style="color: #b22222;">n&#250;mero de unidades LSTM
</span><span style="background-color: #f2f2f2;"> </span>   <span style="background-color: #f2f2f2;"> </span>   activation=<span style="color: #8b2252;">'tanh'</span>,  <span style="color: #b22222;"># </span><span style="color: #b22222;">activaci&#243;n interna
</span><span style="background-color: #f2f2f2;"> </span>   <span style="background-color: #f2f2f2;"> </span>   input_shape=<span style="color: #709870;">(</span>4, 1<span style="color: #709870;">)</span>  <span style="color: #b22222;"># </span><span style="color: #b22222;">(timesteps=4, features=1)
</span><span style="background-color: #f2f2f2;"> </span>   <span style="color: #909183;">)</span>,
<span style="background-color: #f2f2f2;"> </span>   Dense<span style="color: #909183;">(</span>num_classes, activation=<span style="color: #8b2252;">'softmax'</span><span style="color: #909183;">)</span>
<span style="color: #7388d6;">]</span><span style="color: #707183;">)</span>
</pre>
</div>
</div>
<div id="outline-container-org8c6df01" class="outline-4">
<h4 id="org8c6df01">Arquitectura de la red</h4>
<div class="outline-text-4" id="text-org8c6df01">
<ul class="org-ul">
<li><b>Capa de entrada</b>:
<ul class="org-ul">
<li>Forma: (4, 1)</li>
<li>4 pasos "temporales" (cada uno con 1 valor), derivados de las 4 características.</li>
</ul></li>

<li><b>Capa LSTM</b>:
<ul class="org-ul">
<li><code>units=16</code>: 16 unidades de memoria.</li>
<li>Procesa la secuencia \((x_1, x_2, x_3, x_4)\) y genera una representación que captura
dependencias entre pasos.</li>
</ul></li>

<li><b>Capa de salida (Dense + softmax)</b>:
<ul class="org-ul">
<li>3 neuronas (una por clase).</li>
<li>Activación softmax para obtener probabilidades.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org633b9e9" class="outline-3">
<h3 id="org633b9e9">Compilación del modelo</h3>
<div class="outline-text-3" id="text-org633b9e9">
<p>
Escogemos una función de pérdida apropiada para clasificación
multiclase y un optimizador:
</p>

<div class="org-src-container">
<pre class="src src-python">model.<span style="color: #483d8b;">compile</span><span style="color: #707183;">(</span>
<span style="background-color: #f2f2f2;"> </span>   loss=<span style="color: #8b2252;">'categorical_crossentropy'</span>,
<span style="background-color: #f2f2f2;"> </span>   optimizer=tf.keras.optimizers.Adam<span style="color: #7388d6;">(</span>learning_rate=0.001<span style="color: #7388d6;">)</span>,
<span style="background-color: #f2f2f2;"> </span>   metrics=<span style="color: #7388d6;">[</span><span style="color: #8b2252;">'accuracy'</span><span style="color: #7388d6;">]</span>
<span style="color: #707183;">)</span>
</pre>
</div>

<ul class="org-ul">
<li><code>categorical_crossentropy</code>: log-loss multiclase.</li>
<li><b>Adam</b>: optimizador basado en descenso de gradiente con momento y tasas
de aprendizaje adaptativas.</li>
<li>Tasa de aprendizaje inicial \(\eta = 0.001\).</li>
</ul>
</div>
</div>
<div id="outline-container-org5871aca" class="outline-3">
<h3 id="org5871aca">Entrenamiento del modelo LSTM</h3>
<div class="outline-text-3" id="text-org5871aca">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">history</span> = model.fit<span style="color: #707183;">(</span>
<span style="background-color: #f2f2f2;"> </span>   X_train_lstm,
<span style="background-color: #f2f2f2;"> </span>   y_train_cat,
<span style="background-color: #f2f2f2;"> </span>   epochs=200,         <span style="color: #b22222;"># </span><span style="color: #b22222;">n&#250;mero de &#233;pocas
</span><span style="background-color: #f2f2f2;"> </span>   batch_size=16,
<span style="background-color: #f2f2f2;"> </span>   validation_split=0.1,
<span style="background-color: #f2f2f2;"> </span>   verbose=1
<span style="color: #707183;">)</span>
</pre>
</div>

<p>
Durante el entrenamiento:
</p>
</div>
<div id="outline-container-orgf67a0b1" class="outline-4">
<h4 id="orgf67a0b1">Propagación hacia adelante (forward pass)</h4>
<div class="outline-text-4" id="text-orgf67a0b1">
<ul class="org-ul">
<li>Cada muestra de entrada se considera como una secuencia de 4 pasos.</li>
<li>En cada paso, la LSTM actualiza su:
<ul class="org-ul">
<li>estado oculto \(h_t\),</li>
<li>estado de memoria \(c_t\).</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org33f7fe5" class="outline-4">
<h4 id="org33f7fe5">Cálculo del error</h4>
<div class="outline-text-4" id="text-org33f7fe5">
<ul class="org-ul">
<li>La salida softmax se compara con la etiqueta real codificada en one-hot.</li>
<li>Se calcula la pérdida mediante entropía cruzada categórica.</li>
</ul>
</div>
</div>
<div id="outline-container-org6f85fbd" class="outline-4">
<h4 id="org6f85fbd">Backpropagation Through Time (BPTT)</h4>
<div class="outline-text-4" id="text-org6f85fbd">
<ul class="org-ul">
<li>El error se propaga hacia atrás a lo largo del tiempo (sobre los 4 pasos).</li>
<li>Se calculan los gradientes de los pesos de la LSTM y de la capa de salida.</li>
</ul>
</div>
</div>
<div id="outline-container-orgd5fb814" class="outline-4">
<h4 id="orgd5fb814">Descenso de gradiente con Adam</h4>
<div class="outline-text-4" id="text-orgd5fb814">
<ul class="org-ul">
<li>Adam combina Momentum y RMSProp.</li>
<li>Ajusta automáticamente la tasa de aprendizaje para cada parámetro.</li>
<li>Proporciona convergencia rápida y estable.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orga44f5b8" class="outline-3">
<h3 id="orga44f5b8">Evaluación del modelo</h3>
<div class="outline-text-3" id="text-orga44f5b8">
<p>
Una vez entrenado el modelo, se evalúa su desempeño usando datos no
vistos (conjunto de prueba).
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Probabilidades por clase
</span><span style="color: #a0522d;">y_pred_proba</span> = model.predict<span style="color: #707183;">(</span>X_test_lstm<span style="color: #707183;">)</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Clase predicha = &#237;ndice de la probabilidad m&#225;xima
</span><span style="color: #a0522d;">y_pred</span> = np.argmax<span style="color: #707183;">(</span>y_pred_proba, axis=1<span style="color: #707183;">)</span>

<span style="color: #483d8b;">print</span><span style="color: #707183;">(</span><span style="color: #8b2252;">"Accuracy:"</span>, accuracy_score<span style="color: #7388d6;">(</span>y_test, y_pred<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
<span style="color: #483d8b;">print</span><span style="color: #707183;">(</span><span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Reporte de clasificaci&#243;n:</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">"</span><span style="color: #707183;">)</span>
<span style="color: #483d8b;">print</span><span style="color: #707183;">(</span>classification_report<span style="color: #7388d6;">(</span>y_test, y_pred, target_names=iris.target_names<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>
</div>
<div id="outline-container-orgc6f9d34" class="outline-4">
<h4 id="orgc6f9d34">Métricas utilizadas</h4>
<div class="outline-text-4" id="text-orgc6f9d34">
<ul class="org-ul">
<li><b>Accuracy</b>: proporción total de predicciones correctas.</li>
<li><b>Precision</b>: qué tan confiables son las predicciones positivas.</li>
<li><b>Recall</b>: capacidad del modelo para encontrar todos los ejemplos de una clase.</li>
<li><b>F1-score</b>: balance entre precisión y recall.</li>
</ul>

<p>
En el dataset Iris, el accuracy también suele estar en el rango del
95 % al 100 %.
</p>
</div>
</div>
</div>
<div id="outline-container-orgcadf24e" class="outline-3">
<h3 id="orgcadf24e">Predicción con una nueva flor</h3>
<div class="outline-text-3" id="text-orgcadf24e">
<p>
El modelo entrenado puede utilizarse para predecir nuevas muestras.
Es importante repetir el mismo preprocesamiento: normalización y
reshape para LSTM.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">[largo_sepalo, ancho_sepalo, largo_petalo, ancho_petalo]
</span><span style="color: #a0522d;">flor_nueva</span> = np.array<span style="color: #707183;">(</span><span style="color: #7388d6;">[</span><span style="color: #909183;">[</span>5.1, 3.5, 1.4, 0.2<span style="color: #909183;">]</span><span style="color: #7388d6;">]</span><span style="color: #707183;">)</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">1) Normalizaci&#243;n con el mismo scaler del entrenamiento
</span><span style="color: #a0522d;">flor_nueva_scaled</span> = scaler.transform<span style="color: #707183;">(</span>flor_nueva<span style="color: #707183;">)</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">2) Reshape para LSTM: (1, timesteps=4, features=1)
</span><span style="color: #a0522d;">flor_nueva_lstm</span> = flor_nueva_scaled.reshape<span style="color: #707183;">(</span><span style="color: #7388d6;">(</span>1, 4, 1<span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">3) Predicci&#243;n
</span><span style="color: #a0522d;">proba</span> = model.predict<span style="color: #707183;">(</span>flor_nueva_lstm<span style="color: #707183;">)</span>
<span style="color: #a0522d;">prediccion</span> = np.argmax<span style="color: #707183;">(</span>proba, axis=1<span style="color: #707183;">)</span>

<span style="color: #483d8b;">print</span><span style="color: #707183;">(</span><span style="color: #8b2252;">"Clase predicha:"</span>, iris.target_names<span style="color: #7388d6;">[</span>prediccion<span style="color: #909183;">[</span>0<span style="color: #909183;">]</span><span style="color: #7388d6;">]</span><span style="color: #707183;">)</span>
</pre>
</div>

<p>
Es fundamental aplicar la misma normalización y la misma estructura de
entrada (forma (1, 4, 1)) que se usaron en el entrenamiento.
</p>
</div>
</div>
<div id="outline-container-orgfcd1968" class="outline-3">
<h3 id="orgfcd1968">Interpretación matemática básica de la LSTM</h3>
<div class="outline-text-3" id="text-orgfcd1968">
<p>
En una LSTM, en cada paso temporal \(t\) se procesan:
</p>

<ul class="org-ul">
<li>la entrada \(x_t\),</li>
<li>el estado oculto anterior \(h_{t-1}\),</li>
<li>el estado de memoria anterior \(c_{t-1}\).</li>
</ul>

<p>
La LSTM utiliza <b>puertas</b> que controlan el flujo de información:
</p>

<div class="latex" id="orgc1d181d">
\begin{aligned}
f_t &= \sigma(W_f [h_{t-1}, x_t] + b_f) \\
i_t &= \sigma(W_i [h_{t-1}, x_t] + b_i) \\
\tilde{c}_t &= \tanh(W_c [h_{t-1}, x_t] + b_c) \\
c_t &= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t \\
o_t &= \sigma(W_o [h_{t-1}, x_t] + b_o) \\
h_t &= o_t \odot \tanh(c_t)
\end{aligned}

</div>

<p>
donde:
</p>

<ul class="org-ul">
<li>\(\sigma\) es la función sigmoide,</li>
<li>\(\odot\) denota el producto elemento a elemento.</li>
</ul>

<p>
Al final de la secuencia (tras los 4 pasos), el último estado oculto
\(h_T\) se pasa a la capa densa con función softmax:
</p>

<div class="latex" id="org0f0aea0">
<p>
\text{softmax}(z<sub>i</sub>) = \frac{e<sup>z<sub>i</sub></sup>}{&sum;<sub>j</sub> e<sup>z<sub>j</sub></sup>}
</p>

</div>

<p>
lo que permite interpretar las salidas como probabilidades para cada
clase (Setosa, Versicolor, Virginica).
</p>
</div>
</div>
</div>
<div id="outline-container-org8a82987" class="outline-2">
<h2 id="org8a82987">Bibliografía  Deep Learning</h2>
<div class="outline-text-2" id="text-org8a82987">
</div>
<div id="outline-container-org3c076f8" class="outline-3">
<h3 id="org3c076f8">Teórica y Académica&#xa0;&#xa0;&#xa0;<span class="tag"><span class="MATEMATICAS">MATEMATICAS</span>&#xa0;<span class="FUNDAMENTOS">FUNDAMENTOS</span></span></h3>
<div class="outline-text-3" id="text-org3c076f8">
<p>
Libros diseñados para entender el "porqué" de los algoritmos.
</p>
</div>
<div id="outline-container-org1a1484a" class="outline-4">
<h4 id="org1a1484a">Deep Learning</h4>
<div class="outline-text-4" id="text-org1a1484a">
<ul class="org-ul">
<li><b>Autores:</b> Ian Goodfellow, Yoshua Bengio y Aaron Courville.</li>
<li><b>Editorial:</b> MIT Press (2016).</li>
<li><b>Nota:</b> Considerada la "Biblia" de la IA. Cubre álgebra lineal, probabilidad y arquitecturas complejas.</li>
<li><b>Web:</b> <a href="https://www.deeplearningbook.org/">Acceso gratuito a la versión web (HTML)</a></li>
</ul>
</div>
</div>
<div id="outline-container-org654aaee" class="outline-4">
<h4 id="org654aaee">Neural Networks and Deep Learning: A Textbook</h4>
<div class="outline-text-4" id="text-org654aaee">
<ul class="org-ul">
<li><b>Autor:</b> Charu C. Aggarwal.</li>
<li><b>Editorial:</b> Springer (2018).</li>
<li><b>Enfoque:</b> Muy estructurado para estudiantes de ingeniería y computación.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org913b264" class="outline-3">
<h3 id="org913b264">Práctica y Programación&#xa0;&#xa0;&#xa0;<span class="tag"><span class="PYTHON">PYTHON</span>&#xa0;<span class="PYTORCH">PYTORCH</span>&#xa0;<span class="KERAS">KERAS</span></span></h3>
<div class="outline-text-3" id="text-org913b264">
<p>
Libros enfocados en la implementación inmediata usando librerías de Python.
</p>
</div>
<div id="outline-container-org8bb1bdb" class="outline-4">
<h4 id="org8bb1bdb">Deep Learning with Python (3rd Edition)</h4>
<div class="outline-text-4" id="text-org8bb1bdb">
<ul class="org-ul">
<li><b>Autor:</b> François Chollet (Creador de Keras).</li>
<li><b>Editorial:</b> Manning Publications (2025).</li>
<li><b>Por qué leerlo:</b> Explicaciones excepcionales sobre la intuición detrás de las redes neuronales.</li>
<li><b>Enlace:</b> <a href="https://www.manning.com/books/deep-learning-with-python-third-edition">Página del libro</a></li>
</ul>
</div>
</div>
<div id="outline-container-org69f9d04" class="outline-4">
<h4 id="org69f9d04">Hands-On Machine Learning with Scikit-Learn and PyTorch</h4>
<div class="outline-text-4" id="text-org69f9d04">
<ul class="org-ul">
<li><b>Autor:</b> Aurélien Géron.</li>
<li><b>Editorial:</b> O'Reilly Media (Edición 2025).</li>
<li><b>Enfoque:</b> Es el manual más completo para pasar de la teoría a la práctica industrial. Cubre desde regresiones simples hasta Transformers.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgc8123ac" class="outline-3">
<h3 id="orgc8123ac">Aprendizaje "Desde Cero"&#xa0;&#xa0;&#xa0;<span class="tag"><span class="NUMPY">NUMPY</span>&#xa0;<span class="LOGICA">LOGICA</span></span></h3>
<div class="outline-text-3" id="text-orgc8123ac">
<p>
Ideales para entender cómo funciona el "motor" de una red neuronal sin usar herramientas automáticas.
</p>
</div>
<div id="outline-container-org1eefb2e" class="outline-4">
<h4 id="org1eefb2e">Grokking Deep Learning</h4>
<div class="outline-text-4" id="text-org1eefb2e">
<ul class="org-ul">
<li><b>Autor:</b> Andrew Trask.</li>
<li><b>Editorial:</b> Manning Publications.</li>
<li><b>Descripción:</b> Aprenderás a programar redes neuronales usando solo <code>NumPy</code>. Ideal si quieres entender la "caja negra".</li>
</ul>
</div>
</div>
<div id="outline-container-orgdae682c" class="outline-4">
<h4 id="orgdae682c">Neural Networks from Scratch (NNFS)</h4>
<div class="outline-text-4" id="text-orgdae682c">
<ul class="org-ul">
<li><b>Autores:</b> Harrison Kinsley (Sentdex) y Daniel Kukieła.</li>
<li><b>Descripción:</b> Basado en código puro de Python. Muy popular entre autodidactas.</li>
<li><b>Web:</b> <a href="https://nnfs.io/">Sitio Oficial de NNFS</a></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org590c047" class="outline-3">
<h3 id="org590c047">Tabla Comparativa para Elección Rápida</h3>
<div class="outline-text-3" id="text-org590c047">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Libro</th>
<th scope="col" class="org-left">Nivel</th>
<th scope="col" class="org-left">Enfoque Principal</th>
<th scope="col" class="org-left">Tecnología</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Goodfellow</td>
<td class="org-left">Avanzado</td>
<td class="org-left">Teoría/Matemáticas</td>
<td class="org-left">Genérico</td>
</tr>

<tr>
<td class="org-left">Chollet</td>
<td class="org-left">Intermedio</td>
<td class="org-left">Intuición</td>
<td class="org-left">Keras/TensorFlow</td>
</tr>

<tr>
<td class="org-left">Géron</td>
<td class="org-left">Todos</td>
<td class="org-left">Práctica/Industria</td>
<td class="org-left">PyTorch/Scikit-Learn</td>
</tr>

<tr>
<td class="org-left">Trask</td>
<td class="org-left">Principiante</td>
<td class="org-left">Lógica Interna</td>
<td class="org-left">NumPy</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Autor: Eduardo Alcaraz</p>
<p class="date">Created: 2026-01-20 mar 12:01</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
