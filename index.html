<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="es" xml:lang="es">
<head>
<!-- 2026-01-21 mié 08:55 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Redes Neuronales Artificiales</title>
<meta name="author" content="Eduardo Alcaraz" />
<meta name="generator" content="Org Mode" />
<style type="text/css">
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="src/readtheorg_theme/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="src/readtheorg_theme/css/readtheorg.css"/>
<script type="text/javascript" src="src/lib/js/jquery.min.js"></script>
<script type="text/javascript" src="src/lib/js/bootstrap.min.js"></script>
<script type="text/javascript" src="src/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="src/readtheorg_theme/js/readtheorg.js"></script>
<style> #content{max-width:1800px;}</style>
<style>pre.src {background-color: #303030; color: #e5e5e5;}</style>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Redes Neuronales Artificiales</h1>
<div id="table-of-contents" role="doc-toc">
<h2>&Iacute;ndice</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org40993d1">Presentación del Curso</a>
<ul>
<li><a href="#org56f28de">Objetivo general</a></li>
</ul>
</li>
<li><a href="#orgedc048f">Instalación de requerimientos</a>
<ul>
<li><a href="#org70268cc">Verificación de Python</a></li>
<li><a href="#org928c8cc">Creación del entorno virtual</a></li>
<li><a href="#orgee19668">Activación del entorno virtual</a>
<ul>
<li><a href="#orgc943269">GNU/Linux y macOS</a></li>
<li><a href="#orge9eeb8a">Windows – Símbolo del sistema (CMD)</a></li>
<li><a href="#org10b15a5">Windows – PowerShell</a></li>
<li><a href="#org48edf32">Windows – Git Bash</a></li>
</ul>
</li>
<li><a href="#orga2eaaec">Actualización del gestor de paquetes</a></li>
<li><a href="#orgc9b7737">Instalación de dependencias</a></li>
<li><a href="#orga942706">Verificación de la instalación</a></li>
<li><a href="#org604f55e">Desactivación del entorno virtual</a></li>
</ul>
</li>
<li><a href="#org0c0b2ff">Manual de Entornos Virtuales en Python</a>
<ul>
<li><a href="#orgbe5b436">Introducción</a></li>
<li><a href="#orga968817">Flujo de Trabajo Básico</a>
<ul>
<li><a href="#orgef55ea1">1. Creación del entorno</a></li>
<li><a href="#org9b4846f">2. Activación</a></li>
</ul>
</li>
<li><a href="#org668141b">Gestión de paquetes</a></li>
<li><a href="#orgbf5532c">El archivo de Requerimientos</a></li>
<li><a href="#orgbb5f6fa">Exportar dependencias</a></li>
<li><a href="#org35f6133">Instalar desde el archivo</a></li>
<li><a href="#org1805bf6">Tips de Limpieza</a></li>
<li><a href="#org13aa5cf">Entornos Virtuales Python (Edición Windows)</a></li>
<li><a href="#orga9ff0c8">Requisitos Previos</a></li>
<li><a href="#orgc106549">Flujo de Trabajo en Windows</a>
<ul>
<li><a href="#org234af45">1. Crear el Entorno Virtual</a></li>
<li><a href="#org9eace01">2. El Paso Crítico: La Activación</a></li>
<li><a href="#org2fd0ab5">3. Confirmación</a></li>
</ul>
</li>
<li><a href="#orgafc9128">Gestión de Librerías con PIP</a>
<ul>
<li><a href="#org03667ad">Instalación de paquetes</a></li>
<li><a href="#org1c0d3dd">Congelar dependencias (Compartir proyecto)</a></li>
<li><a href="#orgcc11e48">Instalar desde un archivo recibido</a></li>
</ul>
</li>
<li><a href="#org6cb7c58">Uso de Entornos en Emacs (Windows)</a>
<ul>
<li><a href="#orgded36d4">Instalación del paquete pyvenv</a></li>
<li><a href="#org3f1679f">Cómo activarlo dentro de Emacs</a></li>
</ul>
</li>
<li><a href="#orgdb9576a">Solución de Problemas Comunes en Windows</a></li>
<li><a href="#orgc559f4c">Desactivación y Limpieza</a></li>
<li><a href="#orgb67615c">Agregar a jupyter notebook</a></li>
</ul>
</li>
<li><a href="#orgdd9d627">Introducción a la Inteligencia Artificial (IA)</a>
<ul>
<li><a href="#org981316b">Definición formal</a></li>
<li><a href="#org1e3f40e">IA débil vs IA fuerte</a></li>
</ul>
</li>
<li><a href="#org85b13d9">Machine Learning (Aprendizaje Automático)</a>
<ul>
<li><a href="#org3c8e262">Tipos de aprendizaje</a></li>
</ul>
</li>
<li><a href="#org17268f7">Historia de las Redes Neuronales</a>
<ul>
<li><a href="#org43d8c97">Contexto histórico</a></li>
<li><a href="#orge067a16">McCulloch y Pitts (1943)</a></li>
<li><a href="#org5081243">Perceptrón y optimismo inicial</a></li>
<li><a href="#org307ee4e">Críticas y estancamiento</a></li>
<li><a href="#orga3efbe8">Renacimiento moderno</a></li>
</ul>
</li>
<li><a href="#org5aaeffc">La Neurona Artificial</a>
<ul>
<li><a href="#orgbc159cf">Motivación</a></li>
<li><a href="#orgca449e9">Componentes</a></li>
<li><a href="#org1ebe6e5">Modelo matemático</a></li>
</ul>
</li>
<li><a href="#orge03c85b">El Perceptrón</a>
<ul>
<li><a href="#org54d397b">Definición</a></li>
<li><a href="#orgaa799f1">Interpretación geométrica</a></li>
<li><a href="#org9ef3b03">Aprendizaje</a></li>
<li><a href="#org5653bb6">Limitaciones</a></li>
</ul>
</li>
<li><a href="#org48a6f77">¿Qué es un problema linealmente separable?</a>
<ul>
<li><a href="#org61d51b4">Intuición básica</a></li>
</ul>
</li>
<li><a href="#org11585c6">Estructura de un Perceptrón Simple</a>
<ul>
<li><a href="#org9263ea7">Entradas (Inputs)</a></li>
<li><a href="#orgc3795fb">Pesos Sinápticos (Weights)</a></li>
<li><a href="#org7e8a1f0">Suma Ponderada (Weighted Sum)</a></li>
<li><a href="#org7015b5e">Sesgo (Bias)</a></li>
<li><a href="#orge2e03e0">Función de Activación</a></li>
<li><a href="#org48b3368">Salida (Output)</a></li>
</ul>
</li>
<li><a href="#orgdf4db23">Ejemplo Tacos</a>
<ul>
<li><a href="#orgfa2cff3">1. El Escenario: La Decisión de la Cena</a>
<ul>
<li><a href="#orgcd71421">Nuestras Entradas (Inputs)</a></li>
<li><a href="#org6518b8d">El Peso (Weight): La Importancia</a></li>
</ul>
</li>
<li><a href="#org8e3ff16">2. ¿Cómo toma la decisión? (La Suma y el Sesgo)</a>
<ul>
<li><a href="#orgbb14525">El Sesgo (Bias)</a></li>
</ul>
</li>
<li><a href="#orgbb41e1c">3. El Descenso del Gradiente: ¿Cómo "Aprende" la Neurona?</a>
<ul>
<li><a href="#org4a7fe1a">El Error: La Brújula</a></li>
<li><a href="#org266a0b8">El Ajuste (La Regla Delta)</a></li>
</ul>
</li>
<li><a href="#org8a15308">Perceptrón ejemplo (Python)</a>
<ul>
<li><a href="#org8408b10">Perceptrón para una compuerta "AND"</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org87059ae">Codigo C python</a></li>
<li><a href="#org967687f">Redes Neuronales Multicapa (Multilayer Perceptron)</a>
<ul>
<li><a href="#orgb7c23d3">Introducción</a></li>
<li><a href="#orga714d92">Modelo matemático</a>
<ul>
<li><a href="#org737d32d">Neurona individual</a></li>
<li><a href="#org67ccbfd">Forma matricial de una capa</a></li>
</ul>
</li>
<li><a href="#org9c86d9d">Funciones de activación</a>
<ul>
<li><a href="#orgad7e119">¿Para qué sirven las funciones de activación?</a></li>
</ul>
</li>
<li><a href="#orgde5ceef">Clasificación de las funciones de activación</a>
<ul>
<li><a href="#org23c6965">Funciones de activación clásicas</a></li>
<li><a href="#org82e1890">Funciones de activación modernas</a></li>
<li><a href="#org89fa44b">Funciones de activación en la capa de salida</a></li>
<li><a href="#org8fadb20">Impacto en el entrenamiento</a></li>
</ul>
</li>
<li><a href="#org4033825">Propagación hacia adelante (Forward Propagation)</a></li>
<li><a href="#org7479849">Funciones de pérdida</a>
<ul>
<li><a href="#org8d9668e">Error cuadrático medio (MSE)</a></li>
<li><a href="#orgff0870b">Entropía cruzada (Cross-Entropy)</a></li>
</ul>
</li>
<li><a href="#org50eed8d">Backpropagation</a></li>
<li><a href="#org2ae20e6">Descenso de gradiente</a></li>
</ul>
</li>
<li><a href="#orgaae5d32">Ejemplo Clasificación del dataset Iris con MLP</a>
<ul>
<li><a href="#orgb9f554a">Reproducibilidad del experimento</a></li>
<li><a href="#org1c6e1c5">Estratificación de clases</a></li>
<li><a href="#org36797cc">Importancia en redes neuronales</a></li>
<li><a href="#org3636ec6">Normalización de los datos</a></li>
<li><a href="#orgc994b47">Definir el Perceptrón Multicapa (MLP)</a>
<ul>
<li><a href="#org8a9958c">Arquitectura de la red neuronal</a></li>
<li><a href="#org619b778">Capa de entrada</a></li>
<li><a href="#orgba15e1a">Capas ocultas</a></li>
<li><a href="#org8d86e64">Función de activación</a></li>
<li><a href="#org21da9db">Capa de salida</a></li>
<li><a href="#org36595f6">Entrenamiento del modelo</a></li>
</ul>
</li>
<li><a href="#org67ba23d">Número de iteraciones</a></li>
<li><a href="#org5ef4b45">Reproducibilidad</a></li>
<li><a href="#org442a76a">Entrenamiento del modelo</a>
<ul>
<li><a href="#org3a2ed56">Propagación hacia adelante (forward pass)</a></li>
<li><a href="#org57945b6">Cálculo del error</a></li>
<li><a href="#org6be2449">Backpropagation</a></li>
<li><a href="#org3bd576b">Descenso de gradiente con Adam</a></li>
</ul>
</li>
<li><a href="#org93e8393">Evaluación del modelo</a>
<ul>
<li><a href="#orga226bf1">Métricas utilizadas</a></li>
</ul>
</li>
<li><a href="#orgc8cf9ea">Predicción con una nueva flor</a></li>
<li><a href="#orgf8f62fb">Interpretación matemática del MLP</a></li>
</ul>
</li>
<li><a href="#org1a594ca">Ejemplo Clasificación de Cáncer de Mama con MLP</a>
<ul>
<li><a href="#orgcfb19ed">Importar librerías</a></li>
<li><a href="#orge0aca29">Cargar el dataset Breast Cancer</a>
<ul>
<li><a href="#orgdff9e80">Importancia del dominio</a></li>
</ul>
</li>
<li><a href="#orga00e48e">Partición del conjunto de datos</a>
<ul>
<li><a href="#orgc32a140">Significado de los parámetros</a></li>
<li><a href="#orga7160b9">Propósito de la partición</a></li>
</ul>
</li>
<li><a href="#orgd523696">Normalización (Estandarización)</a>
<ul>
<li><a href="#org04bd439">Importante</a></li>
</ul>
</li>
<li><a href="#org5ecb641">Definición del Perceptrón Multicapa (MLP)</a>
<ul>
<li><a href="#orgdb08b61">Arquitectura del modelo</a></li>
<li><a href="#org01f7b3b">Función de activación ReLU</a></li>
<li><a href="#org9685931">Optimizador Adam</a></li>
<li><a href="#org9e89726">Número máximo de iteraciones</a></li>
<li><a href="#orgf000720">Reproducibilidad</a></li>
</ul>
</li>
<li><a href="#org6df4285">Entrenamiento del modelo</a>
<ul>
<li><a href="#orgfa9f612">Proceso interno (visión conceptual)</a></li>
</ul>
</li>
<li><a href="#orgc1c0bc8">Evaluación del modelo</a>
<ul>
<li><a href="#org29cb597">Métricas principales</a></li>
</ul>
</li>
<li><a href="#orgbb4f643">Interpretación matemática de la salida binaria</a></li>
<li><a href="#org8beeb5f">Predicción de una nueva muestra</a>
<ul>
<li><a href="#orgc9397bb">Comentarios importantes</a></li>
</ul>
</li>
<li><a href="#org9adda36">Resumen conceptual</a></li>
</ul>
</li>
<li><a href="#org9ccd6ed">Juego de Bala y Salto con MLP</a>
<ul>
<li><a href="#org32d909e">Introducción</a>
<ul>
<li><a href="#orgce618db">¿Qué es este juego?</a></li>
<li><a href="#org1da87d8">Concepto principal</a></li>
<li><a href="#orgab8bc73">Objetivo del juego</a></li>
</ul>
</li>
<li><a href="#org6003f0e">Requisitos e Instalación</a>
<ul>
<li><a href="#orga871fae">Dependencias necesarias</a></li>
<li><a href="#org908e2fb">Instalación</a></li>
<li><a href="#org9e8a58f">Estructura de archivos</a></li>
</ul>
</li>
<li><a href="#org011ba29">Cómo Jugar</a>
<ul>
<li><a href="#orgb97189e">Iniciar el juego</a></li>
<li><a href="#org63a3479">Menú principal</a></li>
<li><a href="#org6a7cfea">Controles durante el juego</a></li>
</ul>
</li>
<li><a href="#org51f5598">Modo Manual: Recolectando Datos</a>
<ul>
<li><a href="#org9e2e54b">¿Qué hace el modo manual?</a></li>
<li><a href="#org6417dd5">Cómo jugar en modo manual</a></li>
<li><a href="#org863cdab">Consejos para recolectar buenos datos</a></li>
<li><a href="#orge0c2c55">Registro de datos</a></li>
</ul>
</li>
<li><a href="#orgb0aac5a">Entrenamiento del Modelo MLP</a>
<ul>
<li><a href="#org0edd30d">¿Qué es un MLP?</a></li>
<li><a href="#org69f4002">Arquitectura del modelo</a></li>
<li><a href="#org98573b9">Cómo entrenar</a></li>
<li><a href="#org5d637a9">Casos especiales</a></li>
<li><a href="#org90690ea">Interpretando los resultados</a></li>
</ul>
</li>
<li><a href="#orgfdb8df4">Modo Auto: El MLP Juega Solo</a>
<ul>
<li><a href="#org341dc7d">¿Qué hace el modo auto?</a></li>
<li><a href="#org8c29423">Cómo activar modo auto</a></li>
<li><a href="#orgf3524bb">Visualización en tiempo real</a></li>
<li><a href="#org051e40e">Limitaciones del modo auto</a></li>
</ul>
</li>
<li><a href="#orgd577421">Exportación y Visualización de Datos</a>
<ul>
<li><a href="#org53d6a76">Exportar a CSV</a></li>
<li><a href="#org53fa124">Visualizar los datos</a></li>
<li><a href="#org0d9ce84">Interpretando las gráficas</a></li>
</ul>
</li>
<li><a href="#org136715c">Arquitectura del Código</a>
<ul>
<li><a href="#org029746d">Estructura principal</a></li>
<li><a href="#org83040c1">Clase Sample</a></li>
<li><a href="#org8e9a16b">Flujo de datos</a></li>
</ul>
</li>
<li><a href="#org9ea266c">Parámetros Configurables</a>
<ul>
<li><a href="#org62b8f2d">Parámetros del juego</a></li>
<li><a href="#org029443b">Parámetros del MLP</a></li>
<li><a href="#org7c0e74f">Parámetros de registro</a></li>
</ul>
</li>
<li><a href="#org2bca2c8">Mejores Prácticas</a>
<ul>
<li><a href="#org71ee0b1">Recolectando datos de calidad</a></li>
<li><a href="#org6cef365">Entrenando el modelo</a></li>
<li><a href="#org3f53237">Jugando en modo auto</a></li>
</ul>
</li>
<li><a href="#org74204b1">Solución de Problemas</a>
<ul>
<li><a href="#org07f654e">El modelo no aprende (accuracy &lt; 0.5)</a></li>
<li><a href="#org07d4d2a">El modo auto no salta nunca</a></li>
<li><a href="#org8a5193f">El modo auto salta demasiado</a></li>
<li><a href="#orgcb14bb0">Las gráficas no se muestran</a></li>
<li><a href="#org3fae388">El juego se ve lento o con lag</a></li>
</ul>
</li>
<li><a href="#orgc1177cc">Conceptos Técnicos Avanzados</a>
<ul>
<li><a href="#orgc85aeb9">¿Por qué MLP y no otro algoritmo?</a></li>
<li><a href="#org77f8267">Normalización (StandardScaler)</a></li>
<li><a href="#orgaaed977">División train/test</a></li>
<li><a href="#org5eac6e7">Overfitting y Underfitting</a></li>
</ul>
</li>
<li><a href="#orgad1fb95">Código de Ejemplo</a>
<ul>
<li><a href="#orgdfc45b6">Cargar y visualizar datos manualmente</a></li>
<li><a href="#org2fbd753">Entrenar modelo desde CSV</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orge463798">Métricas de Evaluación para Machine Learning</a>
<ul>
<li><a href="#orgce3045e">Introducción</a>
<ul>
<li><a href="#orgbe6518b">¿Qué son las métricas de evaluación?</a></li>
<li><a href="#org57d4141">¿Por qué son importantes?</a></li>
</ul>
</li>
<li><a href="#org81fb02d">Matriz de Confusión: La Base de Todo</a>
<ul>
<li><a href="#org31c9847">¿Qué es una matriz de confusión?</a></li>
<li><a href="#orgeb2ab39">Estructura de la matriz (2 clases)</a></li>
<li><a href="#org07177f8">Interpretación en el contexto del juego</a></li>
<li><a href="#org69d5d15">Ejemplo práctico</a></li>
</ul>
</li>
<li><a href="#orgc95aba1">Accuracy (Precisión General)</a>
<ul>
<li><a href="#org998cd0d">Definición</a></li>
<li><a href="#org2fb3879">Cálculo del ejemplo anterior</a></li>
<li><a href="#org4f9491c">Ventajas</a></li>
<li><a href="#org7e04e13">Desventajas</a></li>
<li><a href="#orgef4fa6a">Ejemplo del problema</a></li>
<li><a href="#orgb5f218c">Cuándo usar Accuracy</a></li>
</ul>
</li>
<li><a href="#orgd2a05c0">Precision (Precisión)</a>
<ul>
<li><a href="#orgf842d4a">Definición</a></li>
<li><a href="#org233c836">Cálculo del ejemplo</a></li>
<li><a href="#orgee9e20b">Interpretación</a></li>
<li><a href="#orga9cf908">En el contexto del juego</a></li>
<li><a href="#org8974ee7">Cuándo es importante</a></li>
</ul>
</li>
<li><a href="#org3aad789">Recall (Sensibilidad o Exhaustividad)</a>
<ul>
<li><a href="#org2d64326">Definición</a></li>
<li><a href="#orga674d11">Cálculo del ejemplo</a></li>
<li><a href="#org020cf8d">Interpretación</a></li>
<li><a href="#org079b905">En el contexto del juego</a></li>
<li><a href="#org0a9335e">Cuándo es crítico</a></li>
<li><a href="#org7dc8878">Trade-off Precision vs Recall</a></li>
</ul>
</li>
<li><a href="#orge49b723">F1-Score: El Balance Perfecto</a>
<ul>
<li><a href="#org6936d67">Definición</a></li>
<li><a href="#org186e411">Cálculo del ejemplo</a></li>
<li><a href="#org069f357">¿Por qué media armónica y no aritmética?</a></li>
<li><a href="#org9c88e78">Ventajas</a></li>
<li><a href="#orgdb9d64e">Desventajas</a></li>
<li><a href="#org12c569a">Cuándo usar F1-Score</a></li>
</ul>
</li>
<li><a href="#orgae3be4e">Especificidad</a>
<ul>
<li><a href="#org9e0112a">Definición</a></li>
<li><a href="#org11755b6">Cálculo del ejemplo</a></li>
<li><a href="#org37ee1b6">Interpretación</a></li>
<li><a href="#org4484503">En el contexto del juego</a></li>
</ul>
</li>
<li><a href="#orgb716a39">Comparación de Métricas: Tabla Resumen</a>
<ul>
<li><a href="#orgf42b41c">Valores del ejemplo</a></li>
<li><a href="#org0424c05">¿Qué métrica usar?</a></li>
</ul>
</li>
<li><a href="#org9d310ac">Casos de Uso en el Juego</a>
<ul>
<li><a href="#orga5a07f6">Escenario 1: Modelo Conservador</a></li>
<li><a href="#orgeb6d638">Escenario 2: Modelo Agresivo</a></li>
<li><a href="#org3321673">Escenario 3: Modelo Balanceado (Ideal)</a></li>
</ul>
</li>
<li><a href="#org126a5fb">Implementación en Python</a>
<ul>
<li><a href="#orgde87149">Cálculo manual de métricas</a></li>
<li><a href="#org4e64726">Reporte completo de clasificación</a></li>
<li><a href="#orgaeff331">Visualización de la matriz de confusión</a></li>
</ul>
</li>
<li><a href="#org16a5b04">Mejorando el Juego con Métricas</a>
<ul>
<li><a href="#org69b6001">Añadir métricas al entrenamiento</a></li>
<li><a href="#orga705a19">Interpretar resultados en el juego</a></li>
</ul>
</li>
<li><a href="#org74cdf0d">Curvas ROC y AUC</a>
<ul>
<li><a href="#org1dad432">¿Qué es la curva ROC?</a></li>
<li><a href="#orgc7255e5">¿Qué es AUC?</a></li>
<li><a href="#orgc66e912">Código para generar curva ROC</a></li>
<li><a href="#org087fd99">Interpretación</a></li>
</ul>
</li>
<li><a href="#org9bbcf26">Métricas para Clases Desbalanceadas</a>
<ul>
<li><a href="#org0632c0e">El problema</a></li>
<li><a href="#org8054b83">Soluciones</a></li>
</ul>
</li>
<li><a href="#orgaa82ca2">Resumen: Guía Rápida</a>
<ul>
<li><a href="#org7787ea3">Tabla de métricas clave</a></li>
<li><a href="#org348356c">Decisión rápida: ¿Qué métrica usar?</a></li>
<li><a href="#org4e42d27">Checklist de evaluación</a></li>
</ul>
</li>
<li><a href="#org1a759ff">Ejercicios Prácticos</a>
<ul>
<li><a href="#org4c6b10d">Ejercicio 1: Calcular métricas manualmente</a></li>
<li><a href="#org85d1624">Ejercicio 2: Interpretar resultados</a></li>
<li><a href="#org172a045">Ejercicio 3: Mejorar un modelo</a></li>
</ul>
</li>
<li><a href="#orgc38b790">Referencias y Recursos</a>
<ul>
<li><a href="#orge4fa7e9">Documentación oficial</a></li>
<li><a href="#org9cedc72">Lecturas recomendadas</a></li>
<li><a href="#orgeb319c5">Herramientas útiles</a></li>
</ul>
</li>
<li><a href="#orgb2e3501">Conclusión</a></li>
</ul>
</li>
<li><a href="#org23c386f">Redes Neuronales LSTM (Long Short-Term Memory)</a>
<ul>
<li><a href="#org07e416b">Estructura general de una RNN</a>
<ul>
<li><a href="#orgcd6e749">Problema: vanishing y exploding gradients</a></li>
</ul>
</li>
<li><a href="#orge603905">Motivación de las LSTM</a></li>
<li><a href="#org21fc0f5">Arquitectura interna de una celda LSTM</a>
<ul>
<li><a href="#org01725c3">Ecuaciones de la LSTM</a></li>
<li><a href="#orgb05eb1c">Interpretación de las puertas</a></li>
</ul>
</li>
<li><a href="#orgb4f8d5e">Intuición conceptual</a></li>
<li><a href="#org2e836ca">Entrenamiento de una LSTM</a></li>
<li><a href="#orgc089635">Tipos de tareas típicas con LSTM</a></li>
<li><a href="#orgca91fd2">Variantes y extensiones de LSTM</a></li>
<li><a href="#org66df1da">Implementación básica en Python (Keras)</a></li>
<li><a href="#org8303ff2">Ventajas y limitaciones de LSTM</a>
<ul>
<li><a href="#org1018157">Ventajas</a></li>
<li><a href="#orge2eea2c">Limitaciones</a></li>
</ul>
</li>
<li><a href="#orgc7c6fac">Conclusión</a></li>
</ul>
</li>
<li><a href="#org3a04f2e">Ejemplo Clasificación del dataset Iris con una red LSTM</a>
<ul>
<li><a href="#org5e4703d">Importar librerías</a></li>
<li><a href="#org40ca51f">Cargar el dataset Iris</a></li>
<li><a href="#org88ba2d6">Partición entrenamiento / prueba</a>
<ul>
<li><a href="#orgcf3c5bf">Propósito de la partición</a></li>
<li><a href="#org4f9ef89">Parámetro test<sub>size</sub></a></li>
<li><a href="#org6ae8ec9">Reproducibilidad del experimento</a></li>
<li><a href="#org6c6c743">Estratificación de clases</a></li>
</ul>
</li>
<li><a href="#org1d9c6e9">Normalización de los datos</a></li>
<li><a href="#orgc4f8d9e">Preparar los datos para LSTM</a></li>
<li><a href="#org1d83e72">Definir la red LSTM</a>
<ul>
<li><a href="#org3b03d1f">Arquitectura de la red</a></li>
</ul>
</li>
<li><a href="#org1d194bf">Compilación del modelo</a></li>
<li><a href="#org930bdb9">Entrenamiento del modelo LSTM</a>
<ul>
<li><a href="#orgf4cc0ac">Propagación hacia adelante (forward pass)</a></li>
<li><a href="#org5b28343">Cálculo del error</a></li>
<li><a href="#org94beedc">Backpropagation Through Time (BPTT)</a></li>
<li><a href="#org24a3254">Descenso de gradiente con Adam</a></li>
</ul>
</li>
<li><a href="#org6ffc5fb">Evaluación del modelo</a>
<ul>
<li><a href="#orgd269206">Métricas utilizadas</a></li>
</ul>
</li>
<li><a href="#org766bf73">Predicción con una nueva flor</a></li>
<li><a href="#orgfc975eb">Interpretación matemática básica de la LSTM</a></li>
</ul>
</li>
<li><a href="#orgf296ad0">Bibliografía  Deep Learning</a>
<ul>
<li><a href="#orgaab7740">Teórica y Académica&#xa0;&#xa0;&#xa0;<span class="tag"><span class="MATEMATICAS">MATEMATICAS</span>&#xa0;<span class="FUNDAMENTOS">FUNDAMENTOS</span></span></a>
<ul>
<li><a href="#org285668d">Deep Learning</a></li>
<li><a href="#org2dd37ca">Neural Networks and Deep Learning: A Textbook</a></li>
</ul>
</li>
<li><a href="#org15e7d2b">Práctica y Programación&#xa0;&#xa0;&#xa0;<span class="tag"><span class="PYTHON">PYTHON</span>&#xa0;<span class="PYTORCH">PYTORCH</span>&#xa0;<span class="KERAS">KERAS</span></span></a>
<ul>
<li><a href="#org97dee16">Deep Learning with Python (3rd Edition)</a></li>
<li><a href="#orgf0c24bc">Hands-On Machine Learning with Scikit-Learn and PyTorch</a></li>
</ul>
</li>
<li><a href="#orgc072223">Aprendizaje "Desde Cero"&#xa0;&#xa0;&#xa0;<span class="tag"><span class="NUMPY">NUMPY</span>&#xa0;<span class="LOGICA">LOGICA</span></span></a>
<ul>
<li><a href="#orgf966900">Grokking Deep Learning</a></li>
<li><a href="#org3978b2a">Neural Networks from Scratch (NNFS)</a></li>
</ul>
</li>
<li><a href="#org112a6d8">Tabla Comparativa para Elección Rápida</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-org40993d1" class="outline-2">
<h2 id="org40993d1">Presentación del Curso</h2>
<div class="outline-text-2" id="text-org40993d1">
</div>
<div id="outline-container-org56f28de" class="outline-3">
<h3 id="org56f28de">Objetivo general</h3>
<div class="outline-text-3" id="text-org56f28de">
<p>
Este curso tiene como objetivo proporcionar una
comprensión <b>profunda y progresiva</b> de las redes neuronales
artificiales, desde sus fundamentos teóricos hasta las principales
arquitecturas modernas utilizadas en la industria y la
investigación.
</p>


<ul class="org-ul">
<li>Fundamentos matemáticos y conceptuales</li>
<li>Funcionamiento interno de una red neuronal</li>
<li>Entrenamiento y optimización</li>
<li>Arquitecturas principales</li>
<li>Intuición práctica y casos de uso</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgedc048f" class="outline-2">
<h2 id="orgedc048f">Instalación de requerimientos</h2>
<div class="outline-text-2" id="text-orgedc048f">
</div>
<div id="outline-container-org70268cc" class="outline-3">
<h3 id="org70268cc">Verificación de Python</h3>
<div class="outline-text-3" id="text-org70268cc">
<p>
Antes de instalar las dependencias, verifique que el sistema cuente con Python
versión 3.9 o superior.
</p>

<div class="org-src-container">
<pre class="src src-bash">python --version
</pre>
</div>

<p>
En caso de que el comando anterior no esté disponible, intente:
</p>

<div class="org-src-container">
<pre class="src src-bash">python3 --version
</pre>
</div>
</div>
</div>
<div id="outline-container-org928c8cc" class="outline-3">
<h3 id="org928c8cc">Creación del entorno virtual</h3>
<div class="outline-text-3" id="text-org928c8cc">
<p>
Se recomienda crear un entorno virtual para aislar las dependencias del proyecto
y evitar conflictos entre versiones de librerías.
</p>

<div class="org-src-container">
<pre class="src src-bash">python -m venv mlp_env
</pre>
</div>
</div>
</div>
<div id="outline-container-orgee19668" class="outline-3">
<h3 id="orgee19668">Activación del entorno virtual</h3>
<div class="outline-text-3" id="text-orgee19668">
<p>
La activación del entorno virtual depende del sistema operativo y del intérprete
de comandos utilizado.
</p>
</div>
<div id="outline-container-orgc943269" class="outline-4">
<h4 id="orgc943269">GNU/Linux y macOS</h4>
<div class="outline-text-4" id="text-orgc943269">
<div class="org-src-container">
<pre class="src src-bash"><span style="color: #fe8019;">source</span> mlp_env/bin/activate
</pre>
</div>
</div>
</div>
<div id="outline-container-orge9eeb8a" class="outline-4">
<h4 id="orge9eeb8a">Windows – Símbolo del sistema (CMD)</h4>
<div class="outline-text-4" id="text-orge9eeb8a">
<p>
Si se utiliza el <b>Símbolo del sistema</b> (cmd.exe), ejecute:
</p>

<div class="org-src-container">
<pre class="src src-bat">mlp_env\Scripts\activate.bat
</pre>
</div>
</div>
</div>
<div id="outline-container-org10b15a5" class="outline-4">
<h4 id="org10b15a5">Windows – PowerShell</h4>
<div class="outline-text-4" id="text-org10b15a5">
<p>
Si se utiliza <b>Windows PowerShell</b>, ejecute:
</p>

<div class="org-src-container">
<pre class="src src-powershell">mlp_env\Scripts\Activate.ps1
</pre>
</div>

<p>
En caso de que la ejecución de scripts esté deshabilitada, habilítela
temporalmente con:
</p>

<div class="org-src-container">
<pre class="src src-powershell">Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process
</pre>
</div>

<p>
Posteriormente, vuelva a ejecutar el comando de activación.
</p>
</div>
</div>
<div id="outline-container-org48edf32" class="outline-4">
<h4 id="org48edf32">Windows – Git Bash</h4>
<div class="outline-text-4" id="text-org48edf32">
<p>
Si se utiliza <b>Git Bash</b>, ejecute:
</p>

<div class="org-src-container">
<pre class="src src-bash"><span style="color: #fe8019;">source</span> mlp_env/Scripts/activate
</pre>
</div>

<p>
Una vez activado el entorno virtual, el nombre del entorno aparecerá entre
paréntesis al inicio de la línea de comandos.
</p>
</div>
</div>
</div>
<div id="outline-container-orga2eaaec" class="outline-3">
<h3 id="orga2eaaec">Actualización del gestor de paquetes</h3>
<div class="outline-text-3" id="text-orga2eaaec">
<p>
Antes de instalar las librerías, se recomienda actualizar el gestor de paquetes
<code>pip</code>.
</p>

<div class="org-src-container">
<pre class="src src-bash">pip install --upgrade pip
</pre>
</div>
</div>
</div>
<div id="outline-container-orgc9b7737" class="outline-3">
<h3 id="orgc9b7737">Instalación de dependencias</h3>
<div class="outline-text-3" id="text-orgc9b7737">
<p>
Ejecute el siguiente comando para instalar los requerimientos del módulo
Perceptrón Multicapa <b>feedforward</b>.
</p>

<div class="org-src-container">
<pre class="src src-bash">pip install numpy matplotlib scikit-learn pygame
</pre>
</div>
</div>
</div>
<div id="outline-container-orga942706" class="outline-3">
<h3 id="orga942706">Verificación de la instalación</h3>
<div class="outline-text-3" id="text-orga942706">
<p>
Para verificar que las dependencias se instalaron correctamente, ejecute Python
en modo interactivo:
</p>

<div class="org-src-container">
<pre class="src src-bash">python
</pre>
</div>

<p>
Posteriormente, importe las librerías:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #fb4934;">import</span> numpy
<span style="color: #fb4934;">import</span> matplotlib
<span style="color: #fb4934;">import</span> sklearn
<span style="color: #fb4934;">import</span> pygame
<span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span><span style="color: #b8bb26;">"Instalaci&#243;n de requerimientos completada correctamente"</span><span style="color: #fe8019;">)</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-org604f55e" class="outline-3">
<h3 id="org604f55e">Desactivación del entorno virtual</h3>
<div class="outline-text-3" id="text-org604f55e">
<p>
Una vez finalizado el trabajo, el entorno virtual puede desactivarse con el
siguiente comando:
</p>

<div class="org-src-container">
<pre class="src src-bash">deactivate
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org0c0b2ff" class="outline-2">
<h2 id="org0c0b2ff">Manual de Entornos Virtuales en Python</h2>
<div class="outline-text-2" id="text-org0c0b2ff">
</div>
<div id="outline-container-orgbe5b436" class="outline-3">
<h3 id="orgbe5b436">Introducción</h3>
<div class="outline-text-3" id="text-orgbe5b436">
<p>
El uso de entornos virtuales es esencial para mantener las
dependencias de tus proyectos aisladas. En este manual aprenderás a
gestionarlos usando el módulo estándar <code>venv</code>.
</p>
</div>
</div>
<div id="outline-container-orga968817" class="outline-3">
<h3 id="orga968817">Flujo de Trabajo Básico</h3>
<div class="outline-text-3" id="text-orga968817">
</div>
<div id="outline-container-orgef55ea1" class="outline-4">
<h4 id="orgef55ea1">1. Creación del entorno</h4>
<div class="outline-text-4" id="text-orgef55ea1">
<p>
Para crear un entorno virtual, navega a la raíz de tu proyecto en la terminal (o dentro de un buffer de Emacs con <code>M-x shell</code>) y ejecuta:
</p>

<div class="org-src-container">
<pre class="src src-bash">python -m venv .venv
</pre>
</div>

<p>
<b>Nota:</b> El nombre <code>.venv</code> es una convención que hace que el directorio sea oculto en sistemas Unix.
</p>
</div>
</div>
<div id="outline-container-org9b4846f" class="outline-4">
<h4 id="org9b4846f">2. Activación</h4>
<div class="outline-text-4" id="text-org9b4846f">
<p>
La activación depende de tu sistema operativo:
</p>
</div>
<ul class="org-ul">
<li><a id="org9dcec4f"></a>En Windows (PowerShell)<br />
<div class="outline-text-5" id="text-org9dcec4f">
<div class="org-src-container">
<pre class="src src-powershell">.\.venv\Scripts\Activate.ps1
</pre>
</div>
</div>
</li>
<li><a id="orgc031b0f"></a>En macOS / Linux<br />
<div class="outline-text-5" id="text-orgc031b0f">
<div class="org-src-container">
<pre class="src src-bash"><span style="color: #fe8019;">source</span> .venv/bin/activate
</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div id="outline-container-org668141b" class="outline-3">
<h3 id="org668141b">Gestión de paquetes</h3>
<div class="outline-text-3" id="text-org668141b">
<p>
Una vez activado (verás el prefijo <code>(.venv)</code> en tu prompt), puedes instalar librerías:
</p>

<div class="org-src-container">
<pre class="src src-bash">pip install requests pandas
</pre>
</div>
</div>
</div>
<div id="outline-container-orgbf5532c" class="outline-3">
<h3 id="orgbf5532c">El archivo de Requerimientos</h3>
<div class="outline-text-3" id="text-orgbf5532c">
<p>
Es fundamental para la reproducibilidad del proyecto.
</p>
</div>
</div>
<div id="outline-container-orgbb5f6fa" class="outline-3">
<h3 id="orgbb5f6fa">Exportar dependencias</h3>
<div class="outline-text-3" id="text-orgbb5f6fa">
<div class="org-src-container">
<pre class="src src-bash">pip freeze &gt; requirements.txt
</pre>
</div>
</div>
</div>
<div id="outline-container-org35f6133" class="outline-3">
<h3 id="org35f6133">Instalar desde el archivo</h3>
<div class="outline-text-3" id="text-org35f6133">
<div class="org-src-container">
<pre class="src src-bash">pip install -r requirements.txt
</pre>
</div>
</div>
</div>
<div id="outline-container-org1805bf6" class="outline-3">
<h3 id="org1805bf6">Tips de Limpieza</h3>
<div class="outline-text-3" id="text-org1805bf6">
<p>
Para salir del entorno virtual:
</p>
<div class="org-src-container">
<pre class="src src-bash">deactivate
</pre>
</div>

<p>
Para borrar el entorno, simplemente elimina la carpeta:
</p>
<div class="org-src-container">
<pre class="src src-bash">rm -rf .venv  <span style="color: #928374;"># </span><span style="color: #928374;">En Linux/macOS
</span>rmdir /s /q .venv  <span style="color: #928374;"># </span><span style="color: #928374;">En Windows</span>
</pre>
</div>

<p>
&#x2014;
</p>
<blockquote>
<p>
"Keep your global Python clean, keep your projects isolated."
</p>
</blockquote>
</div>
</div>
<div id="outline-container-org13aa5cf" class="outline-3">
<h3 id="org13aa5cf">Entornos Virtuales Python (Edición Windows)</h3>
</div>

<div id="outline-container-orga9ff0c8" class="outline-3">
<h3 id="orga9ff0c8">Requisitos Previos</h3>
<div class="outline-text-3" id="text-orga9ff0c8">
<ol class="org-ol">
<li>Tener instalado Python (descargado de <a href="https://www.python.org/">python.org</a> o la Microsoft Store).</li>
<li>Durante la instalación, asegúrate de marcar la casilla: <b><b>"Add Python to PATH"</b></b>.</li>
</ol>
</div>
</div>
<div id="outline-container-orgc106549" class="outline-3">
<h3 id="orgc106549">Flujo de Trabajo en Windows</h3>
<div class="outline-text-3" id="text-orgc106549">
</div>
<div id="outline-container-org234af45" class="outline-4">
<h4 id="org234af45">1. Crear el Entorno Virtual</h4>
<div class="outline-text-4" id="text-org234af45">
<p>
Abre tu terminal (PowerShell o CMD) en la carpeta de tu proyecto. El comando es el mismo para ambos:
</p>

<div class="org-src-container">
<pre class="src src-powershell">python -m venv venv
</pre>
</div>
</div>
</div>
<div id="outline-container-org9eace01" class="outline-4">
<h4 id="org9eace01">2. El Paso Crítico: La Activación</h4>
<div class="outline-text-4" id="text-org9eace01">
<p>
En Windows, la activación depende de qué terminal estés usando.
</p>
</div>
<ul class="org-ul">
<li><a id="org044e572"></a>Opción A: PowerShell (Recomendado)<br />
<div class="outline-text-5" id="text-org044e572">
<p>
Si es la primera vez que usas scripts en Windows, podrías recibir un error de seguridad. Primero, ejecuta esto como administrador (solo una vez):
</p>
<div class="org-src-container">
<pre class="src src-powershell">Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
</pre>
</div>

<p>
Luego, para activar el entorno:
</p>
<div class="org-src-container">
<pre class="src src-powershell">.\venv\Scripts\Activate.ps1
</pre>
</div>
</div>
</li>
<li><a id="org76a531c"></a>Opción B: Símbolo del Sistema (CMD)<br />
<div class="outline-text-5" id="text-org76a531c">
<div class="org-src-container">
<pre class="src src-cmd">.\venv\Scripts\activate.bat
</pre>
</div>
</div>
</li>
</ul>
</div>
<div id="outline-container-org2fd0ab5" class="outline-4">
<h4 id="org2fd0ab5">3. Confirmación</h4>
<div class="outline-text-4" id="text-org2fd0ab5">
<p>
Sabrás que el entorno está activo porque el nombre <code>(venv)</code> aparecerá a la izquierda de la ruta en tu terminal:
#+example
(venv) C:\Proyectos\MiProyecto&gt;
#+example
</p>
</div>
</div>
</div>
<div id="outline-container-orgafc9128" class="outline-3">
<h3 id="orgafc9128">Gestión de Librerías con PIP</h3>
<div class="outline-text-3" id="text-orgafc9128">
</div>
<div id="outline-container-org03667ad" class="outline-4">
<h4 id="org03667ad">Instalación de paquetes</h4>
<div class="outline-text-4" id="text-org03667ad">
<p>
Una vez activo el entorno, instala lo que necesites:
</p>
<div class="org-src-container">
<pre class="src src-powershell">pip install pandas requests openpyxl
</pre>
</div>
</div>
</div>
<div id="outline-container-org1c0d3dd" class="outline-4">
<h4 id="org1c0d3dd">Congelar dependencias (Compartir proyecto)</h4>
<div class="outline-text-4" id="text-org1c0d3dd">
<p>
Para que otros participantes tengan exactamente lo mismo que tú:
</p>
<div class="org-src-container">
<pre class="src src-powershell">pip freeze &gt; requirements.txt
</pre>
</div>
</div>
</div>
<div id="outline-container-orgcc11e48" class="outline-4">
<h4 id="orgcc11e48">Instalar desde un archivo recibido</h4>
<div class="outline-text-4" id="text-orgcc11e48">
<p>
Si un compañero te pasa su <code>requirements.txt</code>:
</p>
<div class="org-src-container">
<pre class="src src-powershell">pip install -r requirements.txt
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org6cb7c58" class="outline-3">
<h3 id="org6cb7c58">Uso de Entornos en Emacs (Windows)</h3>
<div class="outline-text-3" id="text-org6cb7c58">
<p>
Para que Emacs en Windows gestione bien el entorno, añade esto a tu archivo de configuración (<code>init.el</code> o <code>.emacs</code>):
</p>
</div>
<div id="outline-container-orgded36d4" class="outline-4">
<h4 id="orgded36d4">Instalación del paquete pyvenv</h4>
<div class="outline-text-4" id="text-orgded36d4">
<div class="org-src-container">
<pre class="src src-elisp"><span style="color: #fe8019;">(</span><span style="color: #fb4934;">use-package</span> pyvenv
  <span style="color: #fe8019;">:ensure</span> t
  <span style="color: #fe8019;">:config</span>
  <span style="color: #b16286;">(</span>pyvenv-mode 1<span style="color: #b16286;">)</span><span style="color: #fe8019;">)</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-org3f1679f" class="outline-4">
<h4 id="org3f1679f">Cómo activarlo dentro de Emacs</h4>
<div class="outline-text-4" id="text-org3f1679f">
<ol class="org-ol">
<li>Presiona <code>M-x pyvenv-activate</code>.</li>
<li>Emacs te pedirá la ruta. Navega hasta la carpeta <code>venv</code> de tu proyecto.</li>
<li>Al seleccionarla, Emacs usará ese intérprete de Python para todos los scripts que ejecutes.</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-orgdb9576a" class="outline-3">
<h3 id="orgdb9576a">Solución de Problemas Comunes en Windows</h3>
<div class="outline-text-3" id="text-orgdb9576a">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">Error / Problema</td>
<td class="org-left">Solución</td>
</tr>

<tr>
<td class="org-left">:---</td>
<td class="org-left">:---</td>
</tr>

<tr>
<td class="org-left">"python" no se reconoce</td>
<td class="org-left">Reinstala Python y marca "Add to PATH" o usa el comando `py`.</td>
</tr>

<tr>
<td class="org-left">Error de "Execution Policy"</td>
<td class="org-left">Ejecuta `Set-ExecutionPolicy RemoteSigned -Scope CurrentUser`.</td>
</tr>

<tr>
<td class="org-left">No aparece el (venv)</td>
<td class="org-left">Asegúrate de usar el comando de activación correcto para tu terminal (.ps1 vs .bat).</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-orgc559f4c" class="outline-3">
<h3 id="orgc559f4c">Desactivación y Limpieza</h3>
<div class="outline-text-3" id="text-orgc559f4c">
<p>
Para salir del entorno:
</p>
<div class="org-src-container">
<pre class="src src-powershell">deactivate
</pre>
</div>

<p>
Si quieres borrar el entorno por completo (para empezar de cero):
</p>
<div class="org-src-container">
<pre class="src src-powershell">rmdir /s /q venv
</pre>
</div>

<p>
&#x2014;
</p>
<div class="IMPORTANT" id="orgb5d0910">
<p>
<b>Recordatorio:</b> Nunca incluyas la carpeta <code>venv</code> en tus archivos compartidos o en tu repositorio de Git. Solo comparte el código y el archivo <code>requirements.txt</code>.
</p>

</div>
</div>
</div>
<div id="outline-container-orgb67615c" class="outline-3">
<h3 id="orgb67615c">Agregar a jupyter notebook</h3>
<div class="outline-text-3" id="text-orgb67615c">
<div class="org-src-container">
<pre class="src src-shell">pip install ipykernel
python -m ipykernel install --user --name=redes --display-name=<span style="color: #b8bb26;">"redes"</span>
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-orgdd9d627" class="outline-2">
<h2 id="orgdd9d627">Introducción a la Inteligencia Artificial (IA)</h2>
<div class="outline-text-2" id="text-orgdd9d627">
</div>
<div id="outline-container-org981316b" class="outline-3">
<h3 id="org981316b">Definición formal</h3>
<div class="outline-text-3" id="text-org981316b">
<p>
La <b>Inteligencia Artificial</b> es el área de la computación que estudia
  cómo construir sistemas capaces de realizar tareas que, si fueran
  realizadas por humanos, requerirían inteligencia.
</p>

<p>
Esto incluye capacidades como:
</p>

<ul class="org-ul">
<li>Aprender de la experiencia</li>
<li>Razonar</li>
<li>Reconocer patrones</li>
<li>Tomar decisiones bajo incertidumbre</li>
</ul>

<p>
La IA no implica necesariamente consciencia; se centra en <b>comportamiento inteligente observable</b>.
</p>
</div>
</div>
<div id="outline-container-org1e3f40e" class="outline-3">
<h3 id="org1e3f40e">IA débil vs IA fuerte</h3>
<div class="outline-text-3" id="text-org1e3f40e">
<ul class="org-ul">
<li><b>IA débil</b>: sistemas especializados en una tarea concreta (la IA actual)</li>
<li><b>IA fuerte</b>: inteligencia general comparable a la humana (teórica)</li>
</ul>

<p>
Las redes neuronales pertenecen claramente a la IA débil.
</p>
</div>
</div>
</div>
<div id="outline-container-org85b13d9" class="outline-2">
<h2 id="org85b13d9">Machine Learning (Aprendizaje Automático)</h2>
<div class="outline-text-2" id="text-org85b13d9">
<p>
<b>* Relación entre IA y ML
El *Machine Learning (ML)</b> es una subdisciplina de la IA. Mientras la IA es el objetivo general, el ML es una de las herramientas principales para alcanzarlo.
</p>

<p>
Idea clave:
</p>
<blockquote>
<p>
En lugar de programar reglas, el sistema aprende las reglas a partir de datos.
</p>
</blockquote>
</div>
<div id="outline-container-org3c8e262" class="outline-3">
<h3 id="org3c8e262">Tipos de aprendizaje</h3>
<div class="outline-text-3" id="text-org3c8e262">
<ul class="org-ul">
<li><b>Supervisado</b>: datos con etiqueta (clasificación, regresión)</li>
<li><b>No supervisado</b>: datos sin etiqueta (clustering, reducción de dimensión)</li>
<li><b>Por refuerzo</b>: aprendizaje mediante recompensa y castigo</li>
</ul>

<p>
Las redes neuronales pueden adaptarse a los tres esquemas.
</p>
</div>
</div>
</div>
<div id="outline-container-org17268f7" class="outline-2">
<h2 id="org17268f7">Historia de las Redes Neuronales</h2>
<div class="outline-text-2" id="text-org17268f7">
</div>
<div id="outline-container-org43d8c97" class="outline-3">
<h3 id="org43d8c97">Contexto histórico</h3>
<div class="outline-text-3" id="text-org43d8c97">
<p>
Desde mediados del siglo XX, científicos han intentado entender si la inteligencia podía ser replicada mediante modelos matemáticos.
</p>
</div>
</div>
<div id="outline-container-orge067a16" class="outline-3">
<h3 id="orge067a16">McCulloch y Pitts (1943)</h3>
<div class="outline-text-3" id="text-orge067a16">
<p>
Propusieron la primera neurona artificial basada en lógica
booleana. Demostraron que redes de estas neuronas podían computar
cualquier función lógica.
</p>

<p>
Este trabajo sentó las bases teóricas de las redes neuronales.
</p>
</div>
</div>
<div id="outline-container-org5081243" class="outline-3">
<h3 id="org5081243">Perceptrón y optimismo inicial</h3>
<div class="outline-text-3" id="text-org5081243">
<p>
En los años 50 y 60, el perceptrón generó grandes expectativas al ser uno de los primeros sistemas que <b>aprendía</b> automáticamente.
</p>
</div>
</div>
<div id="outline-container-org307ee4e" class="outline-3">
<h3 id="org307ee4e">Críticas y estancamiento</h3>
<div class="outline-text-3" id="text-org307ee4e">
<p>
El libro <b>Perceptrons</b> (Minsky &amp; Papert, 1969) demostró limitaciones severas del perceptrón, provocando una fuerte caída en el interés por las redes neuronales.
</p>
</div>
</div>
<div id="outline-container-orga3efbe8" class="outline-3">
<h3 id="orga3efbe8">Renacimiento moderno</h3>
<div class="outline-text-3" id="text-orga3efbe8">
<p>
Con mayor poder computacional, grandes bases de datos y mejores algoritmos, las redes neuronales resurgen como <b>Deep Learning</b>.
</p>
</div>
</div>
</div>
<div id="outline-container-org5aaeffc" class="outline-2">
<h2 id="org5aaeffc">La Neurona Artificial</h2>
<div class="outline-text-2" id="text-org5aaeffc">
</div>
<div id="outline-container-orgbc159cf" class="outline-3">
<h3 id="orgbc159cf">Motivación</h3>
<div class="outline-text-3" id="text-orgbc159cf">
<p>
Una red neuronal artificial es un modelo matemático inspirado en la
organización y conectividad de las neuronas biológicas, que abstrae
su funcionamiento esencial para construir sistemas capaces de
aprender representaciones a partir de datos mediante el ajuste de
parámetros.
</p>
</div>
</div>
<div id="outline-container-orgca449e9" class="outline-3">
<h3 id="orgca449e9">Componentes</h3>
<div class="outline-text-3" id="text-orgca449e9">
<ul class="org-ul">
<li>Entradas: variables numéricas</li>
<li>Pesos: importancia relativa de cada entrada</li>
<li>Bias: ajuste del umbral</li>
<li>Activación: decisión final</li>
</ul>
</div>
</div>
<div id="outline-container-org1ebe6e5" class="outline-3">
<h3 id="org1ebe6e5">Modelo matemático</h3>
<div class="outline-text-3" id="text-org1ebe6e5">
<div class="org-src-container">
<pre class="src src-text">z = w&#183;x + b
a = f(z)
</pre>
</div>

<p>
Este modelo es la unidad básica de todas las redes neuronales modernas.
</p>
</div>
</div>
</div>
<div id="outline-container-orge03c85b" class="outline-2">
<h2 id="orge03c85b">El Perceptrón</h2>
<div class="outline-text-2" id="text-orge03c85b">
</div>
<div id="outline-container-org54d397b" class="outline-3">
<h3 id="org54d397b">Definición</h3>
<div class="outline-text-3" id="text-org54d397b">
<p>
El perceptrón es una neurona artificial entrenable utilizada para clasificación binaria.
</p>
</div>
</div>
<div id="outline-container-orgaa799f1" class="outline-3">
<h3 id="orgaa799f1">Interpretación geométrica</h3>
<div class="outline-text-3" id="text-orgaa799f1">
<p>
El perceptrón aprende una <b>frontera de decisión lineal</b> que separa los datos en dos clases.
</p>
</div>
</div>
<div id="outline-container-org9ef3b03" class="outline-3">
<h3 id="org9ef3b03">Aprendizaje</h3>
<div class="outline-text-3" id="text-org9ef3b03">
<p>
Cuando el perceptrón se equivoca, ajusta sus pesos para reducir el error.
</p>
</div>
</div>
<div id="outline-container-org5653bb6" class="outline-3">
<h3 id="org5653bb6">Limitaciones</h3>
<div class="outline-text-3" id="text-org5653bb6">
<p>
No puede aprender relaciones no lineales, como XOR.
</p>
</div>
</div>
</div>
<div id="outline-container-org48a6f77" class="outline-2">
<h2 id="org48a6f77">¿Qué es un problema linealmente separable?</h2>
<div class="outline-text-2" id="text-org48a6f77">
</div>
<div id="outline-container-org61d51b4" class="outline-3">
<h3 id="org61d51b4">Intuición básica</h3>
<div class="outline-text-3" id="text-org61d51b4">
<p>
Un problema es linealmente separable si puedes separar las clases usando una línea recta (en 2D),
un plano (en 3D), o en general un hiperplano.
Si existe una sola frontera recta que divide perfectamente las clases, el problema es linealmente separable.
</p>
</div>
</div>
</div>
<div id="outline-container-org11585c6" class="outline-2">
<h2 id="org11585c6">Estructura de un Perceptrón Simple</h2>
<div class="outline-text-2" id="text-org11585c6">
</div>
<div id="outline-container-org9263ea7" class="outline-3">
<h3 id="org9263ea7">Entradas (Inputs)</h3>
<div class="outline-text-3" id="text-org9263ea7">
<ul class="org-ul">
<li>Representadas como \(x_1, x_2, ..., x_n\).</li>
<li>Son los datos brutos o características que recibe el modelo.</li>
</ul>
</div>
</div>
<div id="outline-container-orgc3795fb" class="outline-3">
<h3 id="orgc3795fb">Pesos Sinápticos (Weights)</h3>
<div class="outline-text-3" id="text-orgc3795fb">
<ul class="org-ul">
<li>Representados como \(w_1, w_2, ..., w_n\).</li>
<li>Determinan la importancia o influencia de cada entrada en el resultado final.</li>
</ul>
</div>
</div>
<div id="outline-container-org7e8a1f0" class="outline-3">
<h3 id="org7e8a1f0">Suma Ponderada (Weighted Sum)</h3>
<div class="outline-text-3" id="text-org7e8a1f0">
<ul class="org-ul">
<li>Es la combinación lineal de las entradas y los pesos.</li>
<li>La fórmula matemática es:
\[z = \sum_{i=1}^{n} w_i x_i + b\]</li>
</ul>
</div>
</div>
<div id="outline-container-org7015b5e" class="outline-3">
<h3 id="org7015b5e">Sesgo (Bias)</h3>
<div class="outline-text-3" id="text-org7015b5e">
<ul class="org-ul">
<li>Representado generalmente como \(b\) (o \(w_0\)).</li>
<li>Permite desplazar la función de activación hacia la izquierda o derecha para ajustar mejor los datos.</li>
</ul>
</div>
</div>
<div id="outline-container-orge2e03e0" class="outline-3">
<h3 id="orge2e03e0">Función de Activación</h3>
<div class="outline-text-3" id="text-orge2e03e0">
<ul class="org-ul">
<li>Decide si la neurona debe "dispararse" (activarse) o no.</li>
<li>En el perceptrón original de Rosenblatt, se utiliza la <b>Función Escalón</b> (Heaviside):
<ul class="org-ul">
<li>\(f(z) = 1\) si \(z > 0\)</li>
<li>\(f(z) = 0\) en caso contrario.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org48b3368" class="outline-3">
<h3 id="org48b3368">Salida (Output)</h3>
<div class="outline-text-3" id="text-org48b3368">
<ul class="org-ul">
<li>Es el resultado final del procesamiento (\(\hat{y}\)).</li>
<li>En un perceptrón simple, suele ser un valor binario (0 o 1).</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgdf4db23" class="outline-2">
<h2 id="orgdf4db23">Ejemplo Tacos</h2>
<div class="outline-text-2" id="text-orgdf4db23">
</div>
<div id="outline-container-orgfa2cff3" class="outline-3">
<h3 id="orgfa2cff3">1. El Escenario: La Decisión de la Cena</h3>
<div class="outline-text-3" id="text-orgfa2cff3">
<p>
Para entender cómo aprende una IA, vamos a usar un ejemplo de la vida real. Queremos que nuestra neurona artificial aprenda cuándo estamos "Satisfechos" (1) o "Inconformes" (0).
</p>
</div>
<div id="outline-container-orgcd71421" class="outline-4">
<h4 id="orgcd71421">Nuestras Entradas (Inputs)</h4>
<div class="outline-text-4" id="text-orgcd71421">
<ul class="org-ul">
<li>\(x_0\): ¿Hay Tacos? (1 = Sí, 0 = No)</li>
<li>\(x_1\): ¿Hay Refresco? (1 = Sí, 0 = No)</li>
</ul>
</div>
</div>
<div id="outline-container-org6518b8d" class="outline-4">
<h4 id="org6518b8d">El Peso (Weight): La Importancia</h4>
<div class="outline-text-4" id="text-org6518b8d">
<p>
No todo nos hace igual de felices. Los <b><b>Pesos</b></b> (\(w_0, w_1\)) representan qué tanto nos importa cada ingrediente. 
</p>
<ul class="org-ul">
<li>Si el peso del taco es alto, el taco es fundamental para nuestra felicidad.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org8e3ff16" class="outline-3">
<h3 id="org8e3ff16">2. ¿Cómo toma la decisión? (La Suma y el Sesgo)</h3>
<div class="outline-text-3" id="text-org8e3ff16">
<p>
La neurona hace un cálculo matemático simple:
\[net = (x_0 \cdot w_0) + (x_1 \cdot w_1) - \text{bias}\]
</p>
</div>
<div id="outline-container-orgbb14525" class="outline-4">
<h4 id="orgbb14525">El Sesgo (Bias)</h4>
<div class="outline-text-4" id="text-orgbb14525">
<p>
El <b><b>Bias</b></b> es nuestro "nivel de exigencia". Si el Bias es muy alto, necesitaremos mucha comida (pesos altos) para poder llegar a estar satisfechos.
</p>
</div>
</div>
</div>
<div id="outline-container-orgbb41e1c" class="outline-3">
<h3 id="orgbb41e1c">3. El Descenso del Gradiente: ¿Cómo "Aprende" la Neurona?</h3>
<div class="outline-text-3" id="text-orgbb41e1c">
<p>
Cuando la neurona se equivoca, necesita ajustar sus pesos. Este proceso se llama <b><b>Descenso del Gradiente</b></b>.
</p>
</div>
<div id="outline-container-org4a7fe1a" class="outline-4">
<h4 id="org4a7fe1a">El Error: La Brújula</h4>
<div class="outline-text-4" id="text-org4a7fe1a">
<p>
Si esperábamos estar felices (<code>target = 1</code>) pero la neurona dice que estamos tristes (<code>net = 0</code>), tenemos un <b><b>Error de 1</b></b>.
\[Error = Target - Net\]
</p>
</div>
</div>
<div id="outline-container-org266a0b8" class="outline-4">
<h4 id="org266a0b8">El Ajuste (La Regla Delta)</h4>
<div class="outline-text-4" id="text-org266a0b8">
<p>
Para corregir el error, cambiamos los pesos usando esta lógica:
</p>
<ol class="org-ol">
<li>Miramos el <b><b>Error</b></b>.</li>
<li>Miramos la <b><b>Entrada</b></b> (¿Quién tuvo la culpa? Si no había tacos, el taco no pudo causar el error).</li>
<li>Aplicamos el <b><b>Learning Rate (K)</b></b>: Que es qué tan rápido queremos aprender.</li>
</ol>

<p>
\[Nuevo\_Peso = Peso\_Actual + (K \cdot Error \cdot Entrada)\]
</p>


<p>
Imagina que \(K = 0.1\) y el Peso inicial del taco es \(0.5\).
</p>
<ol class="org-ol">
<li><b><b>Situación:</b></b> Hay tacos (\(x_0=1\)), pero la red dice "Triste" (\(net=0\)). Nosotros queríamos "Feliz" (\(target=1\)).</li>
<li><b><b>Cálculo del Error:</b></b> \(1 - 0 = 1\).</li>
<li><b><b>Ajuste del Peso:</b></b>
<ul class="org-ul">
<li>\(Delta = 0.1 \cdot 1 \cdot 1 = 0.1\)</li>
<li>\(Nuevo\_Peso\_Taco = 0.5 + 0.1 = 0.6\)</li>
</ul></li>
</ol>

<p>
<b><b>Resultado:</b></b> La próxima vez que haya tacos, la neurona estará un poco más cerca de hacernos felices. ¡Eso es el aprendizaje!
</p>

<p>
Puntos importantes
</p>

<ul class="org-ul">
<li>El <b><b>Gradiente</b></b> nos dice en qué dirección mover los pesos para que el error sea cero.</li>
<li>El <b><b>Learning Rate</b></b> controla qué tan grandes son los pasos que damos hacia esa solución.</li>
<li>Si repetimos este proceso miles de veces (Épocas), la neurona encontrará los pesos perfectos para nuestra felicidad.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org8a15308" class="outline-3">
<h3 id="org8a15308">Perceptrón ejemplo (Python)</h3>
<div class="outline-text-3" id="text-org8a15308">
<ol class="org-ol">
<li>Recibe entradas (\(x_1, x_2, \dots\)).</li>
<li>Las multiplica por pesos (\(w_1, w_2, \dots\)).</li>
<li>Suma un sesgo (\(b\)).</li>
<li>Aplica una función de activación (ej. Escalón).</li>
</ol>
</div>
<div id="outline-container-org8408b10" class="outline-4">
<h4 id="org8408b10">Perceptrón para una compuerta "AND"</h4>
<div class="outline-text-4" id="text-org8408b10">
<p>
Este código simula una neurona que solo se activa si ambas entradas son 1.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #fb4934;">def</span> <span style="color: #b8bb26;">perceptron_and</span><span style="color: #fe8019;">(</span>x1, x2<span style="color: #fe8019;">)</span>:
<span style="background-color: #3c3836;"> </span>   <span style="color: #928374;"># </span><span style="color: #928374;">1. Definimos los Pesos y el Sesgo (Bias)
</span><span style="background-color: #3c3836;"> </span>   <span style="color: #928374;"># </span><span style="color: #928374;">Estos valores normalmente se aprenden, aqu&#237; los asignamos manualmente
</span><span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">w1</span>, <span style="color: #83a598;">w2</span> = 0.5, 0.5
<span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">bias</span> = -0.7

<span style="background-color: #3c3836;"> </span>   <span style="color: #928374;"># </span><span style="color: #928374;">2. Suma ponderada: (x1 * w1) + (x2 * w2) + bias
</span><span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">suma</span> = <span style="color: #fe8019;">(</span>x1 * w1<span style="color: #fe8019;">)</span> + <span style="color: #fe8019;">(</span>x2 * w2<span style="color: #fe8019;">)</span> + bias

<span style="background-color: #3c3836;"> </span>   <span style="color: #928374;"># </span><span style="color: #928374;">3. Funci&#243;n de activaci&#243;n (Escal&#243;n de Heaviside)
</span><span style="background-color: #3c3836;"> </span>   <span style="color: #928374;"># </span><span style="color: #928374;">Si la suma es mayor a 0, la neurona se dispara (1)
</span><span style="background-color: #3c3836;"> </span>   <span style="color: #fb4934;">if</span> suma &gt; 0:
<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="color: #fb4934;">return</span> 1
<span style="background-color: #3c3836;"> </span>   <span style="color: #fb4934;">else</span>:
<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="color: #fb4934;">return</span> 0

<span style="color: #928374;"># </span><span style="color: #928374;">Prueba de la tabla de verdad AND
</span><span style="color: #83a598;">entradas</span> = <span style="color: #fe8019;">[</span><span style="color: #b16286;">(</span>0, 0<span style="color: #b16286;">)</span>, <span style="color: #b16286;">(</span>0, 1<span style="color: #b16286;">)</span>, <span style="color: #b16286;">(</span>1, 0<span style="color: #b16286;">)</span>, <span style="color: #b16286;">(</span>1, 1<span style="color: #b16286;">)</span><span style="color: #fe8019;">]</span>

<span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span><span style="color: #b8bb26;">"Entrada | Salida"</span><span style="color: #fe8019;">)</span>
<span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span><span style="color: #b8bb26;">"--------|-------"</span><span style="color: #fe8019;">)</span>
<span style="color: #fb4934;">for</span> e <span style="color: #fb4934;">in</span> entradas:
<span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">resultado</span> = perceptron_and<span style="color: #fe8019;">(</span>e<span style="color: #b16286;">[</span>0<span style="color: #b16286;">]</span>, e<span style="color: #b16286;">[</span>1<span style="color: #b16286;">]</span><span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   <span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>f<span style="color: #b8bb26;">" </span>{e}<span style="color: #b8bb26;">  |   </span>{resultado}<span style="color: #b8bb26;">"</span><span style="color: #fe8019;">)</span>
</pre>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-org87059ae" class="outline-2">
<h2 id="org87059ae">Codigo C python</h2>
<div class="outline-text-2" id="text-org87059ae">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #fb4934;">import</span> math
<span style="color: #fb4934;">import</span> random

<span style="color: #83a598;">EPOCAS</span> = 300000
<span style="color: #83a598;">K</span> = 0.03  <span style="color: #928374;"># </span><span style="color: #928374;">tasa de aprendizaje
</span>
<span style="color: #928374;"># </span><span style="color: #928374;">Pesos y bias globales (como en el c&#243;digo C)
</span><span style="color: #83a598;">Pesos</span> = <span style="color: #fe8019;">[</span>0.0, 0.0<span style="color: #fe8019;">]</span>
<span style="color: #83a598;">bias</span> = 0.5
<span style="color: #83a598;">Error</span> = 0.0


<span style="color: #fb4934;">def</span> <span style="color: #b8bb26;">sigmoide</span><span style="color: #fe8019;">(</span>s: <span style="color: #fe8019;">float</span><span style="color: #fe8019;">)</span> -&gt; <span style="color: #fe8019;">float</span>:
<span style="background-color: #3c3836;"> </span>   <span style="color: #928374;"># </span><span style="color: #928374;">Versi&#243;n correcta de la sigmoide log&#237;stica:
</span><span style="background-color: #3c3836;"> </span>   <span style="color: #928374;"># </span><span style="color: #928374;">1 / (1 + e^{-s})
</span><span style="background-color: #3c3836;"> </span>   <span style="color: #fb4934;">return</span> 1.0 / <span style="color: #fe8019;">(</span>1.0 + math.exp<span style="color: #b16286;">(</span>-s<span style="color: #b16286;">)</span><span style="color: #fe8019;">)</span>


<span style="color: #fb4934;">def</span> <span style="color: #b8bb26;">pesos_initNt</span><span style="color: #fe8019;">()</span>:
<span style="background-color: #3c3836;"> </span>   <span style="color: #dfd2b8;">"""Inicializa los pesos de forma aleatoria en [0,1)."""</span>
<span style="background-color: #3c3836;"> </span>   <span style="color: #fb4934;">global</span> Pesos
<span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">Pesos</span> = <span style="color: #fe8019;">[</span>random.random<span style="color: #b16286;">()</span> <span style="color: #fb4934;">for</span> _ <span style="color: #fb4934;">in</span> <span style="color: #fe8019;">range</span><span style="color: #b16286;">(</span>2<span style="color: #b16286;">)</span><span style="color: #fe8019;">]</span>


<span style="color: #fb4934;">def</span> <span style="color: #b8bb26;">EntNt</span><span style="color: #fe8019;">(</span>x0: <span style="color: #fe8019;">float</span>, x1: <span style="color: #fe8019;">float</span>, target: <span style="color: #fe8019;">float</span><span style="color: #fe8019;">)</span> -&gt; <span style="color: #fe8019;">float</span>:
<span style="background-color: #3c3836;"> </span>   <span style="color: #dfd2b8;">"""
</span><span style="color: #dfd2b8; background-color: #3c3836;"> </span><span style="color: #dfd2b8;">   Funci&#243;n de entrenamiento del perceptr&#243;n (una iteraci&#243;n para un patr&#243;n).
</span><span style="color: #dfd2b8; background-color: #3c3836;"> </span><span style="color: #dfd2b8;">   Modifica Pesos y bias de forma global.
</span><span style="color: #dfd2b8; background-color: #3c3836;"> </span><span style="color: #dfd2b8;">   """</span>
<span style="background-color: #3c3836;"> </span>   <span style="color: #fb4934;">global</span> Pesos, bias, Error

<span style="background-color: #3c3836;"> </span>   <span style="color: #928374;"># </span><span style="color: #928374;">net = w0*x0 + w1*x1 - bias
</span><span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">net</span> = Pesos<span style="color: #fe8019;">[</span>0<span style="color: #fe8019;">]</span> * x0 + Pesos<span style="color: #fe8019;">[</span>1<span style="color: #fe8019;">]</span> * x1 - bias
<span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">net</span> = sigmoide<span style="color: #fe8019;">(</span>net<span style="color: #fe8019;">)</span>

<span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">Error</span> = target - net

<span style="background-color: #3c3836;"> </span>   <span style="color: #928374;"># </span><span style="color: #928374;">Actualizaci&#243;n de bias (se asume entrada de bias = 1)
</span><span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">bias</span> -= K * Error

<span style="background-color: #3c3836;"> </span>   <span style="color: #928374;"># </span><span style="color: #928374;">Variaci&#243;n de los pesos sin&#225;pticos
</span><span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">delta0</span> = K * Error * x0
<span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">delta1</span> = K * Error * x1

<span style="background-color: #3c3836;"> </span>   <span style="color: #928374;"># </span><span style="color: #928374;">Ajuste de pesos
</span><span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">Pesos</span><span style="color: #fe8019;">[</span>0<span style="color: #fe8019;">]</span> += delta0
<span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">Pesos</span><span style="color: #fe8019;">[</span>1<span style="color: #fe8019;">]</span> += delta1

<span style="background-color: #3c3836;"> </span>   <span style="color: #fb4934;">return</span> net


<span style="color: #fb4934;">def</span> <span style="color: #b8bb26;">InitNt</span><span style="color: #fe8019;">(</span>x0: <span style="color: #fe8019;">float</span>, x1: <span style="color: #fe8019;">float</span><span style="color: #fe8019;">)</span> -&gt; <span style="color: #fe8019;">float</span>:
<span style="background-color: #3c3836;"> </span>   <span style="color: #dfd2b8;">"""
</span><span style="color: #dfd2b8; background-color: #3c3836;"> </span><span style="color: #dfd2b8;">   Funci&#243;n que usa pesos fijos (de ejemplo) para calcular la salida.
</span><span style="color: #dfd2b8; background-color: #3c3836;"> </span><span style="color: #dfd2b8;">   Equivalente a la parte comentada en C.
</span><span style="color: #dfd2b8; background-color: #3c3836;"> </span><span style="color: #dfd2b8;">   """</span>
<span style="background-color: #3c3836;"> </span>   <span style="color: #928374;"># </span><span style="color: #928374;">net = 70.934807*x0 + 93.935219*x1 - 187.886169;
</span><span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">net</span> = 70.934807 * x0 + 93.935219 * x1 - 187.886169
<span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">net</span> = sigmoide<span style="color: #fe8019;">(</span>net<span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   <span style="color: #fb4934;">return</span> net


<span style="color: #fb4934;">def</span> <span style="color: #b8bb26;">main</span><span style="color: #fe8019;">()</span>:
<span style="background-color: #3c3836;"> </span>   <span style="color: #fb4934;">global</span> Pesos, bias, Error

<span style="background-color: #3c3836;"> </span>   pesos_initNt<span style="color: #fe8019;">()</span>

<span style="background-color: #3c3836;"> </span>   <span style="color: #fb4934;">for</span> i <span style="color: #fb4934;">in</span> <span style="color: #fe8019;">range</span><span style="color: #fe8019;">(</span>EPOCAS<span style="color: #fe8019;">)</span>:
<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span><span style="color: #b8bb26;">"------------------------"</span><span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>f<span style="color: #b8bb26;">"Salida Entrenamiento Epoca </span>{i}<span style="color: #b8bb26;">"</span><span style="color: #fe8019;">)</span>

<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">apr</span> = EntNt<span style="color: #fe8019;">(</span>1, 1, 0<span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>f<span style="color: #b8bb26;">"1,1 = </span>{apr:.6f}<span style="color: #b8bb26;">"</span><span style="color: #fe8019;">)</span>

<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">apr</span> = EntNt<span style="color: #fe8019;">(</span>1, 0, 1<span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>f<span style="color: #b8bb26;">"1,0 = </span>{apr:.6f}<span style="color: #b8bb26;">"</span><span style="color: #fe8019;">)</span>

<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">apr</span> = EntNt<span style="color: #fe8019;">(</span>0, 1, 1<span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>f<span style="color: #b8bb26;">"0,1 = </span>{apr:.6f}<span style="color: #b8bb26;">"</span><span style="color: #fe8019;">)</span>

<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">apr</span> = EntNt<span style="color: #fe8019;">(</span>0, 0, 0<span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>f<span style="color: #b8bb26;">"0,0 = </span>{apr:.6f}<span style="color: #b8bb26;">"</span><span style="color: #fe8019;">)</span>

<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span><span style="color: #b8bb26;">"</span><span style="color: #d3869b;">\n</span><span style="color: #b8bb26;">Pesos de cada epoca"</span><span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>f<span style="color: #b8bb26;">"Peso 0 = </span>{Pesos[0]:.6f}<span style="color: #b8bb26;">"</span><span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>f<span style="color: #b8bb26;">"Peso 1 = </span>{Pesos[1]:.6f}<span style="color: #b8bb26;">"</span><span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>f<span style="color: #b8bb26;">"Bias   = </span>{bias:.6f}<span style="color: #b8bb26;">"</span><span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>f<span style="color: #b8bb26;">"Error  = </span>{Error:.6f}<span style="color: #b8bb26;">"</span><span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span><span style="color: #b8bb26;">"------------------------"</span><span style="color: #fe8019;">)</span>

<span style="background-color: #3c3836;"> </span>   <span style="color: #928374;"># </span><span style="color: #928374;">Si quieres probar con los pesos fijos del comentario de C:
</span><span style="background-color: #3c3836;"> </span>   <span style="color: #928374;"># </span><span style="color: #928374;">print("Resultados con InitNt:")
</span><span style="background-color: #3c3836;"> </span>   <span style="color: #928374;"># </span><span style="color: #928374;">print("1,1 =", InitNt(1, 1))
</span><span style="background-color: #3c3836;"> </span>   <span style="color: #928374;"># </span><span style="color: #928374;">print("1,0 =", InitNt(1, 0))
</span><span style="background-color: #3c3836;"> </span>   <span style="color: #928374;"># </span><span style="color: #928374;">print("0,1 =", InitNt(0, 1))
</span><span style="background-color: #3c3836;"> </span>   <span style="color: #928374;"># </span><span style="color: #928374;">print("0,0 =", InitNt(0, 0))
</span>

<span style="color: #fb4934;">if</span> <span style="color: #fe8019;">__name__</span> == <span style="color: #b8bb26;">"__main__"</span>:
<span style="background-color: #3c3836;"> </span>   main<span style="color: #fe8019;">()</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-org967687f" class="outline-2">
<h2 id="org967687f">Redes Neuronales Multicapa (Multilayer Perceptron)</h2>
<div class="outline-text-2" id="text-org967687f">
</div>
<div id="outline-container-orgb7c23d3" class="outline-3">
<h3 id="orgb7c23d3">Introducción</h3>
<div class="outline-text-3" id="text-orgb7c23d3">
<p>
Una <b>red neuronal multicapa</b> o <b>Perceptrón Multicapa (MLP)</b> es un modelo
de aprendizaje supervisado que extiende al perceptrón simple mediante
la inclusión de <b>una o más capas ocultas</b>. Estas capas permiten modelar
<b>relaciones no lineales complejas</b> entre las variables de entrada y la salida.
</p>

<p>
Los MLP constituyen la base conceptual del <b>Deep Learning</b> moderno:
</p>

<ul class="org-ul">
<li>El perceptrón simple solo puede resolver problemas <b>linealmente separables</b>.</li>
<li>Muchos problemas reales (visión, texto, señales) son <b>no lineales</b>.</li>
<li>Agregar capas ocultas + funciones de activación no lineales permite
aproximar funciones mucho más complejas (teorema del aproximador universal).</li>
</ul>

<p>
Ejemplos clásicos (con 2 entradas):
</p>

<ul class="org-ul">
<li>AND  → ✓ separable linealmente</li>
<li>OR   → ✓ separable linealmente</li>
<li>XOR  → ✗ <b>no</b> es separable linealmente</li>
</ul>

<p>
El problema XOR demuestra la necesidad de introducir:
</p>

<ul class="org-ul">
<li><b>Capas ocultas</b> (capacidad de representar interacciones no triviales),</li>
<li><b>Funciones de activación no lineales</b>.</li>
</ul>

<p>
Un MLP típico está compuesto por:
</p>

<ol class="org-ol">
<li><b>Capa de entrada</b>  
Recibe el vector de características \(\mathbf{x}\).</li>

<li><b>Capas ocultas</b>  
Realizan transformaciones lineales + no lineales sobre las
representaciones intermedias.</li>

<li><b>Capa de salida</b>  
Produce la predicción final (clase, probabilidad, valor continuo, etc.).</li>
</ol>

<p>
Estructura conceptual:
</p>

<pre class="example" id="orgf718995">
x → Capa Oculta 1 → Capa Oculta 2 → … → Capa de salida → ŷ
</pre>

<p>
Cada “flecha” representa una transformación lineal (multiplicación por
matriz de pesos + suma de bias) seguida de una función de activación.
</p>
</div>
</div>
<div id="outline-container-orga714d92" class="outline-3">
<h3 id="orga714d92">Modelo matemático</h3>
<div class="outline-text-3" id="text-orga714d92">
</div>
<div id="outline-container-org737d32d" class="outline-4">
<h4 id="org737d32d">Neurona individual</h4>
<div class="outline-text-4" id="text-org737d32d">
<p>
Cada neurona recibe un vector de entrada \(\mathbf{x} \in \mathbb{R}^n\) y calcula:
</p>

<p>
\[
z = \sum_{i=1}^{n} w_i x_i + b
\]
</p>

<p>
\[
a = f(z)
\]
</p>

<p>
Donde:
</p>

<ul class="org-ul">
<li>\(w_i\): pesos sinápticos de la neurona,</li>
<li>\(b\): término de sesgo (bias),</li>
<li>\(f(\cdot)\): función de activación,</li>
<li>\(z\): combinación lineal de las entradas,</li>
<li>\(a\): salida (activación) de la neurona.</li>
</ul>

<p>
En forma vectorial para una neurona:
</p>

<p>
\[
z = \mathbf{w}^\top \mathbf{x} + b
\]
</p>

<p>
\[
a = f(z)
\]
</p>
</div>
</div>
<div id="outline-container-org67ccbfd" class="outline-4">
<h4 id="org67ccbfd">Forma matricial de una capa</h4>
<div class="outline-text-4" id="text-org67ccbfd">
<p>
En una capa con múltiples neuronas, se usa notación matricial:
</p>

<ul class="org-ul">
<li>\(\mathbf{a}^{(l-1)}\): vector de activaciones de la capa anterior,</li>
<li>\(\mathbf{W}^{(l)}\): matriz de pesos de la capa \(l\),</li>
<li>\(\mathbf{b}^{(l)}\): vector de bias de la capa \(l\),</li>
<li>\(\mathbf{z}^{(l)}\): preactivaciones de la capa \(l\),</li>
<li>\(\mathbf{a}^{(l)}\): activaciones de la capa \(l\).</li>
</ul>

<p>
Cálculo:
</p>

<p>
\[
\mathbf{z}^{(l)} = \mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}
\]
</p>

<p>
\[
\mathbf{a}^{(l)} = f(\mathbf{z}^{(l)})
\]
</p>

<p>
donde \(f\) se aplica componente a componente.
</p>

<p>
Repitiendo este proceso desde la capa de entrada hasta la salida,
obtenemos la predicción del MLP.
</p>
</div>
</div>
</div>
<div id="outline-container-org9c86d9d" class="outline-3">
<h3 id="org9c86d9d">Funciones de activación</h3>
<div class="outline-text-3" id="text-org9c86d9d">
<p>
Las funciones de activación son componentes fundamentales en las redes
neuronales multicapa. Su función principal es introducir <b>no linealidad</b>
en el modelo.
</p>

<p>
Sin funciones de activación no lineales:
</p>

<ul class="org-ul">
<li>cada capa sería solo una transformación lineal,</li>
<li>y la composición de múltiples transformaciones lineales es <b>otra</b>
transformación lineal,</li>
<li>por lo tanto, una “red profunda” sin no linealidad se comporta como
un <b>modelo lineal simple</b>, independientemente del número de capas.</li>
</ul>
</div>
<div id="outline-container-orgad7e119" class="outline-4">
<h4 id="orgad7e119">¿Para qué sirven las funciones de activación?</h4>
<div class="outline-text-4" id="text-orgad7e119">
<p>
Las funciones de activación permiten:
</p>

<ul class="org-ul">
<li>Introducir <b>no linealidad</b> en la red.</li>
<li>Modelar relaciones complejas entre variables de entrada y salida.</li>
<li>Aprender fronteras de decisión no lineales en problemas de clasificación.</li>
<li>Aproximar cualquier función continua bajo ciertas condiciones
(teorema del aproximador universal).</li>
</ul>

<p>
Recordemos que una neurona realiza:
</p>

<p>
\[
z = \sum_{i=1}^{n} w_i x_i + b
\]
</p>

<p>
\[
a = f(z)
\]
</p>

<p>
Donde \(f\) es la función de activación.
</p>
</div>
</div>
</div>
<div id="outline-container-orgde5ceef" class="outline-3">
<h3 id="orgde5ceef">Clasificación de las funciones de activación</h3>
<div class="outline-text-3" id="text-orgde5ceef">
<p>
Las funciones de activación pueden clasificarse según:
</p>

<ul class="org-ul">
<li>su forma (escalón, sigmoide, lineal por partes, etc.),</li>
<li>su rango de salida,</li>
<li>su uso (capas ocultas vs capa de salida),</li>
<li>su comportamiento para valores grandes de \(|z|\).</li>
</ul>
</div>
<div id="outline-container-org23c6965" class="outline-4">
<h4 id="org23c6965">Funciones de activación clásicas</h4>
<div class="outline-text-4" id="text-org23c6965">
</div>
<ul class="org-ul">
<li><a id="orgd19e6b6"></a>Función escalón (Step Function)<br />
<div class="outline-text-5" id="text-orgd19e6b6">
<p>
\[
f(z) =
</p>
\begin{cases}
1 & \text{si } z \ge 0 \\
0 & \text{si } z < 0
\end{cases}
<p>
\]
</p>

<p>
Características:
</p>

<ul class="org-ul">
<li>Usada en el perceptrón simple original.</li>
<li>No es continua ni derivable (no adecuada para descenso de gradiente).</li>
<li>Genera salidas binarias (0/1).</li>
</ul>

<p>
Uso:
</p>

<ul class="org-ul">
<li>Modelos teóricos e históricos.</li>
<li>Introducción al concepto de neurona como “disparar / no disparar”.</li>
<li>No se utiliza en redes multicapa modernas para entrenamiento con
backpropagation.</li>
</ul>
</div>
</li>
<li><a id="orgebbabdd"></a>Sigmoide logística<br />
<div class="outline-text-5" id="text-orgebbabdd">
<p>
\[
f(z) = \frac{1}{1 + e^{-z}}
\]
</p>

<p>
Características:
</p>

<ul class="org-ul">
<li>Salida en el intervalo \((0, 1)\).</li>
<li>Puede interpretarse como una probabilidad.</li>
<li>Es suave y derivable en todo \(\mathbb{R}\).</li>
</ul>

<p>
Ventajas:
</p>

<ul class="org-ul">
<li>Interpretación probabilística directa.</li>
<li>Históricamente muy usada en redes neuronales tempranas.</li>
</ul>

<p>
Desventajas:
</p>

<ul class="org-ul">
<li>Para \(|z|\) grandes, la función se <b>satura</b> cerca de 0 o 1:
<ul class="org-ul">
<li>la derivada es muy pequeña,</li>
<li>aparece el problema de <b>vanishing gradient</b>,</li>
<li>el entrenamiento de redes profundas se vuelve lento o ineficaz.</li>
</ul></li>
</ul>

<p>
Uso típico actual:
</p>

<ul class="org-ul">
<li>Capa de salida en algunos modelos de clasificación binaria (aunque
en deep learning moderno también se usan otras combinaciones).</li>
</ul>
</div>
</li>
<li><a id="orgddceefd"></a>Tangente hiperbólica (tanh)<br />
<div class="outline-text-5" id="text-orgddceefd">
<p>
\[
f(z) = \tanh(z)
\]
</p>

<p>
Características:
</p>

<ul class="org-ul">
<li>Salida en el intervalo \((-1, 1)\).</li>
<li>Es simétrica alrededor de 0 (centrada en cero).</li>
</ul>

<p>
Ventajas:
</p>

<ul class="org-ul">
<li>En comparación con la sigmoide logística, su salida centrada en cero:
<ul class="org-ul">
<li>puede favorecer una convergencia algo más rápida,</li>
<li>reduce ciertos sesgos en las activaciones.</li>
</ul></li>
</ul>

<p>
Desventajas:
</p>

<ul class="org-ul">
<li>También se satura para \(|z|\) grandes.</li>
<li>Sigue presentando <b>vanishing gradient</b> en redes muy profundas.</li>
</ul>

<p>
Uso clásico:
</p>

<ul class="org-ul">
<li>Capas ocultas en redes no demasiado profundas, antes de la adopción
masiva de ReLU.</li>
</ul>
</div>
</li>
</ul>
</div>
<div id="outline-container-org82e1890" class="outline-4">
<h4 id="org82e1890">Funciones de activación modernas</h4>
<div class="outline-text-4" id="text-org82e1890">
</div>
<ul class="org-ul">
<li><a id="org440e9cd"></a>ReLU (Rectified Linear Unit)<br />
<div class="outline-text-5" id="text-org440e9cd">
<p>
\[
f(z) = \max(0, z)
\]
</p>

<p>
Características:
</p>

<ul class="org-ul">
<li>Función lineal por partes:
<ul class="org-ul">
<li>para \(z < 0\): salida 0,</li>
<li>para \(z \ge 0\): salida \(z\).</li>
</ul></li>
<li>Muy sencilla y eficiente de calcular.</li>
</ul>

<p>
Ventajas:
</p>

<ul class="org-ul">
<li>No se satura para valores positivos grandes (derivada constante 1).</li>
<li>Reduce considerablemente el problema del <b>vanishing gradient</b>.</li>
<li>Ha permitido entrenar redes profundas con muchas capas.</li>
</ul>

<p>
Desventajas:
</p>

<ul class="org-ul">
<li><b>Dead neurons</b>: si una neurona recibe valores de \(z\) siempre
negativos, su salida es siempre 0 y el gradiente puede quedar en 0
→ deja de aprender.</li>
</ul>

<p>
Uso típico:
</p>

<ul class="org-ul">
<li>Función estándar en <b>capas ocultas</b> de MLP y CNN modernos.</li>
</ul>
</div>
</li>
<li><a id="org7078816"></a>Leaky ReLU<br />
<div class="outline-text-5" id="text-org7078816">
<p>
\[
f(z) =
</p>
\begin{cases}
z & \text{si } z \ge 0 \\
\alpha z & \text{si } z < 0
\end{cases}
<p>
\]
</p>

<p>
donde \(\alpha\) es un pequeño número positivo (por ejemplo, 0.01).
</p>

<p>
Características:
</p>

<ul class="org-ul">
<li>Variante de ReLU que, en lugar de “apagar” totalmente la neurona para
\(z < 0\), permite un pequeño gradiente negativo.</li>
</ul>

<p>
Ventajas:
</p>

<ul class="org-ul">
<li>Reduce el problema de neuronas muertas (<b>dead ReLU</b>).</li>
<li>Mantiene muchas ventajas de ReLU.</li>
</ul>

<p>
Uso típico:
</p>

<ul class="org-ul">
<li>Alternativa a ReLU cuando se observa que muchas neuronas quedan
inactivas de manera permanente.</li>
</ul>
</div>
</li>
<li><a id="org5a3150e"></a>ELU (Exponential Linear Unit)<br />
<div class="outline-text-5" id="text-org5a3150e">
<p>
\[
f(z) =
</p>
\begin{cases}
z & \text{si } z \ge 0 \\
\alpha (e^{z} - 1) & \text{si } z < 0
\end{cases}
<p>
\]
</p>

<p>
Características:
</p>

<ul class="org-ul">
<li>Para \(z \ge 0\): igual que ReLU.</li>
<li>Para \(z < 0\): decae de forma exponencial, con salida negativa acotada.</li>
</ul>

<p>
Ventajas:
</p>

<ul class="org-ul">
<li>Puede acelerar la convergencia en ciertos casos.</li>
<li>Las salidas negativas ayudan a centrar la activación alrededor de 0,
lo que puede mejorar la propagación del gradiente.</li>
</ul>

<p>
Uso típico:
</p>

<ul class="org-ul">
<li>Capas ocultas, en algunos modelos donde se ha observado mejor
rendimiento que con ReLU estándar.</li>
</ul>
</div>
</li>
</ul>
</div>
<div id="outline-container-org89fa44b" class="outline-4">
<h4 id="org89fa44b">Funciones de activación en la capa de salida</h4>
<div class="outline-text-4" id="text-org89fa44b">
<p>
La función de activación de la <b>capa de salida</b> depende del tipo de
problema:
</p>
</div>
<ul class="org-ul">
<li><a id="org35d43f2"></a>Clasificación binaria<br />
<div class="outline-text-5" id="text-org35d43f2">
<p>
Se usa típicamente una función <b>sigmoide</b> para obtener una probabilidad
en \((0,1)\):
</p>

<p>
\[
\hat{y} = \sigma(z) \in (0,1)
\]
</p>

<p>
donde \(\hat{y}\) se interpreta como la probabilidad de clase “1”.
</p>
</div>
</li>
<li><a id="org0c24c39"></a>Clasificación multiclase (mutuamente excluyentes)<br />
<div class="outline-text-5" id="text-org0c24c39">
<p>
Se usa la función <b>softmax</b>:
</p>

<p>
\[
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}
\]
</p>

<ul class="org-ul">
<li>Convierte un vector de puntajes \(\mathbf{z}\) en un vector de
probabilidades que:
<ul class="org-ul">
<li>son positivas,</li>
<li>y suman 1.</li>
</ul></li>
</ul>
</div>
</li>
<li><a id="orgf38a0b6"></a>Regresión<br />
<div class="outline-text-5" id="text-orgf38a0b6">
<ul class="org-ul">
<li>Se usa generalmente una función de activación <b>lineal</b> en la salida:
\[
    f(z) = z
  \]</li>
<li>La red puede entonces producir valores reales sin restricción de
rango (o con restricciones adicionales impuestas por el preprocesado).</li>
</ul>
</div>
</li>
</ul>
</div>
<div id="outline-container-org8fadb20" class="outline-4">
<h4 id="org8fadb20">Impacto en el entrenamiento</h4>
<div class="outline-text-4" id="text-org8fadb20">
<p>
La elección de la función de activación afecta:
</p>

<ul class="org-ul">
<li>La <b>velocidad de convergencia</b>.</li>
<li>La <b>estabilidad</b> del entrenamiento.</li>
<li>El <b>flujo del gradiente</b> a través de las capas.</li>
<li>La capacidad de representar ciertas distribuciones de salida.</li>
</ul>

<p>
Una elección inadecuada:
</p>

<ul class="org-ul">
<li>puede causar vanishing/exploding gradient,</li>
<li>puede impedir que la red aprenda adecuadamente,</li>
<li>o requerir tiempos de entrenamiento excesivos.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org4033825" class="outline-3">
<h3 id="org4033825">Propagación hacia adelante (Forward Propagation)</h3>
<div class="outline-text-3" id="text-org4033825">
<p>
La <b>propagación hacia adelante</b> es el proceso mediante el cual la red
calcula su salida a partir de una entrada dada.
</p>

<p>
Pasos generales:
</p>

<ol class="org-ol">
<li>Se recibe el vector de entrada \(\mathbf{x}\).</li>
<li>Se calcula sucesivamente:
<ul class="org-ul">
<li>\(\mathbf{z}^{(1)}\), \(\mathbf{a}^{(1)}\) (primera capa oculta),</li>
<li>\(\mathbf{z}^{(2)}\), \(\mathbf{a}^{(2)}\), etc.</li>
</ul></li>
<li>Se obtiene la activación de la capa de salida \(\mathbf{a}^{(L)}\),
que corresponde a la predicción \(\hat{\mathbf{y}}\).</li>
</ol>

<p>
Podemos verlo como una composición de funciones:
</p>

<p>
\[
\mathbf{a}^{(L)} = f^{(L)}\left( \mathbf{W}^{(L)} \, f^{(L-1)}\left( \dots f^{(1)}\left(\mathbf{W}^{(1)} \mathbf{x} + \mathbf{b}^{(1)}\right) \dots \right) + \mathbf{b}^{(L)} \right)
\]
</p>

<p>
o, de forma más compacta:
</p>

<p>
\[
\hat{\mathbf{y}} = F(\mathbf{x}; \theta)
\]
</p>

<p>
donde \(\theta\) denota el conjunto de todos los pesos y bias de la red.
</p>
</div>
</div>
<div id="outline-container-org7479849" class="outline-3">
<h3 id="org7479849">Funciones de pérdida</h3>
<div class="outline-text-3" id="text-org7479849">
<p>
Las funciones de pérdida (o <b>funciones de coste</b>) miden el error entre:
</p>

<ul class="org-ul">
<li>la predicción del modelo \(\hat{y}\) (o \(\hat{\mathbf{y}}\)),</li>
<li>y el valor real \(y\) (o \(\mathbf{y}\)).</li>
</ul>

<p>
El objetivo del entrenamiento es encontrar parámetros \(\theta\) que
<b>minimicen</b> la pérdida promedio sobre el conjunto de entrenamiento.
</p>
</div>
<div id="outline-container-org8d9668e" class="outline-4">
<h4 id="org8d9668e">Error cuadrático medio (MSE)</h4>
<div class="outline-text-4" id="text-org8d9668e">
<p>
Común en problemas de regresión:
</p>

<p>
\[
\mathcal{L}_{\text{MSE}} = \frac{1}{n} \sum_{k=1}^{n} (y_k - \hat{y}_k)^2
\]
</p>

<p>
donde:
</p>

<ul class="org-ul">
<li>\(n\) es el número de muestras,</li>
<li>\(y_k\) es el valor real de la muestra \(k\),</li>
<li>\(\hat{y}_k\) es la predicción del modelo.</li>
</ul>
</div>
</div>
<div id="outline-container-orgff0870b" class="outline-4">
<h4 id="orgff0870b">Entropía cruzada (Cross-Entropy)</h4>
<div class="outline-text-4" id="text-orgff0870b">
<p>
Muy utilizada en clasificación.
</p>

<ul class="org-ul">
<li>Para clasificación binaria:</li>
</ul>

<p>
\[
\mathcal{L}_{\text{binaria}} = - \frac{1}{n} \sum_{k=1}^{n} \left[ y_k \log(\hat{y}_k) + (1 - y_k)\log(1 - \hat{y}_k) \right]
\]
</p>

<ul class="org-ul">
<li>Para clasificación multiclase (one-hot \(\mathbf{y}\) y softmax \(\hat{\mathbf{y}}\)):</li>
</ul>

<p>
\[
\mathcal{L}_{\text{multiclase}} = - \frac{1}{n} \sum_{k=1}^{n} \sum_{c=1}^{C} y_{k,c} \log(\hat{y}_{k,c})
\]
</p>

<p>
donde:
</p>

<ul class="org-ul">
<li>\(C\) es el número de clases,</li>
<li>\(y_{k,c}\) es 1 si la muestra \(k\) pertenece a la clase \(c\), y 0 en caso contrario,</li>
<li>\(\hat{y}_{k,c}\) es la probabilidad predicha para la clase \(c\).</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org50eed8d" class="outline-3">
<h3 id="org50eed8d">Backpropagation</h3>
<div class="outline-text-3" id="text-org50eed8d">
<p>
<b>Backpropagation</b> es el algoritmo fundamental para entrenar MLP. Se
basa en la aplicación sistemática de la <b>regla de la cadena</b> del cálculo
diferencial para propagar el error desde la capa de salida hacia las
capas anteriores.
</p>

<p>
Para un peso genérico \(w\), la derivada de la pérdida se expresa como:
</p>

<p>
\[
\frac{\partial \mathcal{L}}{\partial w} =
\frac{\partial \mathcal{L}}{\partial a}
\cdot \frac{\partial a}{\partial z}
\cdot \frac{\partial z}{\partial w}
\]
</p>

<p>
donde:
</p>

<ul class="org-ul">
<li>\(\frac{\partial \mathcal{L}}{\partial a}\): cuánto cambia la pérdida
si cambia la activación \(a\),</li>
<li>\(\frac{\partial a}{\partial z}\): deriva de la función de activación,</li>
<li>\(\frac{\partial z}{\partial w}\): relación lineal entre \(z\) y el
peso \(w\).</li>
</ul>

<p>
El algoritmo:
</p>

<ol class="org-ol">
<li>Calcula un <b>forward pass</b> (de entrada a salida).</li>
<li>Calcula la pérdida \(\mathcal{L}\).</li>
<li>Propaga gradientes hacia atrás usando la regla de la cadena.</li>
<li>Obtiene \(\frac{\partial \mathcal{L}}{\partial w}\) y
\(\frac{\partial \mathcal{L}}{\partial b}\) para todos los pesos y
bias de la red.</li>
<li>Usa estos gradientes para actualizar los parámetros.</li>
</ol>
</div>
</div>
<div id="outline-container-org2ae20e6" class="outline-3">
<h3 id="org2ae20e6">Descenso de gradiente</h3>
<div class="outline-text-3" id="text-org2ae20e6">
<p>
El <b>descenso de gradiente</b> (gradient descent) es la regla de
actualización básica para minimizar la función de pérdida.
</p>

<p>
Para un peso \(w\):
</p>

<p>
\[
w := w - \eta \, \frac{\partial \mathcal{L}}{\partial w}
\]
</p>

<p>
donde:
</p>

<ul class="org-ul">
<li>\(\eta\) es la <b>tasa de aprendizaje</b> (learning rate),</li>
<li>\(\frac{\partial \mathcal{L}}{\partial w}\) es el gradiente de la
pérdida respecto a \(w\).</li>
</ul>

<p>
Interpretación:
</p>

<ul class="org-ul">
<li>Nos movemos en la dirección <b>opuesta</b> al gradiente (descenso),</li>
<li>La magnitud del paso está controlada por \(\eta\).</li>
</ul>

<p>
En la práctica:
</p>

<ul class="org-ul">
<li>Se usan variantes como:
<ul class="org-ul">
<li><b>Stochastic Gradient Descent (SGD)</b>,</li>
<li>SGD con <b>momentum</b>,</li>
<li><b>Adam</b>, <b>RMSProp</b>, etc.</li>
</ul></li>
<li>Estas variantes mejoran:
<ul class="org-ul">
<li>la velocidad de convergencia,</li>
<li>la estabilidad numérica,</li>
<li>la capacidad de escapar de mínimos locales poco profundos.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgaae5d32" class="outline-2">
<h2 id="orgaae5d32">Ejemplo Clasificación del dataset Iris con MLP</h2>
<div class="outline-text-2" id="text-orgaae5d32">
<p>
Este apartado muestra un ejemplo completo en Python usando el dataset
clásico Iris y una red neuronal feedforward (Perceptrón Multicapa)
usando la librería scikit-learn.
</p>

<p>
El objetivo es clasificar flores Iris en tres clases:
</p>

<ul class="org-ul">
<li>Setosa</li>
<li>Versicolor</li>
<li>Virginica</li>
</ul>


<p>
Importar librerías
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #fb4934;">import</span> numpy <span style="color: #fb4934;">as</span> np
<span style="color: #fb4934;">from</span> sklearn.datasets <span style="color: #fb4934;">import</span> load_iris 
<span style="color: #fb4934;">from</span> sklearn.model_selection <span style="color: #fb4934;">import</span> train_test_split 
<span style="color: #fb4934;">from</span> sklearn.preprocessing <span style="color: #fb4934;">import</span> StandardScaler 
<span style="color: #fb4934;">from</span> sklearn.neural_network <span style="color: #fb4934;">import</span> MLPClassifier 
<span style="color: #fb4934;">from</span> sklearn.metrics <span style="color: #fb4934;">import</span> accuracy_score, classification_report
</pre>
</div>

<p>
Cargar el dataset Iris
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">iris</span> = load_iris<span style="color: #fe8019;">()</span>
<span style="color: #83a598;">X</span> = iris.data <span style="color: #928374;">#</span><span style="color: #928374;">
</span><span style="color: #928374;">#</span><span style="color: #928374;">Caracter&#237;sticas (150 x 4)
</span><span style="color: #83a598;">y</span> = iris.target <span style="color: #928374;"># </span><span style="color: #928374;">Clases (150,)
</span><span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>X.shape<span style="color: #fe8019;">)</span> <span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>y.shape<span style="color: #fe8019;">)</span>
</pre>
</div>

<p>
Las 4 características son:
</p>

<ul class="org-ul">
<li>Largo del sépalo</li>
<li>Ancho del sépalo</li>
<li>Largo del pétalo</li>
<li>Ancho del pétalo</li>
</ul>

<p>
Las clases están codificadas como:
</p>

<ul class="org-ul">
<li>0 → Setosa</li>
<li>1 → Versicolor</li>
<li>2 → Virginica</li>
</ul>

<p>
El siguiente fragmento de código realiza la partición del conjunto de
datos original en dos subconjuntos disjuntos: uno destinado al
entrenamiento del modelo y otro a su evaluación.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">X_train</span>, <span style="color: #83a598;">X_test</span>, <span style="color: #83a598;">y_train</span>, <span style="color: #83a598;">y_test</span> = train_test_split<span style="color: #fe8019;">(</span> X, y, test_size=0.2, random_state=42, stratify=y <span style="color: #fe8019;">)</span>
</pre>
</div>

<p>
Propósito de la partición:
</p>

<p>
En aprendizaje automático supervisado, es fundamental evaluar la
capacidad de generalización de un modelo. Para ello, los datos
disponibles se dividen en:
</p>

<ul class="org-ul">
<li><b>Conjunto de entrenamiento</b>: utilizado para ajustar los parámetros del modelo (pesos y sesgos).</li>
<li><b>Conjunto de prueba</b>: utilizado exclusivamente para medir el desempeño
del modelo sobre datos no vistos durante el entrenamiento.</li>
</ul>

<p>
Esta separación permite detectar fenómenos como overfitting y
underfitting, y proporciona una estimación más realista del
rendimiento esperado en producción.
</p>

<p>
<b>El parámetro</b>:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">test_size</span>=0.2
</pre>
</div>
<p>
indica que el 20 % del conjunto total de datos se reserva para el
conjunto de prueba, mientras que el 80 % restante se utiliza para
entrenamiento. Esta proporción es común en la práctica y representa un
compromiso razonable entre:
</p>

<ul class="org-ul">
<li>disponer de suficientes datos para entrenar el modelo, y</li>
<li>contar con una muestra representativa para evaluar su generalización.</li>
</ul>
</div>
<div id="outline-container-orgb9f554a" class="outline-3">
<h3 id="orgb9f554a">Reproducibilidad del experimento</h3>
<div class="outline-text-3" id="text-orgb9f554a">
<p>
El parámetro:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">random_state</span>=42
</pre>
</div>

<p>
fija la semilla del generador de números aleatorios utilizado durante
la partición.  Esto garantiza que la división de los datos sea
reproducible, es decir, que múltiples ejecuciones del mismo código
produzcan exactamente la misma separación entre entrenamiento y
prueba.
</p>

<p>
La reproducibilidad es un requisito esencial en entornos científicos y
académicos, ya que permite validar y comparar resultados de manera
consistente.
</p>
</div>
</div>
<div id="outline-container-org1c6e1c5" class="outline-3">
<h3 id="org1c6e1c5">Estratificación de clases</h3>
<div class="outline-text-3" id="text-org1c6e1c5">
<p>
El parámetro:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">stratify</span>=y
</pre>
</div>

<p>
indica que la división debe realizarse de forma estratificada,
conservando la proporción original de cada clase en ambos
subconjuntos.
</p>

<p>
Esto es especialmente importante en problemas de clasificación, ya que
una división aleatoria sin estratificación podría provocar:
</p>

<ul class="org-ul">
<li>conjuntos de entrenamiento o prueba con clases desbalanceadas,</li>
<li>métricas de desempeño engañosas,</li>
<li>modelos que no aprendan adecuadamente clases minoritarias.</li>
</ul>

<p>
Mediante la estratificación, se asegura que tanto el conjunto de
entrenamiento como el de prueba sean representativos de la
distribución real de las clases.
</p>
</div>
</div>
<div id="outline-container-org36797cc" class="outline-3">
<h3 id="org36797cc">Importancia en redes neuronales</h3>
<div class="outline-text-3" id="text-org36797cc">
<p>
En el contexto de redes neuronales (incluyendo MLP):
</p>

<ul class="org-ul">
<li>el entrenamiento ajusta los pesos minimizando una función de pérdida
sobre X<sub>train</sub>,</li>

<li>la evaluación sobre X<sub>test</sub> permite medir la capacidad del modelo
para generalizar,</li>

<li>una mala partición puede conducir a conclusiones erróneas sobre el
desempeño del modelo.</li>
</ul>

<p>
Por esta razón, la división adecuada del conjunto de datos constituye
un paso crítico previo al entrenamiento y no debe considerarse un
detalle menor de implementación.
</p>
</div>
</div>
<div id="outline-container-org3636ec6" class="outline-3">
<h3 id="org3636ec6">Normalización de los datos</h3>
<div class="outline-text-3" id="text-org3636ec6">
<p>
La normalización de los datos de entrada es un paso crítico en el
entrenamiento de redes neuronales, ya que influye directamente en la
estabilidad numérica, la velocidad de convergencia y la eficacia del
aprendizaje.
</p>

<p>
En una red neuronal, los pesos se ajustan mediante métodos de
optimización basados en gradientes. Si las variables de entrada
presentan escalas muy diferentes, el proceso de entrenamiento puede
volverse ineficiente o inestable.
</p>

<p>
En muchos conjuntos de datos, las características pueden tener rangos muy distintos. Por ejemplo:
</p>

<ul class="org-ul">
<li>una variable puede tomar valores entre 0 y 1,</li>
<li>otra entre 0 y 10⁶,</li>
<li>otra puede incluir valores negativos.</li>
</ul>

<p>
Cuando estos datos se introducen directamente en una red neuronal:
</p>

<ul class="org-ul">
<li>los gradientes asociados a variables de gran escala dominan la actualización de los pesos,</li>
<li>las funciones de activación pueden saturarse,</li>
<li>el descenso por gradiente se vuelve lento o errático.</li>
</ul>

<p>
La normalización busca homogeneizar la escala de las entradas,
permitiendo que todas las características contribuyan de manera
equilibrada al aprendizaje.
</p>


<p>
Una de las técnicas más utilizadas es la estandarización, implementada
en scikit-learn mediante la clase StandardScaler.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">scaler</span> = StandardScaler<span style="color: #fe8019;">()</span>
<span style="color: #83a598;">X_train</span> = scaler.fit_transform<span style="color: #fe8019;">(</span>X_train<span style="color: #fe8019;">)</span> <span style="color: #83a598;">X_test</span> = scaler.transform<span style="color: #fe8019;">(</span>X_test<span style="color: #fe8019;">)</span>
</pre>
</div>

<p>
Este procedimiento transforma cada característica \(x\) según la siguiente expresión:
</p>

<p>
\[
x' = \frac{x - \mu}{\sigma}
\]
</p>

<p>
donde:
</p>

<ul class="org-ul">
<li>\( \mu \) es la media de la característica,</li>
<li>\( \sigma \) es la desviación estándar de la característica.</li>
</ul>

<p>
Como resultado de esta transformación, cada variable queda con:
</p>

<ul class="org-ul">
<li>media aproximadamente igual a 0,</li>
<li>desviación estándar igual a 1.</li>
</ul>
</div>
</div>
<div id="outline-container-orgc994b47" class="outline-3">
<h3 id="orgc994b47">Definir el Perceptrón Multicapa (MLP)</h3>
<div class="outline-text-3" id="text-orgc994b47">
<p>
El siguiente fragmento de código define un <b>Perceptrón Multicapa</b> (Multilayer Perceptron, MLP)
para un problema de clasificación multiclase, utilizando la implementación provista por
la biblioteca <b>scikit-learn</b>.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">mlp</span> = MLPClassifier<span style="color: #fe8019;">(</span>
<span style="background-color: #3c3836;"> </span>   hidden_layer_sizes=<span style="color: #b16286;">(</span>10, 10<span style="color: #b16286;">)</span>,
<span style="background-color: #3c3836;"> </span>   activation=<span style="color: #b8bb26;">'relu'</span>,
<span style="background-color: #3c3836;"> </span>   solver=<span style="color: #b8bb26;">'adam'</span>,
<span style="background-color: #3c3836;"> </span>   learning_rate_init=0.001,
<span style="background-color: #3c3836;"> </span>   max_iter=2000,
<span style="background-color: #3c3836;"> </span>   random_state=42
<span style="color: #fe8019;">)</span>
</pre>
</div>

<p>
Este modelo corresponde a una red neuronal <b>feedforward completamente conectada</b>,
entrenada mediante <b>backpropagation</b> y optimizada usando métodos de descenso por gradiente.
</p>
</div>
<div id="outline-container-org8a9958c" class="outline-4">
<h4 id="org8a9958c">Arquitectura de la red neuronal</h4>
<div class="outline-text-4" id="text-org8a9958c">
<p>
La arquitectura del MLP queda determinada por el número de características de entrada,
las capas ocultas definidas explícitamente y el número de neuronas en la capa de salida.
</p>
</div>
</div>
<div id="outline-container-org619b778" class="outline-4">
<h4 id="org619b778">Capa de entrada</h4>
<div class="outline-text-4" id="text-org619b778">
<ul class="org-ul">
<li>La capa de entrada está compuesta por <b>4 neuronas</b>.</li>
<li>Cada neurona representa una característica del vector de entrada.</li>
<li>Esta capa no aplica transformaciones, únicamente propaga los valores hacia la primera capa oculta.</li>
</ul>

<p>
Formalmente, el vector de entrada se expresa como:
</p>

<p>
\[
\mathbf{x} = (x_1, x_2, x_3, x_4)
\]
</p>
</div>
</div>
<div id="outline-container-orgba15e1a" class="outline-4">
<h4 id="orgba15e1a">Capas ocultas</h4>
<div class="outline-text-4" id="text-orgba15e1a">
<p>
El parámetro:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">hidden_layer_sizes</span>=<span style="color: #fe8019;">(</span>10, 10<span style="color: #fe8019;">)</span>
</pre>
</div>

<p>
define <b>dos capas ocultas</b>, cada una con <b>10 neuronas</b>.
</p>

<p>
Características principales:
</p>

<ul class="org-ul">
<li>Son capas <b>densas (fully connected)</b>.</li>
<li>Cada neurona recibe como entrada la salida de <b>todas</b> las neuronas de la capa anterior.</li>
<li>Permiten aprender representaciones no lineales del espacio de características.</li>
</ul>

<p>
Cada capa oculta implementa la transformación:
</p>

<p>
\[
\mathbf{h}^{(l)} = f\left( \mathbf{W}^{(l)} \mathbf{h}^{(l-1)} + \mathbf{b}^{(l)} \right)
\]
</p>

<p>
donde:
</p>

<ul class="org-ul">
<li>\( \mathbf{W}^{(l)} \) es la matriz de pesos,</li>
<li>\( \mathbf{b}^{(l)} \) es el vector de sesgos,</li>
<li>\( f(\cdot) \) es la función de activación.</li>
</ul>
</div>
</div>
<div id="outline-container-org8d86e64" class="outline-4">
<h4 id="org8d86e64">Función de activación</h4>
<div class="outline-text-4" id="text-org8d86e64">
<p>
El parámetro:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">activation</span>=<span style="color: #b8bb26;">'relu'</span>
</pre>
</div>

<p>
indica el uso de la función <b>ReLU (Rectified Linear Unit)</b>, definida como:
</p>

<p>
\[
\text{ReLU}(z) = \max(0, z)
\]
</p>

<p>
Esta función se utiliza porque:
</p>

<ul class="org-ul">
<li>introduce no linealidad,</li>
<li>reduce el problema del <b>vanishing gradient</b>,</li>
<li>mejora la estabilidad del entrenamiento,</li>
<li>es computacionalmente eficiente.</li>
</ul>
</div>
</div>
<div id="outline-container-org21da9db" class="outline-4">
<h4 id="org21da9db">Capa de salida</h4>
<div class="outline-text-4" id="text-org21da9db">
<ul class="org-ul">
<li>La capa de salida está compuesta por <b>3 neuronas</b>.</li>
<li>Cada neurona corresponde a una de las clases del problema.</li>
<li>Internamente, el modelo utiliza una función <b>softmax</b> para obtener probabilidades.</li>
</ul>

<p>
La clase predicha se obtiene mediante la operación <b>argmax</b> sobre las salidas.
</p>
</div>
</div>
<div id="outline-container-org36595f6" class="outline-4">
<h4 id="org36595f6">Entrenamiento del modelo</h4>
<div class="outline-text-4" id="text-org36595f6">
<p>
El entrenamiento se realiza mediante el optimizador <b>Adam</b>:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">solver</span>=<span style="color: #b8bb26;">'adam'</span>
</pre>
</div>

<p>
Adam combina:
</p>
<ul class="org-ul">
<li>momentum</li>
<li>tasas de aprendizaje adaptativas</li>
</ul>

<p>
lo que permite una convergencia más rápida y estable en redes con múltiples capas.
</p>

<p>
La tasa de aprendizaje inicial se define como:
</p>

<p>
\[
\eta = 0.001
\]
</p>

<p>
controlando el tamaño de los pasos durante la actualización de los pesos.
</p>
</div>
</div>
</div>
<div id="outline-container-org67ba23d" class="outline-3">
<h3 id="org67ba23d">Número de iteraciones</h3>
<div class="outline-text-3" id="text-org67ba23d">
<p>
El parámetro:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">max_iter</span>=2000
</pre>
</div>

<p>
establece el número máximo de épocas de entrenamiento.
Una época corresponde a un pase completo sobre el conjunto de entrenamiento.
</p>

<p>
Un número elevado de épocas favorece la convergencia, aunque incrementa el riesgo de <b>overfitting</b>.
</p>
</div>
</div>
<div id="outline-container-org5ef4b45" class="outline-3">
<h3 id="org5ef4b45">Reproducibilidad</h3>
<div class="outline-text-3" id="text-org5ef4b45">
<p>
El parámetro:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">random_state</span>=42
</pre>
</div>

<p>
fija la semilla del generador de números aleatorios, garantizando resultados reproducibles
en la inicialización de los pesos y el proceso de entrenamiento.
</p>
</div>
</div>
<div id="outline-container-org442a76a" class="outline-3">
<h3 id="org442a76a">Entrenamiento del modelo</h3>
<div class="outline-text-3" id="text-org442a76a">
<p>
El entrenamiento deñ Perceptrón Multicapa (MLP) consiste en ajustar
los pesos y sesgos de la red para minimizar el error de predicción
sobre los datos de entrenamiento.
</p>

<div class="org-src-container">
<pre class="src src-python">mlp.fit<span style="color: #fe8019;">(</span>X_train, y_train<span style="color: #fe8019;">)</span>
</pre>
</div>

<p>
Durante este proceso ocurren los siguientes pasos fundamentales:
</p>
</div>
<div id="outline-container-org3a2ed56" class="outline-4">
<h4 id="org3a2ed56">Propagación hacia adelante (forward pass)</h4>
<div class="outline-text-4" id="text-org3a2ed56">
<p>
Cada muestra de entrada x atraviesa la red capa por capa.
</p>

<p>
En cada neurona se calcula una combinación lineal y se aplica una función de activación.
</p>

<p>
Se obtiene una salida y que representa la predicción del modelo.
</p>
</div>
</div>
<div id="outline-container-org57945b6" class="outline-4">
<h4 id="org57945b6">Cálculo del error</h4>
<div class="outline-text-4" id="text-org57945b6">
<p>
La salida predicha se compara con la etiqueta real 𝑦.
Se utiliza una función de pérdida (por ejemplo, log-loss para clasificación multiclase).
</p>
</div>
</div>
<div id="outline-container-org6be2449" class="outline-4">
<h4 id="org6be2449">Backpropagation</h4>
<div class="outline-text-4" id="text-org6be2449">
<p>
El error se propaga desde la capa de salida hacia las capas
anteriores. Se calculan los gradientes del error respecto a cada peso
y sesgo. Se aplica la regla de la cadena del cálculo diferencial.
</p>
</div>
</div>
<div id="outline-container-org3bd576b" class="outline-4">
<h4 id="org3bd576b">Descenso de gradiente con Adam</h4>
<div class="outline-text-4" id="text-org3bd576b">
<ul class="org-ul">
<li>Adam combina Momentum y RMSProp.</li>
<li>Ajusta automáticamente la tasa de aprendizaje para cada parámetro.</li>
<li>Proporciona convergencia rápida y estable.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org93e8393" class="outline-3">
<h3 id="org93e8393">Evaluación del modelo</h3>
<div class="outline-text-3" id="text-org93e8393">
<p>
Una vez entrenado el modelo, se evalúa su desempeño usando datos no vistos durante el entrenamiento.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">y_pred</span> = mlp.predict<span style="color: #fe8019;">(</span>X_test<span style="color: #fe8019;">)</span>
<span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span><span style="color: #b8bb26;">"Accuracy:"</span>, accuracy_score<span style="color: #b16286;">(</span>y_test, y_pred<span style="color: #b16286;">)</span><span style="color: #fe8019;">)</span> <span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span><span style="color: #b8bb26;">"</span><span style="color: #d3869b;">\n</span><span style="color: #b8bb26;">Reporte de clasificaci&#243;n:</span><span style="color: #d3869b;">\n</span><span style="color: #b8bb26;">"</span><span style="color: #fe8019;">)</span> <span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>classification_report<span style="color: #b16286;">(</span>y_test, y_pred, target_names=iris.target_names<span style="color: #b16286;">)</span><span style="color: #fe8019;">)</span>
</pre>
</div>
</div>
<div id="outline-container-orga226bf1" class="outline-4">
<h4 id="orga226bf1">Métricas utilizadas</h4>
<div class="outline-text-4" id="text-orga226bf1">
<ul class="org-ul">
<li>Accuracy: proporción total de predicciones correctas.</li>
<li>Precision: qué tan confiables son las predicciones positivas.</li>
<li>Recall: capacidad del modelo para encontrar todos los ejemplos de una clase.</li>
<li>F1-score: balance entre precisión y recall.</li>
</ul>

<p>
En el dataset Iris, el accuracy suele estar en el rango del 95% al
100%, debido a su baja dimensionalidad y buena separación entre
clases.
</p>
</div>
</div>
</div>
<div id="outline-container-orgc8cf9ea" class="outline-3">
<h3 id="orgc8cf9ea">Predicción con una nueva flor</h3>
<div class="outline-text-3" id="text-orgc8cf9ea">
<p>
El modelo entrenado puede utilizarse para predecir nuevas muestras.
</p>

<div class="org-src-container">
<pre class="src src-python">
<span style="color: #928374;">#</span><span style="color: #928374;">[largo_sepalo, ancho_sepalo, largo_petalo, ancho_petalo]
</span>
<span style="color: #83a598;">flor_nueva</span> = np.array<span style="color: #fe8019;">(</span><span style="color: #b16286;">[</span><span style="color: #b8bb26;">[</span>5.1, 3.5, 1.4, 0.2<span style="color: #b8bb26;">]</span><span style="color: #b16286;">]</span><span style="color: #fe8019;">)</span>
<span style="color: #928374;">#</span><span style="color: #928374;">Normalizaci&#243;n con el mismo scaler del entrenamiento
</span><span style="color: #83a598;">flor_nueva</span> = scaler.transform<span style="color: #fe8019;">(</span>flor_nueva<span style="color: #fe8019;">)</span>
<span style="color: #83a598;">prediccion</span> = mlp.predict<span style="color: #fe8019;">(</span>flor_nueva<span style="color: #fe8019;">)</span> <span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span><span style="color: #b8bb26;">"Clase predicha:"</span>, iris.target_names<span style="color: #b16286;">[</span>prediccion<span style="color: #b8bb26;">[</span>0<span style="color: #b8bb26;">]</span><span style="color: #b16286;">]</span><span style="color: #fe8019;">)</span>
</pre>
</div>

<p>
Es fundamental aplicar la misma normalización usada en el entrenamiento para mantener coherencia en las escalas.
</p>
</div>
</div>
<div id="outline-container-orgf8f62fb" class="outline-3">
<h3 id="orgf8f62fb">Interpretación matemática del MLP</h3>
<div class="outline-text-3" id="text-orgf8f62fb">
<p>
Cada neurona de la red realiza el siguiente cálculo:
</p>

<pre class="example" id="org92d89fd">
z = w · x + b a = f(z)
</pre>

<p>
Donde: 𝑤 es el vector de pesos 𝑥 es el vector de entradas 𝑏 es el sesgo (bias) 𝑓
es la función de activación (ReLU en las capas ocultas). La capa de salida utiliza la función Softmax:
</p>

<div class="latex" id="orge1b009b">
<p>
\text{softmax}(z<sub>i</sub>) = \frac{e<sup>z<sub>i</sub></sup>}{&sum;<sub>j</sub> e<sup>z<sub>j</sub></sup>}
</p>

</div>

<p>
Esto permite interpretar las salidas como probabilidades para cada clase.
</p>
</div>
</div>
</div>
<div id="outline-container-org1a594ca" class="outline-2">
<h2 id="org1a594ca">Ejemplo Clasificación de Cáncer de Mama con MLP</h2>
<div class="outline-text-2" id="text-org1a594ca">
<p>
Este apartado muestra un ejemplo completo en Python usando el dataset
<b>Breast Cancer Wisconsin (Diagnostic)</b> y un <b>Perceptrón Multicapa (MLP)</b>
usando la librería scikit-learn.
</p>

<p>
El objetivo es clasificar tumores en dos categorías:
</p>

<ul class="org-ul">
<li>0 → Maligno</li>
<li>1 → Benigno</li>
</ul>

<p>
Este problema es un caso típico de <b><b>clasificación binaria</b></b> en el
ámbito médico, donde es crucial:
</p>

<ul class="org-ul">
<li>minimizar falsos negativos (tumor maligno clasificado como benigno),</li>
<li>mantener una buena precisión general del modelo.</li>
</ul>
</div>
<div id="outline-container-orgcfb19ed" class="outline-3">
<h3 id="orgcfb19ed">Importar librerías</h3>
<div class="outline-text-3" id="text-orgcfb19ed">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #fb4934;">import</span> numpy <span style="color: #fb4934;">as</span> np
<span style="color: #fb4934;">from</span> sklearn.datasets <span style="color: #fb4934;">import</span> load_breast_cancer
<span style="color: #fb4934;">from</span> sklearn.model_selection <span style="color: #fb4934;">import</span> train_test_split
<span style="color: #fb4934;">from</span> sklearn.preprocessing <span style="color: #fb4934;">import</span> StandardScaler
<span style="color: #fb4934;">from</span> sklearn.neural_network <span style="color: #fb4934;">import</span> MLPClassifier
<span style="color: #fb4934;">from</span> sklearn.metrics <span style="color: #fb4934;">import</span> accuracy_score, classification_report
</pre>
</div>

<p>
En este bloque:
</p>

<ul class="org-ul">
<li><code>load_breast_cancer</code>: carga el dataset de cáncer de mama.</li>
<li><code>train_test_split</code>: divide en entrenamiento y prueba.</li>
<li><code>StandardScaler</code>: aplica estandarización a las características.</li>
<li><code>MLPClassifier</code>: implementa un Perceptrón Multicapa (MLP).</li>
<li><code>accuracy_score</code> y <code>classification_report</code>: permiten evaluar el modelo.</li>
</ul>
</div>
</div>
<div id="outline-container-orge0aca29" class="outline-3">
<h3 id="orge0aca29">Cargar el dataset Breast Cancer</h3>
<div class="outline-text-3" id="text-orge0aca29">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">data</span> = load_breast_cancer<span style="color: #fe8019;">()</span>
<span style="color: #83a598;">X</span> = data.data      <span style="color: #928374;"># </span><span style="color: #928374;">Caracter&#237;sticas (569 x 30)
</span><span style="color: #83a598;">y</span> = data.target    <span style="color: #928374;"># </span><span style="color: #928374;">Etiquetas (569,)
</span>
<span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span><span style="color: #b8bb26;">"Dimensiones de X:"</span>, X.shape<span style="color: #fe8019;">)</span>
<span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span><span style="color: #b8bb26;">"Clases num&#233;ricas:"</span>, np.unique<span style="color: #b16286;">(</span>y<span style="color: #b16286;">)</span><span style="color: #fe8019;">)</span>
<span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span><span style="color: #b8bb26;">"Nombres de clases:"</span>, data.target_names<span style="color: #fe8019;">)</span>
</pre>
</div>

<p>
Este dataset contiene:
</p>

<ul class="org-ul">
<li>569 muestras (pacientes).</li>
<li>30 características numéricas que describen propiedades de los tumores
(radio, textura, perímetro, área, suavidad, compacidad, etc.).</li>
<li>2 clases:
<ul class="org-ul">
<li>0 → malignant (maligno)</li>
<li>1 → benign (benigno)</li>
</ul></li>
</ul>

<p>
Cada muestra corresponde a una imagen de tejido mamario analizada
digitalmente. Las 30 características se derivan de medidas estadísticas
de la forma, textura y estructura del tumor.
</p>
</div>
<div id="outline-container-orgdff9e80" class="outline-4">
<h4 id="orgdff9e80">Importancia del dominio</h4>
<div class="outline-text-4" id="text-orgdff9e80">
<p>
En aplicaciones médicas, la interpretación de las predicciones es muy
sensible:
</p>

<ul class="org-ul">
<li>un <b>falso negativo</b> (maligno etiquetado como benigno) puede retrasar
un diagnóstico crítico;</li>
<li>un <b>falso positivo</b> (benigno etiquetado como maligno) puede generar
ansiedad y procedimientos innecesarios.</li>
</ul>

<p>
Por ello, además de la <b>accuracy</b>, suelen analizarse:
</p>

<ul class="org-ul">
<li>sensibilidad (<b>recall</b>) para la clase “maligno”,</li>
<li>especificidad,</li>
<li>matriz de confusión, etc.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orga00e48e" class="outline-3">
<h3 id="orga00e48e">Partición del conjunto de datos</h3>
<div class="outline-text-3" id="text-orga00e48e">
<p>
Dividimos el dataset en entrenamiento y prueba:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">X_train</span>, <span style="color: #83a598;">X_test</span>, <span style="color: #83a598;">y_train</span>, <span style="color: #83a598;">y_test</span> = train_test_split<span style="color: #fe8019;">(</span>
<span style="background-color: #3c3836;"> </span>   X,
<span style="background-color: #3c3836;"> </span>   y,
<span style="background-color: #3c3836;"> </span>   test_size=0.3,
<span style="background-color: #3c3836;"> </span>   random_state=42,
<span style="background-color: #3c3836;"> </span>   stratify=y
<span style="color: #fe8019;">)</span>
</pre>
</div>
</div>
<div id="outline-container-orgc32a140" class="outline-4">
<h4 id="orgc32a140">Significado de los parámetros</h4>
<div class="outline-text-4" id="text-orgc32a140">
<ul class="org-ul">
<li><code>test_size=0.3</code>:
<ul class="org-ul">
<li>30 % de los datos se reservan para el <b>conjunto de prueba</b>.</li>
<li>70 % se usan para entrenar el modelo.</li>
<li>En este caso:
<ul class="org-ul">
<li>Entrenamiento: ≈ 398 muestras.</li>
<li>Prueba: ≈ 171 muestras.</li>
</ul></li>
</ul></li>

<li><code>random_state=42</code>:
<ul class="org-ul">
<li>Fija la semilla del generador de números aleatorios.</li>
<li>Permite que la partición sea reproducible: el mismo código genera
siempre la misma división.</li>
</ul></li>

<li><code>stratify=y</code>:
<ul class="org-ul">
<li>Asegura que la proporción de clases (maligno/benigno) sea similar
tanto en entrenamiento como en prueba.</li>
<li>Esto es crucial en problemas médicos, donde el balance de clases
puede influir fuertemente en las métricas.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orga7160b9" class="outline-4">
<h4 id="orga7160b9">Propósito de la partición</h4>
<div class="outline-text-4" id="text-orga7160b9">
<p>
En aprendizaje automático supervisado:
</p>

<ul class="org-ul">
<li>El <b>conjunto de entrenamiento</b> se usa para ajustar los parámetros
(pesos y sesgos) del modelo.</li>
<li>El <b>conjunto de prueba</b> se usa <b>solo</b> para evaluar la capacidad de
generalización sobre datos no vistos.</li>
</ul>

<p>
Esta separación es esencial para:
</p>

<ul class="org-ul">
<li>detectar <b>overfitting</b> (el modelo memoriza el entrenamiento pero
generaliza mal),</li>
<li>obtener una estimación realista del rendimiento.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgd523696" class="outline-3">
<h3 id="orgd523696">Normalización (Estandarización)</h3>
<div class="outline-text-3" id="text-orgd523696">
<p>
Las características tienen escalas muy diferentes (áreas grandes,
texturas pequeñas, etc.). Esto puede causar:
</p>

<ul class="org-ul">
<li>problemas numéricos,</li>
<li>gradientes desbalanceados,</li>
<li>convergencia lenta o inestable.</li>
</ul>

<p>
Por ello, aplicamos estandarización:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">scaler</span> = StandardScaler<span style="color: #fe8019;">()</span>
<span style="color: #83a598;">X_train</span> = scaler.fit_transform<span style="color: #fe8019;">(</span>X_train<span style="color: #fe8019;">)</span>
<span style="color: #83a598;">X_test</span> = scaler.transform<span style="color: #fe8019;">(</span>X_test<span style="color: #fe8019;">)</span>
</pre>
</div>

<p>
Este procedimiento transforma cada característica \(x\) según:
</p>

<div class="latex" id="orge42b922">
<p>
x' = \frac{x - \mu}{\sigma}
</p>

</div>

<p>
donde:
</p>

<ul class="org-ul">
<li>\(\mu\) es la media de la característica calculada sobre el conjunto
de entrenamiento,</li>
<li>\(\sigma\) es la desviación estándar de dicha característica.</li>
</ul>

<p>
Después de la transformación:
</p>

<ul class="org-ul">
<li>cada variable tiene media aproximadamente 0,</li>
<li>y desviación estándar aproximadamente 1.</li>
</ul>
</div>
<div id="outline-container-org04bd439" class="outline-4">
<h4 id="org04bd439">Importante</h4>
<div class="outline-text-4" id="text-org04bd439">
<ul class="org-ul">
<li>Solo se llama a <code>fit_transform</code> en <code>X_train</code>:
<ul class="org-ul">
<li><code>fit</code> calcula \(\mu\) y \(\sigma\) usando <b>solo entrenamiento</b>.</li>
<li><code>transform</code> aplica la transformación.</li>
</ul></li>
<li>Para <code>X_test</code> se usa únicamente <code>transform</code>:
<ul class="org-ul">
<li>se reescalan los datos de prueba con los mismos parámetros
(\(\mu\) y \(\sigma\)) obtenidos del entrenamiento.</li>
</ul></li>
<li>Esto evita <b>filtrar información del conjunto de prueba</b> hacia el
proceso de entrenamiento (data leakage).</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org5ecb641" class="outline-3">
<h3 id="org5ecb641">Definición del Perceptrón Multicapa (MLP)</h3>
<div class="outline-text-3" id="text-org5ecb641">
<p>
Definimos un MLP para clasificación binaria:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">mlp</span> = MLPClassifier<span style="color: #fe8019;">(</span>
<span style="background-color: #3c3836;"> </span>   hidden_layer_sizes=<span style="color: #b16286;">(</span>30, 15<span style="color: #b16286;">)</span>,
<span style="background-color: #3c3836;"> </span>   activation=<span style="color: #b8bb26;">'relu'</span>,
<span style="background-color: #3c3836;"> </span>   solver=<span style="color: #b8bb26;">'adam'</span>,
<span style="background-color: #3c3836;"> </span>   max_iter=1000,
<span style="background-color: #3c3836;"> </span>   random_state=42
<span style="color: #fe8019;">)</span>
</pre>
</div>
</div>
<div id="outline-container-orgdb08b61" class="outline-4">
<h4 id="orgdb08b61">Arquitectura del modelo</h4>
<div class="outline-text-4" id="text-orgdb08b61">
<p>
El MLP es una red neuronal <b>feedforward</b> completamente conectada.
</p>

<ul class="org-ul">
<li><b>Capa de entrada</b>:
<ul class="org-ul">
<li>Tiene tantas neuronas como características de entrada: 30.</li>
<li>Cada componente del vector \(\mathbf{x} \in \mathbb{R}^{30}\)
representa una característica del tumor.</li>
</ul></li>

<li><p>
<b>Capas ocultas</b>:
</p>
<ul class="org-ul">
<li>Primer capa oculta: 30 neuronas.</li>
<li>Segunda capa oculta: 15 neuronas.</li>
<li><p>
Se especifican con el parámetro:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">hidden_layer_sizes</span> = <span style="color: #fe8019;">(</span>30, 15<span style="color: #fe8019;">)</span>
</pre>
</div></li>
</ul>

<p>
Cada neurona en una capa oculta:
</p>
<ul class="org-ul">
<li>recibe como entrada la salida de todas las neuronas de la capa
anterior (capas densas),</li>
<li>aplica una transformación lineal seguida de una función de
activación no lineal.</li>
</ul></li>

<li><b>Capa de salida</b>:
<ul class="org-ul">
<li>Para clasificación binaria, internamente el MLP de scikit-learn
puede usar:
<ul class="org-ul">
<li>1 neurona con activación logística (sigmoide),</li>
<li>o una representación equivalente a 2 clases.</li>
</ul></li>
<li>La salida se interpreta como una probabilidad de pertenecer a la
clase “1” (benigno).</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org01f7b3b" class="outline-4">
<h4 id="org01f7b3b">Función de activación ReLU</h4>
<div class="outline-text-4" id="text-org01f7b3b">
<p>
El parámetro:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">activation</span> = <span style="color: #b8bb26;">'relu'</span>
</pre>
</div>

<p>
indica el uso de la función <b>ReLU (Rectified Linear Unit)</b> en las capas
ocultas:
</p>

<div class="latex" id="org8146d3f">
<p>
\text{ReLU}(z) = max(0, z)
</p>

</div>

<p>
Ventajas de ReLU:
</p>

<ul class="org-ul">
<li>Introduce no linealidad (permite aproximar funciones no lineales).</li>
<li>Reduce el problema del <b>vanishing gradient</b> en comparación con
funciones como sigmoide o tanh.</li>
<li>Es simple y eficiente de calcular.</li>
</ul>
</div>
</div>
<div id="outline-container-org9685931" class="outline-4">
<h4 id="org9685931">Optimizador Adam</h4>
<div class="outline-text-4" id="text-org9685931">
<p>
El parámetro:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">solver</span> = <span style="color: #b8bb26;">'adam'</span>
</pre>
</div>

<p>
selecciona el optimizador <b>Adam</b> (Adaptive Moment Estimation), que:
</p>

<ul class="org-ul">
<li>combina ideas de Momentum y RMSProp,</li>
<li>mantiene promedios móviles de:
<ul class="org-ul">
<li>los gradientes,</li>
<li>los gradientes al cuadrado,</li>
</ul></li>
<li>adapta la tasa de aprendizaje para cada parámetro.</li>
</ul>

<p>
Esto suele producir:
</p>

<ul class="org-ul">
<li>convergencia rápida,</li>
<li>estabilidad numérica,</li>
<li>buen desempeño sin necesidad de mucha sintonía manual.</li>
</ul>
</div>
</div>
<div id="outline-container-org9e89726" class="outline-4">
<h4 id="org9e89726">Número máximo de iteraciones</h4>
<div class="outline-text-4" id="text-org9e89726">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">max_iter</span> = 1000
</pre>
</div>

<ul class="org-ul">
<li>Indica el número máximo de épocas (o iteraciones sobre los datos)
para entrenar la red.</li>
<li>Un valor alto permite que el modelo converja, aunque si es excesivo
puede aumentar el riesgo de <b>overfitting</b> (la red se ajusta demasiado
al entrenamiento).</li>
</ul>
</div>
</div>
<div id="outline-container-orgf000720" class="outline-4">
<h4 id="orgf000720">Reproducibilidad</h4>
<div class="outline-text-4" id="text-orgf000720">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">random_state</span> = 42
</pre>
</div>

<ul class="org-ul">
<li>Fija la semilla para la inicialización de pesos y el muestreo interno.</li>
<li>Permite reproducir exactamente los mismos resultados entre ejecuciones.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org6df4285" class="outline-3">
<h3 id="org6df4285">Entrenamiento del modelo</h3>
<div class="outline-text-3" id="text-org6df4285">
<p>
Entrenamos el MLP ajustando pesos y sesgos para minimizar el error de
clasificación en el conjunto de entrenamiento:
</p>

<div class="org-src-container">
<pre class="src src-python">mlp.fit<span style="color: #fe8019;">(</span>X_train, y_train<span style="color: #fe8019;">)</span>
</pre>
</div>
</div>
<div id="outline-container-orgfa9f612" class="outline-4">
<h4 id="orgfa9f612">Proceso interno (visión conceptual)</h4>
<div class="outline-text-4" id="text-orgfa9f612">
<ol class="org-ol">
<li><b>Propagación hacia adelante</b>:
<ul class="org-ul">
<li>Cada muestra \(\mathbf{x}\) pasa por las capas:
\[
       \mathbf{h}^{(1)} = f_1(W^{(1)} \mathbf{x} + \mathbf{b}^{(1)})
     \]
\[
       \mathbf{h}^{(2)} = f_2(W^{(2)} \mathbf{h}^{(1)} + \mathbf{b}^{(2)})
     \]
\[
       \hat{y} = \sigma(W^{(3)} \mathbf{h}^{(2)} + \mathbf{b}^{(3)})
     \]</li>
<li>\(\hat{y}\) es la probabilidad estimada de que el tumor sea benigno
(clase 1).</li>
</ul></li>

<li><b>Cálculo del error</b>:
<ul class="org-ul">
<li>La salida \(\hat{y}\) se compara con la etiqueta real \(y \in \{0, 1\}\).</li>
<li>En problemas binarios, se usa típicamente la <b>entropía cruzada
binaria</b> (o log-loss):
\[
       L(y, \hat{y}) = -\left[y \log(\hat{y}) + (1 - y)\log(1 - \hat{y})\right]
     \]</li>
</ul></li>

<li><b>Backpropagation</b>:
<ul class="org-ul">
<li>Se calculan las derivadas parciales de la pérdida con respecto a
todos los pesos y sesgos mediante la regla de la cadena.</li>
<li>Se propaga el error desde la capa de salida hacia las capas
ocultas.</li>
</ul></li>

<li><b>Actualización de pesos</b>:
<ul class="org-ul">
<li>El optimizador Adam actualiza cada parámetro en la dirección que
reduce la pérdida:
\[
       \theta \leftarrow \theta - \eta \cdot \hat{g}
     \]
donde \(\hat{g}\) es una versión adaptada del gradiente, y
\(\eta\) la tasa de aprendizaje efectiva.</li>
</ul></li>
</ol>
</div>
</div>
</div>
<div id="outline-container-orgc1c0bc8" class="outline-3">
<h3 id="orgc1c0bc8">Evaluación del modelo</h3>
<div class="outline-text-3" id="text-orgc1c0bc8">
<p>
Una vez entrenado, evaluamos el modelo sobre el conjunto de prueba:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #928374;"># </span><span style="color: #928374;">Predicci&#243;n
</span><span style="color: #83a598;">y_pred</span> = mlp.predict<span style="color: #fe8019;">(</span>X_test<span style="color: #fe8019;">)</span>

<span style="color: #928374;"># </span><span style="color: #928374;">M&#233;tricas
</span><span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span><span style="color: #b8bb26;">"Accuracy:"</span>, accuracy_score<span style="color: #b16286;">(</span>y_test, y_pred<span style="color: #b16286;">)</span><span style="color: #fe8019;">)</span>
<span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span><span style="color: #b8bb26;">"</span><span style="color: #d3869b;">\n</span><span style="color: #b8bb26;">Reporte de clasificaci&#243;n:</span><span style="color: #d3869b;">\n</span><span style="color: #b8bb26;">"</span><span style="color: #fe8019;">)</span>
<span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>classification_report<span style="color: #b16286;">(</span>y_test, y_pred, target_names=data.target_names<span style="color: #b16286;">)</span><span style="color: #fe8019;">)</span>
</pre>
</div>
</div>
<div id="outline-container-org29cb597" class="outline-4">
<h4 id="org29cb597">Métricas principales</h4>
<div class="outline-text-4" id="text-org29cb597">
<ul class="org-ul">
<li><b>Accuracy</b>:
<ul class="org-ul">
<li>Proporción de predicciones correctas:
\[
      \text{Accuracy} = \frac{\text{número de aciertos}}{\text{número total de muestras}}
    \]</li>
</ul></li>
<li><b>Precision</b>:
<ul class="org-ul">
<li>De todos los ejemplos predichos como positivos, qué fracción son
realmente positivos.</li>
<li>Importante para evitar falsos positivos.</li>
</ul></li>

<li><b>Recall (Sensibilidad)</b>:
<ul class="org-ul">
<li>De todos los ejemplos realmente positivos, qué fracción se detecta
correctamente.</li>
<li>En diagnóstico médico, el recall para la clase “maligno” es crítico
(minimizar falsos negativos).</li>
</ul></li>

<li><b>F1-score</b>:
<ul class="org-ul">
<li>Media armónica de precision y recall:
\[
      F1 = 2 \cdot \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}
    \]</li>
<li>Útil cuando hay cierto desbalance entre clases.</li>
</ul></li>
</ul>

<p>
El dataset Breast Cancer suele producir <b>accuracy</b> muy altas (por
encima de 95 %) con modelos bien entrenados, debido a que las
características están relativamente bien separadas.
</p>
</div>
</div>
</div>
<div id="outline-container-orgbb4f643" class="outline-3">
<h3 id="orgbb4f643">Interpretación matemática de la salida binaria</h3>
<div class="outline-text-3" id="text-orgbb4f643">
<p>
En este modelo binario, la capa de salida puede verse como una neurona
con activación <b>sigmoide (logística)</b>:
</p>

<div class="latex" id="orgc24b7af">
<p>
&sigma;(z) = \frac{1}{1 + e<sup>-z</sup>}
</p>

</div>

<p>
donde:
</p>

<ul class="org-ul">
<li>\(z = \mathbf{w} \cdot \mathbf{h} + b\),</li>
<li>\(\mathbf{h}\) es el vector de activaciones de la última capa oculta.</li>
</ul>

<p>
La salida \(\sigma(z)\) se interpreta como:
</p>

<ul class="org-ul">
<li>\(\sigma(z) \approx 1\): alta probabilidad de clase 1 (benigno),</li>
<li>\(\sigma(z) \approx 0\): alta probabilidad de clase 0 (maligno).</li>
</ul>

<p>
En scikit-learn, la predicción de clase se realiza típicamente como:
</p>

<ul class="org-ul">
<li>Si \(\sigma(z) \geq 0.5\) → clase 1 (benigno).</li>
<li>Si \(\sigma(z) < 0.5\) → clase 0 (maligno).</li>
</ul>

<p>
Este umbral puede modificarse en contextos médicos, por ejemplo:
</p>

<ul class="org-ul">
<li>usar un umbral menor que 0.5 para aumentar la sensibilidad a tumores
malignos (aceptando más falsos positivos).</li>
</ul>
</div>
</div>
<div id="outline-container-org8beeb5f" class="outline-3">
<h3 id="org8beeb5f">Predicción de una nueva muestra</h3>
<div class="outline-text-3" id="text-org8beeb5f">
<p>
Podemos usar el modelo entrenado para predecir el diagnóstico de un
nuevo paciente.
</p>

<p>
En este ejemplo, tomamos la primera muestra del set de prueba (ya
normalizada) y realizamos la predicción:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #928374;"># </span><span style="color: #928374;">Ejemplo con la primera fila del set de prueba
</span><span style="color: #83a598;">nuevo_paciente</span> = X_test<span style="color: #fe8019;">[</span>0:1<span style="color: #fe8019;">]</span>     <span style="color: #928374;"># </span><span style="color: #928374;">forma (1, 30)
</span><span style="color: #83a598;">pred</span> = mlp.predict<span style="color: #fe8019;">(</span>nuevo_paciente<span style="color: #fe8019;">)</span>
<span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span><span style="color: #b8bb26;">"Resultado del diagn&#243;stico:"</span>, data.target_names<span style="color: #b16286;">[</span>pred<span style="color: #b8bb26;">[</span>0<span style="color: #b8bb26;">]</span><span style="color: #b16286;">]</span><span style="color: #fe8019;">)</span>
</pre>
</div>
</div>
<div id="outline-container-orgc9397bb" class="outline-4">
<h4 id="orgc9397bb">Comentarios importantes</h4>
<div class="outline-text-4" id="text-orgc9397bb">
<ul class="org-ul">
<li>Es fundamental que la nueva muestra haya sido procesada con el mismo:

<ol class="org-ol">
<li>Conjunto de características (mismas columnas y orden).</li>
<li>Mismo procedimiento de normalización (mismo <code>scaler</code>, con las
medias y desviaciones aprendidas en el entrenamiento).</li>
</ol></li>

<li>En la práctica, se suele:

<ul class="org-ul">
<li>guardar el modelo entrenado (por ejemplo con <code>joblib</code>),</li>
<li>guardar también el objeto <code>StandardScaler</code>,</li>
<li>aplicar ambos de manera coherente a los nuevos datos.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org9adda36" class="outline-3">
<h3 id="org9adda36">Resumen conceptual</h3>
<div class="outline-text-3" id="text-org9adda36">
<p>
Este ejemplo ilustra varios conceptos clave en el uso de redes
neuronales (MLP) para clasificación binaria en un contexto médico:
</p>

<ul class="org-ul">
<li>Importancia de:
<ul class="org-ul">
<li>Partición entrenamiento/prueba estratificada.</li>
<li>Normalización de características.</li>
<li>Arquitectura de red (capas y neuronas).</li>
<li>Función de activación (ReLU).</li>
<li>Optimizador (Adam) y número de iteraciones.</li>
</ul></li>

<li>Interpretación de la salida:
<ul class="org-ul">
<li>Probabilidad generada por una función sigmoide.</li>
<li>Decisión de clase basada en un umbral.</li>
</ul></li>

<li>Evaluación:
<ul class="org-ul">
<li>No basta con accuracy: hay que considerar precision, recall y F1,
especialmente para la clase de interés (por ejemplo, “maligno”).</li>
</ul></li>
</ul>

<p>
Este mismo flujo (carga, partición, normalización, definición de modelo,
entrenamiento, evaluación, predicción) puede adaptarse a otros modelos
(árboles, SVM, LSTM, etc.) y a otros datasets de clasificación binaria
o multiclase.
</p>
</div>
</div>
</div>
<div id="outline-container-org9ccd6ed" class="outline-2">
<h2 id="org9ccd6ed">Juego de Bala y Salto con MLP</h2>
<div class="outline-text-2" id="text-org9ccd6ed">
</div>
<div id="outline-container-org32d909e" class="outline-3">
<h3 id="org32d909e">Introducción</h3>
<div class="outline-text-3" id="text-org32d909e">
</div>
<div id="outline-container-orgce618db" class="outline-4">
<h4 id="orgce618db">¿Qué es este juego?</h4>
<div class="outline-text-4" id="text-orgce618db">
<p>
Este es un juego que combina mecánicas de juego simples con
aprendizaje automático (Machine Learning). El objetivo es entrenar una
red neuronal (MLP - Multi-Layer Perceptron) para que aprenda a jugar
imitando tu estilo de juego.
</p>
</div>
</div>
<div id="outline-container-org1da87d8" class="outline-4">
<h4 id="org1da87d8">Concepto principal</h4>
<div class="outline-text-4" id="text-org1da87d8">
<ul class="org-ul">
<li>Tú juegas en modo MANUAL y el juego registra tus decisiones.</li>
<li>El modelo MLP aprende de tus patrones de juego.</li>
<li>En modo AUTO, el MLP juega por ti usando lo que aprendió.</li>
</ul>
</div>
</div>
<div id="outline-container-orgab8bc73" class="outline-4">
<h4 id="orgab8bc73">Objetivo del juego</h4>
<div class="outline-text-4" id="text-orgab8bc73">
<p>
Esquivar las balas que vienen desde la derecha saltando en el momento adecuado. El MLP aprende cuándo saltar basándose en:
</p>
<ul class="org-ul">
<li>La velocidad de la bala</li>
<li>La distancia entre el jugador y la bala</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org6003f0e" class="outline-3">
<h3 id="org6003f0e">Requisitos e Instalación</h3>
<div class="outline-text-3" id="text-org6003f0e">
</div>
<div id="outline-container-orga871fae" class="outline-4">
<h4 id="orga871fae">Dependencias necesarias</h4>
<div class="outline-text-4" id="text-orga871fae">
<p>
El juego requiere las siguientes librerías de Python:
</p>

<div class="org-src-container">
<pre class="src src-python">pygame &gt;= 2.6.0
scikit-learn &gt;= 1.0.0
matplotlib &gt;= 3.5.0
numpy &gt;= 1.21.0
</pre>
</div>
</div>
</div>
<div id="outline-container-org908e2fb" class="outline-4">
<h4 id="org908e2fb">Instalación</h4>
<div class="outline-text-4" id="text-org908e2fb">
<div class="org-src-container">
<pre class="src src-bash">pip install pygame scikit-learn matplotlib numpy
</pre>
</div>
</div>
</div>
<div id="outline-container-org9e8a58f" class="outline-4">
<h4 id="org9e8a58f">Estructura de archivos</h4>
<div class="outline-text-4" id="text-org9e8a58f">
<p>
El proyecto contiene:
</p>

<dl class="org-dl">
<dt><code>juego_pygame_mlp.py</code></dt><dd>Juego principal con MLP</dd>
<dt><code>juego_pygame_mlp_plot.py</code></dt><dd>Script para visualizar datos del CSV</dd>
<dt><code>datos_mlp.csv</code></dt><dd>Datos exportados (se genera al usar la opción C)</dd>
<dt><code>assets/</code></dt><dd>Carpeta con sprites y fondos del juego</dd>
</dl>
</div>
</div>
</div>
<div id="outline-container-org011ba29" class="outline-3">
<h3 id="org011ba29">Cómo Jugar</h3>
<div class="outline-text-3" id="text-org011ba29">
</div>
<div id="outline-container-orgb97189e" class="outline-4">
<h4 id="orgb97189e">Iniciar el juego</h4>
<div class="outline-text-4" id="text-orgb97189e">
<div class="org-src-container">
<pre class="src src-bash">python juego_pygame_mlp.py
</pre>
</div>
</div>
</div>
<div id="outline-container-org63a3479" class="outline-4">
<h4 id="org63a3479">Menú principal</h4>
<div class="outline-text-4" id="text-org63a3479">
<p>
Al iniciar verás un menú con las siguientes opciones:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Tecla</th>
<th scope="col" class="org-left">Acción</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">M</td>
<td class="org-left">Modo Manual (reinicia dataset y borra modelo)</td>
</tr>

<tr>
<td class="org-left">A</td>
<td class="org-left">Modo Auto (usa MLP; sin modelo NO salta)</td>
</tr>

<tr>
<td class="org-left">T</td>
<td class="org-left">Entrenar MLP</td>
</tr>

<tr>
<td class="org-left">C</td>
<td class="org-left">Exportar datos a CSV</td>
</tr>

<tr>
<td class="org-left">F</td>
<td class="org-left">Fullscreen (toggle)</td>
</tr>

<tr>
<td class="org-left">Q</td>
<td class="org-left">Salir</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-org6a7cfea" class="outline-4">
<h4 id="org6a7cfea">Controles durante el juego</h4>
<div class="outline-text-4" id="text-org6a7cfea">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Tecla</th>
<th scope="col" class="org-left">Acción</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">ESPACIO</td>
<td class="org-left">Saltar (solo en modo manual)</td>
</tr>

<tr>
<td class="org-left">ESC / P</td>
<td class="org-left">Volver al menú</td>
</tr>

<tr>
<td class="org-left">F</td>
<td class="org-left">Alternar pantalla completa</td>
</tr>

<tr>
<td class="org-left">Q</td>
<td class="org-left">Salir del juego</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="outline-container-org51f5598" class="outline-3">
<h3 id="org51f5598">Modo Manual: Recolectando Datos</h3>
<div class="outline-text-3" id="text-org51f5598">
</div>
<div id="outline-container-org9e2e54b" class="outline-4">
<h4 id="org9e2e54b">¿Qué hace el modo manual?</h4>
<div class="outline-text-4" id="text-org9e2e54b">
<p>
En modo manual, tú controlas al personaje y el juego registra automáticamente:
</p>
<ul class="org-ul">
<li>La velocidad de cada bala</li>
<li>La distancia entre el jugador y la bala</li>
<li>Si decidiste saltar (1) o no saltar (0) en cada frame</li>
</ul>
</div>
</div>
<div id="outline-container-org6417dd5" class="outline-4">
<h4 id="org6417dd5">Cómo jugar en modo manual</h4>
<div class="outline-text-4" id="text-org6417dd5">
<ol class="org-ol">
<li>Presiona <code>M</code> en el menú para entrar en modo manual</li>
<li>El juego se reinicia y borra cualquier modelo anterior</li>
<li>Juega normalmente usando ESPACIO para saltar</li>
<li>El juego registra tus decisiones en cada frame donde hay una bala activa</li>
</ol>
</div>
</div>
<div id="outline-container-org863cdab" class="outline-4">
<h4 id="org863cdab">Consejos para recolectar buenos datos</h4>
<div class="outline-text-4" id="text-org863cdab">
<ul class="org-ul">
<li>Juega de forma natural, como lo harías normalmente</li>
<li>Mezcla situaciones: a veces salta temprano, a veces tarde, a veces no saltes</li>
<li>Juega varias partidas para tener más datos</li>
<li>Intenta tener al menos 80+ muestras antes de entrenar (verás el contador en el menú)</li>
</ul>
</div>
</div>
<div id="outline-container-orge0c2c55" class="outline-4">
<h4 id="orge0c2c55">Registro de datos</h4>
<div class="outline-text-4" id="text-orge0c2c55">
<p>
El juego registra datos en cada frame donde:
</p>
<ul class="org-ul">
<li>La bala está disparada</li>
<li>El jugador está en el suelo (para decisiones de salto)</li>
<li>O está en el aire (marcado como salto=1 durante todo el tiempo en el aire)</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgb0aac5a" class="outline-3">
<h3 id="orgb0aac5a">Entrenamiento del Modelo MLP</h3>
<div class="outline-text-3" id="text-orgb0aac5a">
</div>
<div id="outline-container-org0edd30d" class="outline-4">
<h4 id="org0edd30d">¿Qué es un MLP?</h4>
<div class="outline-text-4" id="text-org0edd30d">
<p>
MLP (Multi-Layer Perceptron) es un tipo de red neuronal artificial que:
</p>
<ul class="org-ul">
<li>Tiene capas ocultas entre la entrada y la salida</li>
<li>Puede aprender patrones no lineales</li>
<li>En este juego: aprende a predecir cuándo saltar basándose en velocidad y distancia</li>
</ul>
</div>
</div>
<div id="outline-container-org69f4002" class="outline-4">
<h4 id="org69f4002">Arquitectura del modelo</h4>
<div class="outline-text-4" id="text-org69f4002">
<p>
El MLP usado en este juego tiene:
</p>
<ul class="org-ul">
<li>Entrada: 2 características (velocidad<sub>bala</sub>, distancia)</li>
<li>Capas ocultas: (24, 12) neuronas</li>
<li>Salida: 1 neurona (probabilidad de salto: 0 o 1)</li>
<li>Activación: ReLU</li>
<li>Optimizador: Adam</li>
</ul>
</div>
</div>
<div id="outline-container-org98573b9" class="outline-4">
<h4 id="org98573b9">Cómo entrenar</h4>
<div class="outline-text-4" id="text-org98573b9">
<ol class="org-ol">
<li>Recolecta datos en modo manual (al menos 80 muestras)</li>
<li>Presiona <code>T</code> en el menú para entrenar</li>
<li>El modelo se entrena automáticamente:
<ul class="org-ul">
<li>Divide los datos en entrenamiento (80%) y prueba (20%)</li>
<li>Normaliza las características con StandardScaler</li>
<li>Entrena el MLP con hasta 3000 iteraciones</li>
</ul></li>
<li>Verás un mensaje con la precisión (accuracy) del modelo</li>
</ol>
</div>
</div>
<div id="outline-container-org5d637a9" class="outline-4">
<h4 id="org5d637a9">Casos especiales</h4>
<div class="outline-text-4" id="text-org5d637a9">
<ul class="org-ul">
<li>Si tienes menos de 80 muestras: el entrenamiento fallará con un mensaje</li>
<li>Si solo tienes una clase (solo 0s o solo 1s): se crea un "modelo trivial" que siempre devuelve esa clase</li>
<li>Para un modelo útil: necesitas datos con ambas clases (0 y 1)</li>
</ul>
</div>
</div>
<div id="outline-container-org90690ea" class="outline-4">
<h4 id="org90690ea">Interpretando los resultados</h4>
<div class="outline-text-4" id="text-org90690ea">
<ul class="org-ul">
<li>Accuracy ≈ 0.5-0.6: El modelo no está aprendiendo bien, necesitas más datos variados</li>
<li>Accuracy ≈ 0.7-0.8: El modelo está aprendiendo, pero puede mejorar</li>
<li>Accuracy ≈ 0.8-0.9: Buen modelo, está capturando tus patrones</li>
<li>Accuracy &gt; 0.9: Excelente, el modelo imita muy bien tu estilo</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgfdb8df4" class="outline-3">
<h3 id="orgfdb8df4">Modo Auto: El MLP Juega Solo</h3>
<div class="outline-text-3" id="text-orgfdb8df4">
</div>
<div id="outline-container-org341dc7d" class="outline-4">
<h4 id="org341dc7d">¿Qué hace el modo auto?</h4>
<div class="outline-text-4" id="text-org341dc7d">
<p>
En modo auto, el MLP toma las decisiones de salto basándose en lo que aprendió de ti.
</p>
</div>
</div>
<div id="outline-container-org8c29423" class="outline-4">
<h4 id="org8c29423">Cómo activar modo auto</h4>
<div class="outline-text-4" id="text-org8c29423">
<ol class="org-ol">
<li>Primero debes entrenar un modelo (presiona <code>T</code>)</li>
<li>Luego presiona <code>A</code> en el menú</li>
<li>El juego se reinicia y el MLP controla los saltos</li>
</ol>
</div>
</div>
<div id="outline-container-orgf3524bb" class="outline-4">
<h4 id="orgf3524bb">Visualización en tiempo real</h4>
<div class="outline-text-4" id="text-orgf3524bb">
<p>
Mientras juegas en modo auto, verás en la esquina superior izquierda:
</p>
<dl class="org-dl">
<dt><code>proba_salto≈0.XX</code></dt><dd>La probabilidad que el modelo calcula de que debería saltar</dd>
</dl>
</div>
</div>
<div id="outline-container-org051e40e" class="outline-4">
<h4 id="org051e40e">Limitaciones del modo auto</h4>
<div class="outline-text-4" id="text-org051e40e">
<ul class="org-ul">
<li>Si no hay modelo entrenado, el modo auto no saltará nunca</li>
<li>El modelo solo puede hacer lo que aprendió de tus datos</li>
<li>Si tus datos son inconsistentes, el modelo también lo será</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgd577421" class="outline-3">
<h3 id="orgd577421">Exportación y Visualización de Datos</h3>
<div class="outline-text-3" id="text-orgd577421">
</div>
<div id="outline-container-org53d6a76" class="outline-4">
<h4 id="org53d6a76">Exportar a CSV</h4>
<div class="outline-text-4" id="text-org53d6a76">
<ol class="org-ol">
<li>En el menú, presiona <code>C</code></li>
<li>Se crea/sobrescribe el archivo <code>datos_mlp.csv</code></li>
<li>El CSV contiene tres columnas:
<dl class="org-dl">
<dt><code>velocidad_bala</code></dt><dd>Velocidad de la bala (negativa, en píxeles/frame)</dd>
<dt><code>distancia</code></dt><dd>Distancia entre jugador y bala (en píxeles)</dd>
<dt><code>salto</code></dt><dd>1 si saltaste, 0 si no saltaste</dd>
</dl></li>
</ol>
</div>
</div>
<div id="outline-container-org53fa124" class="outline-4">
<h4 id="org53fa124">Visualizar los datos</h4>
<div class="outline-text-4" id="text-org53fa124">
<p>
Para ver gráficas de tus datos:
</p>

<div class="org-src-container">
<pre class="src src-bash">python juego_pygame_mlp_plot.py
</pre>
</div>

<p>
Esto abre dos ventanas:
</p>
<ul class="org-ul">
<li>Gráfica 2D: Distancia vs Velocidad (colores: rojo=salto, azul=no salto)</li>
<li>Gráfica 3D: Distancia, Velocidad y Clase (0 abajo, 1 arriba)</li>
</ul>
</div>
</div>
<div id="outline-container-org0d9ce84" class="outline-4">
<h4 id="org0d9ce84">Interpretando las gráficas</h4>
<div class="outline-text-4" id="text-org0d9ce84">
<ul class="org-ul">
<li>Si ves dos nubes bien separadas: tus datos son consistentes, el modelo debería aprender bien</li>
<li>Si las nubes están mezcladas: tus decisiones son más variadas, el modelo puede tener más dificultad</li>
<li>Si solo ves una nube: solo tienes una clase, necesitas más variedad en tus datos</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org136715c" class="outline-3">
<h3 id="org136715c">Arquitectura del Código</h3>
<div class="outline-text-3" id="text-org136715c">
</div>
<div id="outline-container-org029746d" class="outline-4">
<h4 id="org029746d">Estructura principal</h4>
<div class="outline-text-4" id="text-org029746d">
<p>
El código está organizado en una clase <code>Juego</code> con los siguientes módulos:
</p>

<dl class="org-dl">
<dt><code>__init__</code></dt><dd>Inicialización de pygame, ventana, estado del juego</dd>
<dt><code>_apply_resolution</code></dt><dd>Manejo de escalado y resolución</dd>
<dt><code>_cargar_assets</code></dt><dd>Carga de sprites e imágenes</dd>
<dt><code>_reset_estado_juego</code></dt><dd>Reinicia posiciones y estado</dd>
<dt><code>disparar_bala</code> / <code>reset_bala</code></dt><dd>Lógica de las balas</dd>
<dt><code>iniciar_salto</code> / <code>manejar_salto</code></dt><dd>Física del salto</dd>
<dt><code>registrar_decision_manual</code></dt><dd>Guarda datos mientras juegas</dd>
<dt><code>entrenar_modelo</code></dt><dd>Entrena el MLP con los datos recolectados</dd>
<dt><code>decision_auto_saltar</code></dt><dd>El MLP decide si saltar o no</dd>
<dt><code>mostrar_menu</code></dt><dd>Menú principal</dd>
<dt><code>loop</code></dt><dd>Bucle principal del juego</dd>
</dl>
</div>
</div>
<div id="outline-container-org83040c1" class="outline-4">
<h4 id="org83040c1">Clase Sample</h4>
<div class="outline-text-4" id="text-org83040c1">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #fabd2f;">@dataclass</span>
<span style="color: #fb4934;">class</span> <span style="color: #fabd2f;">Sample</span>:
<span style="background-color: #3c3836;"> </span>   velocidad_bala: <span style="color: #fe8019;">float</span>
<span style="background-color: #3c3836;"> </span>   distancia: <span style="color: #fe8019;">float</span>
<span style="background-color: #3c3836;"> </span>   salto: <span style="color: #fe8019;">int</span>  <span style="color: #928374;"># </span><span style="color: #928374;">1 si salt&#243; EN ESE FRAME, 0 si no</span>
</pre>
</div>

<p>
Cada muestra representa una decisión en un frame específico.
</p>
</div>
</div>
<div id="outline-container-org8e9a16b" class="outline-4">
<h4 id="org8e9a16b">Flujo de datos</h4>
<div class="outline-text-4" id="text-org8e9a16b">
<ol class="org-ol">
<li>Modo Manual:
<ul class="org-ul">
<li>Juegas → <code>registrar_decision_manual()</code> → <code>datos_modelo.append(Sample(...))</code></li>
</ul></li>

<li>Entrenamiento:
<ul class="org-ul">
<li><code>datos_modelo</code> → <code>entrenar_modelo()</code> → MLP entrenado guardado en <code>self.modelo</code></li>
</ul></li>

<li>Modo Auto:
<ul class="org-ul">
<li>Cada frame → <code>decision_auto_saltar()</code> → MLP predice → Salta o no salta</li>
</ul></li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org9ea266c" class="outline-3">
<h3 id="org9ea266c">Parámetros Configurables</h3>
<div class="outline-text-3" id="text-org9ea266c">
</div>
<div id="outline-container-org62b8f2d" class="outline-4">
<h4 id="org62b8f2d">Parámetros del juego</h4>
<div class="outline-text-4" id="text-org62b8f2d">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Variable</th>
<th scope="col" class="org-right">Valor</th>
<th scope="col" class="org-left">Descripción</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left"><code>BASE_W, BASE_H</code></td>
<td class="org-right">1080, 720</td>
<td class="org-left">Tamaño base de la ventana</td>
</tr>

<tr>
<td class="org-left"><code>velocidad_bala</code></td>
<td class="org-right">-12 a -6</td>
<td class="org-left">Rango de velocidad de las balas</td>
</tr>

<tr>
<td class="org-left"><code>salto_vel_inicial</code></td>
<td class="org-right">15.0</td>
<td class="org-left">Velocidad inicial del salto</td>
</tr>

<tr>
<td class="org-left"><code>gravedad</code></td>
<td class="org-right">1.0</td>
<td class="org-left">Fuerza de gravedad</td>
</tr>

<tr>
<td class="org-left"><code>fondo_speed</code></td>
<td class="org-right">3</td>
<td class="org-left">Velocidad de desplazamiento del fondo</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-org029443b" class="outline-4">
<h4 id="org029443b">Parámetros del MLP</h4>
<div class="outline-text-4" id="text-org029443b">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Variable</th>
<th scope="col" class="org-left">Valor</th>
<th scope="col" class="org-left">Descripción</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left"><code>hidden_layer_sizes</code></td>
<td class="org-left">(24, 12)</td>
<td class="org-left">Neuronas en cada capa oculta</td>
</tr>

<tr>
<td class="org-left"><code>activation</code></td>
<td class="org-left">"relu"</td>
<td class="org-left">Función de activación</td>
</tr>

<tr>
<td class="org-left"><code>solver</code></td>
<td class="org-left">"adam"</td>
<td class="org-left">Algoritmo de optimización</td>
</tr>

<tr>
<td class="org-left"><code>max_iter</code></td>
<td class="org-left">3000</td>
<td class="org-left">Máximo de iteraciones de entrenamiento</td>
</tr>

<tr>
<td class="org-left"><code>test_size</code></td>
<td class="org-left">0.2</td>
<td class="org-left">20% de los datos para prueba</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-org7c0e74f" class="outline-4">
<h4 id="org7c0e74f">Parámetros de registro</h4>
<div class="outline-text-4" id="text-org7c0e74f">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Variable</th>
<th scope="col" class="org-right">Valor</th>
<th scope="col" class="org-left">Descripción</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left"><code>decision_window</code></td>
<td class="org-right">500</td>
<td class="org-left">Ventana de decisión (ya no se usa, registra todo)</td>
</tr>

<tr>
<td class="org-left"><code>min_samples</code></td>
<td class="org-right">80</td>
<td class="org-left">Mínimo de muestras para entrenar</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="outline-container-org2bca2c8" class="outline-3">
<h3 id="org2bca2c8">Mejores Prácticas</h3>
<div class="outline-text-3" id="text-org2bca2c8">
</div>
<div id="outline-container-org71ee0b1" class="outline-4">
<h4 id="org71ee0b1">Recolectando datos de calidad</h4>
<div class="outline-text-4" id="text-org71ee0b1">
<ul class="org-ul">
<li>Juega varias partidas (no solo una)</li>
<li>Varía tu estilo: a veces conservador, a veces agresivo</li>
<li>Asegúrate de tener ejemplos de ambas clases (saltos y no-saltos)</li>
<li>Recolecta al menos 200-300 muestras para un modelo robusto</li>
</ul>
</div>
</div>
<div id="outline-container-org6cef365" class="outline-4">
<h4 id="org6cef365">Entrenando el modelo</h4>
<div class="outline-text-4" id="text-org6cef365">
<ul class="org-ul">
<li>Siempre revisa el accuracy: si es muy bajo (&lt;0.6), recolecta más datos</li>
<li>Si el accuracy es perfecto (1.0), puede ser sobreajuste: prueba con más datos de prueba</li>
<li>Exporta el CSV y visualiza los datos para ver si hay patrones claros</li>
</ul>
</div>
</div>
<div id="outline-container-org3f53237" class="outline-4">
<h4 id="org3f53237">Jugando en modo auto</h4>
<div class="outline-text-4" id="text-org3f53237">
<ul class="org-ul">
<li>Observa la probabilidad en tiempo real (<code>proba_salto≈XX</code>)</li>
<li>Si el modelo nunca salta o siempre salta, revisa tus datos</li>
<li>Compara el comportamiento del modelo con cómo tú jugarías</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org74204b1" class="outline-3">
<h3 id="org74204b1">Solución de Problemas</h3>
<div class="outline-text-3" id="text-org74204b1">
</div>
<div id="outline-container-org07f654e" class="outline-4">
<h4 id="org07f654e">El modelo no aprende (accuracy &lt; 0.5)</h4>
<div class="outline-text-4" id="text-org07f654e">
<ul class="org-ul">
<li>Posibles causas:
<ul class="org-ul">
<li>Datos insuficientes (&lt; 80 muestras)</li>
<li>Solo una clase en los datos (solo 0s o solo 1s)</li>
<li>Datos muy inconsistentes o aleatorios</li>
</ul></li>
<li>Solución:
<ul class="org-ul">
<li>Recolecta más datos variados</li>
<li>Asegúrate de tener ejemplos de ambas clases</li>
<li>Juega de forma más consistente</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org07d4d2a" class="outline-4">
<h4 id="org07d4d2a">El modo auto no salta nunca</h4>
<div class="outline-text-4" id="text-org07d4d2a">
<ul class="org-ul">
<li>Verifica que el modelo esté entrenado (<code>Modelo: sí</code> en el menú)</li>
<li>Si el modelo es trivial de clase 0, siempre dirá "no saltar"</li>
<li>Solución: recolecta datos donde sí saltas y reentrena</li>
</ul>
</div>
</div>
<div id="outline-container-org8a5193f" class="outline-4">
<h4 id="org8a5193f">El modo auto salta demasiado</h4>
<div class="outline-text-4" id="text-org8a5193f">
<ul class="org-ul">
<li>El modelo aprendió que siempre debes saltar</li>
<li>Solución: recolecta más datos donde NO saltas y reentrena</li>
</ul>
</div>
</div>
<div id="outline-container-orgcb14bb0" class="outline-4">
<h4 id="orgcb14bb0">Las gráficas no se muestran</h4>
<div class="outline-text-4" id="text-orgcb14bb0">
<ul class="org-ul">
<li>Verifica que tengas <code>matplotlib</code> instalado</li>
<li>Verifica que el CSV tenga datos válidos</li>
<li>Ejecuta <code>juego_pygame_mlp_plot.py</code> desde la terminal</li>
</ul>
</div>
</div>
<div id="outline-container-org3fae388" class="outline-4">
<h4 id="org3fae388">El juego se ve lento o con lag</h4>
<div class="outline-text-4" id="text-org3fae388">
<ul class="org-ul">
<li>Reduce los FPS en <code>reloj.tick(45)</code> a un valor menor (ej: 30)</li>
<li>Cierra otras aplicaciones que consuman recursos</li>
<li>Verifica que tu sistema tenga aceleración gráfica habilitada</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgc1177cc" class="outline-3">
<h3 id="orgc1177cc">Conceptos Técnicos Avanzados</h3>
<div class="outline-text-3" id="text-orgc1177cc">
</div>
<div id="outline-container-orgc85aeb9" class="outline-4">
<h4 id="orgc85aeb9">¿Por qué MLP y no otro algoritmo?</h4>
<div class="outline-text-4" id="text-orgc85aeb9">
<ul class="org-ul">
<li>MLP puede aprender patrones no lineales (la relación velocidad-distancia-salto no es lineal)</li>
<li>Es relativamente simple y rápido de entrenar</li>
<li>Funciona bien con pocas características (solo 2 en este caso)</li>
</ul>
</div>
</div>
<div id="outline-container-org77f8267" class="outline-4">
<h4 id="org77f8267">Normalización (StandardScaler)</h4>
<div class="outline-text-4" id="text-org77f8267">
<p>
Las características se normalizan porque:
</p>
<ul class="org-ul">
<li>La velocidad de la bala y la distancia tienen escalas muy diferentes</li>
<li>El MLP funciona mejor con datos normalizados</li>
<li>Ayuda a que el entrenamiento converja más rápido</li>
</ul>
</div>
</div>
<div id="outline-container-orgaaed977" class="outline-4">
<h4 id="orgaaed977">División train/test</h4>
<div class="outline-text-4" id="text-orgaaed977">
<ul class="org-ul">
<li>80% entrenamiento, 20% prueba</li>
<li>Estratificado (<code>stratify=y</code>) para mantener proporción de clases</li>
<li>El accuracy en test es más confiable que en entrenamiento</li>
</ul>
</div>
</div>
<div id="outline-container-org5eac6e7" class="outline-4">
<h4 id="org5eac6e7">Overfitting y Underfitting</h4>
<div class="outline-text-4" id="text-org5eac6e7">
<ul class="org-ul">
<li>Overfitting: El modelo memoriza los datos de entrenamiento pero falla en nuevos datos</li>
<li>Underfitting: El modelo no aprende lo suficiente</li>
<li>En este juego: si accuracy test &lt;&lt; accuracy train → overfitting</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgad1fb95" class="outline-3">
<h3 id="orgad1fb95">Código de Ejemplo</h3>
<div class="outline-text-3" id="text-orgad1fb95">
</div>
<div id="outline-container-orgdfc45b6" class="outline-4">
<h4 id="orgdfc45b6">Cargar y visualizar datos manualmente</h4>
<div class="outline-text-4" id="text-orgdfc45b6">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #fb4934;">import</span> csv
<span style="color: #fb4934;">import</span> matplotlib.pyplot <span style="color: #fb4934;">as</span> plt

<span style="color: #928374;"># </span><span style="color: #928374;">Cargar CSV
</span><span style="color: #83a598;">xs</span>, <span style="color: #83a598;">ys</span>, <span style="color: #83a598;">cs</span> = <span style="color: #fe8019;">[]</span>, <span style="color: #fe8019;">[]</span>, <span style="color: #fe8019;">[]</span>
<span style="color: #fb4934;">with</span> <span style="color: #fe8019;">open</span><span style="color: #fe8019;">(</span><span style="color: #b8bb26;">"datos_mlp.csv"</span>, <span style="color: #b8bb26;">"r"</span><span style="color: #fe8019;">)</span> <span style="color: #fb4934;">as</span> f:
<span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">reader</span> = csv.DictReader<span style="color: #fe8019;">(</span>f<span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   <span style="color: #fb4934;">for</span> row <span style="color: #fb4934;">in</span> reader:
<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   xs.append<span style="color: #fe8019;">(</span><span style="color: #fe8019;">float</span><span style="color: #b16286;">(</span>row<span style="color: #b8bb26;">[</span><span style="color: #b8bb26;">"distancia"</span><span style="color: #b8bb26;">]</span><span style="color: #b16286;">)</span><span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   ys.append<span style="color: #fe8019;">(</span><span style="color: #fe8019;">float</span><span style="color: #b16286;">(</span>row<span style="color: #b8bb26;">[</span><span style="color: #b8bb26;">"velocidad_bala"</span><span style="color: #b8bb26;">]</span><span style="color: #b16286;">)</span><span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   cs.append<span style="color: #fe8019;">(</span><span style="color: #fe8019;">int</span><span style="color: #b16286;">(</span>row<span style="color: #b8bb26;">[</span><span style="color: #b8bb26;">"salto"</span><span style="color: #b8bb26;">]</span><span style="color: #b16286;">)</span><span style="color: #fe8019;">)</span>

<span style="color: #928374;"># </span><span style="color: #928374;">Separar por clases
</span><span style="color: #83a598;">xs_0</span> = <span style="color: #fe8019;">[</span>x <span style="color: #fb4934;">for</span> x, c <span style="color: #fb4934;">in</span> <span style="color: #fe8019;">zip</span><span style="color: #b16286;">(</span>xs, cs<span style="color: #b16286;">)</span> <span style="color: #fb4934;">if</span> c == 0<span style="color: #fe8019;">]</span>
<span style="color: #83a598;">ys_0</span> = <span style="color: #fe8019;">[</span>y <span style="color: #fb4934;">for</span> y, c <span style="color: #fb4934;">in</span> <span style="color: #fe8019;">zip</span><span style="color: #b16286;">(</span>ys, cs<span style="color: #b16286;">)</span> <span style="color: #fb4934;">if</span> c == 0<span style="color: #fe8019;">]</span>
<span style="color: #83a598;">xs_1</span> = <span style="color: #fe8019;">[</span>x <span style="color: #fb4934;">for</span> x, c <span style="color: #fb4934;">in</span> <span style="color: #fe8019;">zip</span><span style="color: #b16286;">(</span>xs, cs<span style="color: #b16286;">)</span> <span style="color: #fb4934;">if</span> c == 1<span style="color: #fe8019;">]</span>
<span style="color: #83a598;">ys_1</span> = <span style="color: #fe8019;">[</span>y <span style="color: #fb4934;">for</span> y, c <span style="color: #fb4934;">in</span> <span style="color: #fe8019;">zip</span><span style="color: #b16286;">(</span>ys, cs<span style="color: #b16286;">)</span> <span style="color: #fb4934;">if</span> c == 1<span style="color: #fe8019;">]</span>

<span style="color: #928374;"># </span><span style="color: #928374;">Graficar
</span>plt.scatter<span style="color: #fe8019;">(</span>xs_0, ys_0, c=<span style="color: #b8bb26;">"blue"</span>, label=<span style="color: #b8bb26;">"No salto"</span><span style="color: #fe8019;">)</span>
plt.scatter<span style="color: #fe8019;">(</span>xs_1, ys_1, c=<span style="color: #b8bb26;">"red"</span>, label=<span style="color: #b8bb26;">"Salto"</span><span style="color: #fe8019;">)</span>
plt.xlabel<span style="color: #fe8019;">(</span><span style="color: #b8bb26;">"Distancia"</span><span style="color: #fe8019;">)</span>
plt.ylabel<span style="color: #fe8019;">(</span><span style="color: #b8bb26;">"Velocidad"</span><span style="color: #fe8019;">)</span>
plt.legend<span style="color: #fe8019;">()</span>
plt.show<span style="color: #fe8019;">()</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-org2fbd753" class="outline-4">
<h4 id="org2fbd753">Entrenar modelo desde CSV</h4>
<div class="outline-text-4" id="text-org2fbd753">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #fb4934;">from</span> sklearn.neural_network <span style="color: #fb4934;">import</span> MLPClassifier
<span style="color: #fb4934;">from</span> sklearn.preprocessing <span style="color: #fb4934;">import</span> StandardScaler
<span style="color: #fb4934;">import</span> csv

<span style="color: #928374;"># </span><span style="color: #928374;">Cargar datos
</span><span style="color: #83a598;">X</span>, <span style="color: #83a598;">y</span> = <span style="color: #fe8019;">[]</span>, <span style="color: #fe8019;">[]</span>
<span style="color: #fb4934;">with</span> <span style="color: #fe8019;">open</span><span style="color: #fe8019;">(</span><span style="color: #b8bb26;">"datos_mlp.csv"</span>, <span style="color: #b8bb26;">"r"</span><span style="color: #fe8019;">)</span> <span style="color: #fb4934;">as</span> f:
<span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">reader</span> = csv.DictReader<span style="color: #fe8019;">(</span>f<span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   <span style="color: #fb4934;">for</span> row <span style="color: #fb4934;">in</span> reader:
<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   X.append<span style="color: #fe8019;">(</span><span style="color: #b16286;">[</span><span style="color: #fe8019;">float</span><span style="color: #b8bb26;">(</span>row<span style="color: #83a598;">[</span><span style="color: #b8bb26;">"velocidad_bala"</span><span style="color: #83a598;">]</span><span style="color: #b8bb26;">)</span>, <span style="color: #fe8019;">float</span><span style="color: #b8bb26;">(</span>row<span style="color: #83a598;">[</span><span style="color: #b8bb26;">"distancia"</span><span style="color: #83a598;">]</span><span style="color: #b8bb26;">)</span><span style="color: #b16286;">]</span><span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   y.append<span style="color: #fe8019;">(</span><span style="color: #fe8019;">int</span><span style="color: #b16286;">(</span>row<span style="color: #b8bb26;">[</span><span style="color: #b8bb26;">"salto"</span><span style="color: #b8bb26;">]</span><span style="color: #b16286;">)</span><span style="color: #fe8019;">)</span>

<span style="color: #928374;"># </span><span style="color: #928374;">Normalizar
</span><span style="color: #83a598;">scaler</span> = StandardScaler<span style="color: #fe8019;">()</span>
<span style="color: #83a598;">X_scaled</span> = scaler.fit_transform<span style="color: #fe8019;">(</span>X<span style="color: #fe8019;">)</span>

<span style="color: #928374;"># </span><span style="color: #928374;">Entrenar
</span><span style="color: #83a598;">modelo</span> = MLPClassifier<span style="color: #fe8019;">(</span>hidden_layer_sizes=<span style="color: #b16286;">(</span>24, 12<span style="color: #b16286;">)</span>, max_iter=3000<span style="color: #fe8019;">)</span>
modelo.fit<span style="color: #fe8019;">(</span>X_scaled, y<span style="color: #fe8019;">)</span>

<span style="color: #928374;"># </span><span style="color: #928374;">Predecir
</span><span style="color: #83a598;">nueva_velocidad</span> = -8.5
<span style="color: #83a598;">nueva_distancia</span> = 250.0
<span style="color: #83a598;">X_nuevo</span> = scaler.transform<span style="color: #fe8019;">(</span><span style="color: #b16286;">[</span><span style="color: #b8bb26;">[</span>nueva_velocidad, nueva_distancia<span style="color: #b8bb26;">]</span><span style="color: #b16286;">]</span><span style="color: #fe8019;">)</span>
<span style="color: #83a598;">probabilidad</span> = modelo.predict_proba<span style="color: #fe8019;">(</span>X_nuevo<span style="color: #fe8019;">)[</span>0<span style="color: #fe8019;">][</span>1<span style="color: #fe8019;">]</span>
<span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>f<span style="color: #b8bb26;">"Probabilidad de salto: </span>{probabilidad:.2f}<span style="color: #b8bb26;">"</span><span style="color: #fe8019;">)</span>
</pre>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-orge463798" class="outline-2">
<h2 id="orge463798">Métricas de Evaluación para Machine Learning</h2>
<div class="outline-text-2" id="text-orge463798">
</div>
<div id="outline-container-orgce3045e" class="outline-3">
<h3 id="orgce3045e">Introducción</h3>
<div class="outline-text-3" id="text-orgce3045e">
</div>
<div id="outline-container-orgbe6518b" class="outline-4">
<h4 id="orgbe6518b">¿Qué son las métricas de evaluación?</h4>
<div class="outline-text-4" id="text-orgbe6518b">
<p>
Las métricas de evaluación son medidas numéricas que nos permiten cuantificar qué tan bien está funcionando un modelo de Machine Learning. Nos ayudan a:
</p>
<ul class="org-ul">
<li>Comparar diferentes modelos</li>
<li>Entender las fortalezas y debilidades del modelo</li>
<li>Decidir si un modelo es útil para nuestro problema</li>
<li>Detectar problemas como sobreajuste o sesgo</li>
</ul>
</div>
</div>
<div id="outline-container-org57d4141" class="outline-4">
<h4 id="org57d4141">¿Por qué son importantes?</h4>
<div class="outline-text-4" id="text-org57d4141">
<p>
Un modelo puede tener un accuracy alto pero aún así tener problemas graves. Por ejemplo:
</p>
<ul class="org-ul">
<li>Un modelo que siempre predice "no salto" tendría 90% accuracy si el 90% de los casos son "no salto"</li>
<li>Pero sería inútil porque nunca saltaría cuando debería</li>
</ul>

<p>
Las métricas nos ayudan a detectar estos problemas.
</p>
</div>
</div>
</div>
<div id="outline-container-org81fb02d" class="outline-3">
<h3 id="org81fb02d">Matriz de Confusión: La Base de Todo</h3>
<div class="outline-text-3" id="text-org81fb02d">
</div>
<div id="outline-container-org31c9847" class="outline-4">
<h4 id="org31c9847">¿Qué es una matriz de confusión?</h4>
<div class="outline-text-4" id="text-org31c9847">
<p>
La matriz de confusión es una tabla que muestra cómo se clasificaron las predicciones del modelo comparadas con los valores reales.
</p>
</div>
</div>
<div id="outline-container-orgeb2ab39" class="outline-4">
<h4 id="orgeb2ab39">Estructura de la matriz (2 clases)</h4>
<div class="outline-text-4" id="text-orgeb2ab39">
<p>
Para un problema de clasificación binaria (como nuestro juego: salto=1, no salto=0):
</p>

<pre class="example" id="orgd21f4e1">
                    Predicción
                 No Salto  Salto
Real  No Salto     TN      FP
      Salto        FN      TP
</pre>

<p>
Donde:
</p>
<ul class="org-ul">
<li><code>TP</code> (True Positive): Predijo salto y era salto ✓</li>
<li><code>TN</code> (True Negative): Predijo no salto y era no salto ✓</li>
<li><code>FP</code> (False Positive): Predijo salto pero era no salto ✗</li>
<li><code>FN</code> (False Negative): Predijo no salto pero era salto ✗</li>
</ul>
</div>
</div>
<div id="outline-container-org07177f8" class="outline-4">
<h4 id="org07177f8">Interpretación en el contexto del juego</h4>
<div class="outline-text-4" id="text-org07177f8">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Caso</th>
<th scope="col" class="org-left">Significado en el juego</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">TP</td>
<td class="org-left">El modelo saltó y era correcto hacerlo</td>
</tr>

<tr>
<td class="org-left">TN</td>
<td class="org-left">El modelo no saltó y era correcto no saltar</td>
</tr>

<tr>
<td class="org-left">FP</td>
<td class="org-left">El modelo saltó innecesariamente (gasto de salto)</td>
</tr>

<tr>
<td class="org-left">FN</td>
<td class="org-left">El modelo NO saltó cuando debería haber saltado (¡colisión!)</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-org69d5d15" class="outline-4">
<h4 id="org69d5d15">Ejemplo práctico</h4>
<div class="outline-text-4" id="text-org69d5d15">
<p>
Supongamos que el modelo hizo 100 predicciones:
</p>

<pre class="example" id="org369232e">
                    Predicción
                 No Salto  Salto  Total
Real  No Salto      70      10     80
      Salto          5      15     20
      Total          75      25    100
</pre>

<ul class="org-ul">
<li>TP = 15 (saltó correctamente 15 veces)</li>
<li>TN = 70 (no saltó correctamente 70 veces)</li>
<li>FP = 10 (saltó innecesariamente 10 veces)</li>
<li>FN = 5 (no saltó cuando debía 5 veces → ¡5 colisiones!)</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgc95aba1" class="outline-3">
<h3 id="orgc95aba1">Accuracy (Precisión General)</h3>
<div class="outline-text-3" id="text-orgc95aba1">
</div>
<div id="outline-container-org998cd0d" class="outline-4">
<h4 id="org998cd0d">Definición</h4>
<div class="outline-text-4" id="text-org998cd0d">
<p>
Accuracy mide la proporción de predicciones correctas sobre el total:
</p>

<p>
\[
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} = \frac{\text{Predicciones correctas}}{\text{Total de predicciones}}
\]
</p>
</div>
</div>
<div id="outline-container-org2fb3879" class="outline-4">
<h4 id="org2fb3879">Cálculo del ejemplo anterior</h4>
<div class="outline-text-4" id="text-org2fb3879">
<p>
\[
\text{Accuracy} = \frac{15 + 70}{100} = \frac{85}{100} = 0.85 = 85\%
\]
</p>
</div>
</div>
<div id="outline-container-org4f9491c" class="outline-4">
<h4 id="org4f9491c">Ventajas</h4>
<div class="outline-text-4" id="text-org4f9491c">
<ul class="org-ul">
<li>Fácil de entender: "¿Qué porcentaje acertó?"</li>
<li>Útil cuando las clases están balanceadas</li>
<li>Buena métrica general para comparar modelos</li>
</ul>
</div>
</div>
<div id="outline-container-org7e04e13" class="outline-4">
<h4 id="org7e04e13">Desventajas</h4>
<div class="outline-text-4" id="text-org7e04e13">
<ul class="org-ul">
<li>Puede ser engañosa con clases desbalanceadas</li>
<li>No distingue entre tipos de errores</li>
<li>Un modelo que siempre predice la clase mayoritaria puede tener buen accuracy</li>
</ul>
</div>
</div>
<div id="outline-container-orgef4fa6a" class="outline-4">
<h4 id="orgef4fa6a">Ejemplo del problema</h4>
<div class="outline-text-4" id="text-orgef4fa6a">
<p>
Si en el juego el 90% de los casos son "no salto":
</p>
<ul class="org-ul">
<li>Un modelo que siempre dice "no salto" tendría 90% accuracy</li>
<li>Pero sería inútil porque nunca saltaría</li>
<li>Accuracy alto ≠ modelo bueno</li>
</ul>
</div>
</div>
<div id="outline-container-orgb5f218c" class="outline-4">
<h4 id="orgb5f218c">Cuándo usar Accuracy</h4>
<div class="outline-text-4" id="text-orgb5f218c">
<ul class="org-ul">
<li>Clases balanceadas (50-50 o similar)</li>
<li>Todos los errores tienen el mismo costo</li>
<li>Quieres una métrica simple y general</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgd2a05c0" class="outline-3">
<h3 id="orgd2a05c0">Precision (Precisión)</h3>
<div class="outline-text-3" id="text-orgd2a05c0">
</div>
<div id="outline-container-orgf842d4a" class="outline-4">
<h4 id="orgf842d4a">Definición</h4>
<div class="outline-text-4" id="text-orgf842d4a">
<p>
Precision mide: "De todas las veces que predije salto, ¿cuántas eran correctas?"
</p>

<p>
\[
\text{Precision} = \frac{TP}{TP + FP} = \frac{\text{Verdaderos positivos}}{\text{Todos los positivos predichos}}
\]
</p>
</div>
</div>
<div id="outline-container-org233c836" class="outline-4">
<h4 id="org233c836">Cálculo del ejemplo</h4>
<div class="outline-text-4" id="text-org233c836">
<p>
\[
\text{Precision} = \frac{15}{15 + 10} = \frac{15}{25} = 0.60 = 60\%
\]
</p>
</div>
</div>
<div id="outline-container-orgee9e20b" class="outline-4">
<h4 id="orgee9e20b">Interpretación</h4>
<div class="outline-text-4" id="text-orgee9e20b">
<ul class="org-ul">
<li>De las 25 veces que el modelo decidió saltar, solo 15 eran correctas</li>
<li>10 veces saltó innecesariamente (FP)</li>
<li>"Cuando el modelo dice que hay que saltar, ¿qué tan confiable es?"</li>
</ul>
</div>
</div>
<div id="outline-container-orga9cf908" class="outline-4">
<h4 id="orga9cf908">En el contexto del juego</h4>
<div class="outline-text-4" id="text-orga9cf908">
<ul class="org-ul">
<li>Precision alta: Cuando el modelo decide saltar, casi siempre es correcto</li>
<li>Precision baja: El modelo salta demasiado, muchas veces innecesariamente</li>
<li>Costo: Saltos innecesarios no son críticos, pero desperdician recursos</li>
</ul>
</div>
</div>
<div id="outline-container-org8974ee7" class="outline-4">
<h4 id="org8974ee7">Cuándo es importante</h4>
<div class="outline-text-4" id="text-org8974ee7">
<ul class="org-ul">
<li>Los falsos positivos son costosos</li>
<li>Quieres minimizar acciones innecesarias</li>
<li>Ejemplo: Sistema de spam (no quieres marcar emails legítimos como spam)</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org3aad789" class="outline-3">
<h3 id="org3aad789">Recall (Sensibilidad o Exhaustividad)</h3>
<div class="outline-text-3" id="text-org3aad789">
</div>
<div id="outline-container-org2d64326" class="outline-4">
<h4 id="org2d64326">Definición</h4>
<div class="outline-text-4" id="text-org2d64326">
<p>
Recall mide: "De todos los casos donde realmente debía saltar, ¿cuántos detecté?"
</p>

<p>
\[
\text{Recall} = \frac{TP}{TP + FN} = \frac{\text{Verdaderos positivos}}{\text{Todos los positivos reales}}
\]
</p>
</div>
</div>
<div id="outline-container-orga674d11" class="outline-4">
<h4 id="orga674d11">Cálculo del ejemplo</h4>
<div class="outline-text-4" id="text-orga674d11">
<p>
\[
\text{Recall} = \frac{15}{15 + 5} = \frac{15}{20} = 0.75 = 75\%
\]
</p>
</div>
</div>
<div id="outline-container-org020cf8d" class="outline-4">
<h4 id="org020cf8d">Interpretación</h4>
<div class="outline-text-4" id="text-org020cf8d">
<ul class="org-ul">
<li>De las 20 veces que realmente debía saltar, el modelo detectó 15</li>
<li>5 veces no saltó cuando debía (FN) → ¡5 colisiones!</li>
<li>"¿Qué porcentaje de las situaciones peligrosas detecta el modelo?"</li>
</ul>
</div>
</div>
<div id="outline-container-org079b905" class="outline-4">
<h4 id="org079b905">En el contexto del juego</h4>
<div class="outline-text-4" id="text-org079b905">
<ul class="org-ul">
<li>Recall alto: El modelo detecta casi todas las situaciones donde debe saltar</li>
<li>Recall bajo: El modelo se pierde muchas situaciones peligrosas → muchas colisiones</li>
<li>Costo: Falsos negativos son MUY costosos (colisiones = perder)</li>
</ul>
</div>
</div>
<div id="outline-container-org0a9335e" class="outline-4">
<h4 id="org0a9335e">Cuándo es crítico</h4>
<div class="outline-text-4" id="text-org0a9335e">
<ul class="org-ul">
<li>Los falsos negativos son muy costosos</li>
<li>Es mejor "sobre-detectar" que "sub-detectar"</li>
<li>Ejemplos: Detección de cáncer, detección de fraude, nuestro juego</li>
</ul>
</div>
</div>
<div id="outline-container-org7dc8878" class="outline-4">
<h4 id="org7dc8878">Trade-off Precision vs Recall</h4>
<div class="outline-text-4" id="text-org7dc8878">
<p>
Generalmente hay un trade-off:
</p>
<ul class="org-ul">
<li>Si aumentas el umbral de decisión → más Precision, menos Recall</li>
<li>Si disminuyes el umbral → más Recall, menos Precision</li>
</ul>

<p>
En el juego:
</p>
<ul class="org-ul">
<li>Umbral alto (0.7): Solo salta cuando está muy seguro → alta Precision, bajo Recall</li>
<li>Umbral bajo (0.3): Salta con más frecuencia → bajo Precision, alto Recall</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orge49b723" class="outline-3">
<h3 id="orge49b723">F1-Score: El Balance Perfecto</h3>
<div class="outline-text-3" id="text-orge49b723">
</div>
<div id="outline-container-org6936d67" class="outline-4">
<h4 id="org6936d67">Definición</h4>
<div class="outline-text-4" id="text-org6936d67">
<p>
F1-Score es la media armónica de Precision y Recall:
</p>

<p>
\[
F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2 \times TP}{2 \times TP + FP + FN}
\]
</p>
</div>
</div>
<div id="outline-container-org186e411" class="outline-4">
<h4 id="org186e411">Cálculo del ejemplo</h4>
<div class="outline-text-4" id="text-org186e411">
<p>
\[
F1 = 2 \times \frac{0.60 \times 0.75}{0.60 + 0.75} = 2 \times \frac{0.45}{1.35} = 0.667 = 66.7\%
\]
</p>
</div>
</div>
<div id="outline-container-org069f357" class="outline-4">
<h4 id="org069f357">¿Por qué media armónica y no aritmética?</h4>
<div class="outline-text-4" id="text-org069f357">
<p>
La media armónica penaliza más cuando una métrica es muy baja:
</p>
<ul class="org-ul">
<li>Si Precision = 0.1 y Recall = 0.9 → F1 = 0.18 (bajo)</li>
<li>Si Precision = 0.5 y Recall = 0.5 → F1 = 0.5 (mejor balance)</li>
</ul>
</div>
</div>
<div id="outline-container-org9c88e78" class="outline-4">
<h4 id="org9c88e78">Ventajas</h4>
<div class="outline-text-4" id="text-org9c88e78">
<ul class="org-ul">
<li>Balancea Precision y Recall</li>
<li>Útil cuando necesitas ambas métricas</li>
<li>No se ve afectado por clases desbalanceadas tanto como Accuracy</li>
</ul>
</div>
</div>
<div id="outline-container-orgdb9d64e" class="outline-4">
<h4 id="orgdb9d64e">Desventajas</h4>
<div class="outline-text-4" id="text-orgdb9d64e">
<ul class="org-ul">
<li>Puede ocultar problemas si una métrica es muy baja</li>
<li>No siempre es la métrica más importante</li>
</ul>
</div>
</div>
<div id="outline-container-org12c569a" class="outline-4">
<h4 id="org12c569a">Cuándo usar F1-Score</h4>
<div class="outline-text-4" id="text-org12c569a">
<ul class="org-ul">
<li>Necesitas balance entre Precision y Recall</li>
<li>Las clases están desbalanceadas</li>
<li>Quieres una métrica única que considere ambos aspectos</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgae3be4e" class="outline-3">
<h3 id="orgae3be4e">Especificidad</h3>
<div class="outline-text-3" id="text-orgae3be4e">
</div>
<div id="outline-container-org9e0112a" class="outline-4">
<h4 id="org9e0112a">Definición</h4>
<div class="outline-text-4" id="text-org9e0112a">
<p>
Especificidad mide: "De todos los casos donde NO debía saltar, ¿cuántos detecté correctamente?"
</p>

<p>
\[
\text{Especificidad} = \frac{TN}{TN + FP} = \frac{\text{Verdaderos negativos}}{\text{Todos los negativos reales}}
\]
</p>
</div>
</div>
<div id="outline-container-org11755b6" class="outline-4">
<h4 id="org11755b6">Cálculo del ejemplo</h4>
<div class="outline-text-4" id="text-org11755b6">
<p>
\[
\text{Especificidad} = \frac{70}{70 + 10} = \frac{70}{80} = 0.875 = 87.5\%
\]
</p>
</div>
</div>
<div id="outline-container-org37ee1b6" class="outline-4">
<h4 id="org37ee1b6">Interpretación</h4>
<div class="outline-text-4" id="text-org37ee1b6">
<ul class="org-ul">
<li>De las 80 veces que NO debía saltar, el modelo acertó 70</li>
<li>10 veces saltó innecesariamente</li>
<li>Es el "Recall de la clase negativa"</li>
</ul>
</div>
</div>
<div id="outline-container-org4484503" class="outline-4">
<h4 id="org4484503">En el contexto del juego</h4>
<div class="outline-text-4" id="text-org4484503">
<ul class="org-ul">
<li>Especificidad alta: El modelo rara vez salta innecesariamente</li>
<li>Especificidad baja: El modelo salta demasiado cuando no es necesario</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgb716a39" class="outline-3">
<h3 id="orgb716a39">Comparación de Métricas: Tabla Resumen</h3>
<div class="outline-text-3" id="text-orgb716a39">
</div>
<div id="outline-container-orgf42b41c" class="outline-4">
<h4 id="orgf42b41c">Valores del ejemplo</h4>
<div class="outline-text-4" id="text-orgf42b41c">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Métrica</th>
<th scope="col" class="org-right">Valor</th>
<th scope="col" class="org-left">Interpretación</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Accuracy</td>
<td class="org-right">85%</td>
<td class="org-left">85 de cada 100 predicciones fueron correctas</td>
</tr>

<tr>
<td class="org-left">Precision</td>
<td class="org-right">60%</td>
<td class="org-left">De cada 10 saltos predichos, 6 eran correctos</td>
</tr>

<tr>
<td class="org-left">Recall</td>
<td class="org-right">75%</td>
<td class="org-left">De cada 10 situaciones peligrosas, detectó 7.5</td>
</tr>

<tr>
<td class="org-left">F1-Score</td>
<td class="org-right">66.7%</td>
<td class="org-left">Balance entre Precision y Recall</td>
</tr>

<tr>
<td class="org-left">Especificidad</td>
<td class="org-right">87.5%</td>
<td class="org-left">De cada 10 casos seguros, detectó 8.75 correctamente</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-org0424c05" class="outline-4">
<h4 id="org0424c05">¿Qué métrica usar?</h4>
<div class="outline-text-4" id="text-org0424c05">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Situación</th>
<th scope="col" class="org-left">Métrica recomendada</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Clases balanceadas, errores igual costo</td>
<td class="org-left">Accuracy</td>
</tr>

<tr>
<td class="org-left">Falsos positivos muy costosos</td>
<td class="org-left">Precision</td>
</tr>

<tr>
<td class="org-left">Falsos negativos muy costosos</td>
<td class="org-left">Recall</td>
</tr>

<tr>
<td class="org-left">Necesitas balance Precision/Recall</td>
<td class="org-left">F1-Score</td>
</tr>

<tr>
<td class="org-left">Clases muy desbalanceadas</td>
<td class="org-left">F1-Score o Recall</td>
</tr>

<tr>
<td class="org-left">Quieres ver todo el panorama</td>
<td class="org-left">Matriz de confusión + todas</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="outline-container-org9d310ac" class="outline-3">
<h3 id="org9d310ac">Casos de Uso en el Juego</h3>
<div class="outline-text-3" id="text-org9d310ac">
</div>
<div id="outline-container-orga5a07f6" class="outline-4">
<h4 id="orga5a07f6">Escenario 1: Modelo Conservador</h4>
<div class="outline-text-4" id="text-orga5a07f6">
<p>
Un modelo que casi nunca salta:
</p>

<pre class="example" id="org1ff32d6">
                    Predicción
                 No Salto  Salto
Real  No Salto      75       5
      Salto         15       5
</pre>

<ul class="org-ul">
<li>Accuracy: 80% (80/100)</li>
<li>Precision: 50% (5/10) - Cuando salta, acierta la mitad</li>
<li>Recall: 25% (5/20) - Solo detecta 1 de cada 4 peligros → ¡MUCHAS COLISIONES!</li>
<li>F1: 33.3%</li>
</ul>

<p>
<b><b>Veredicto</b></b>: Mal modelo para el juego. Aunque tiene buen accuracy, tiene muy bajo Recall → muchas colisiones.
</p>
</div>
</div>
<div id="outline-container-orgeb6d638" class="outline-4">
<h4 id="orgeb6d638">Escenario 2: Modelo Agresivo</h4>
<div class="outline-text-4" id="text-orgeb6d638">
<p>
Un modelo que salta muy frecuentemente:
</p>

<pre class="example" id="org4bae45c">
                    Predicción
                 No Salto  Salto
Real  No Salto      50      30
      Salto           2      18
</pre>

<ul class="org-ul">
<li>Accuracy: 68% (68/100)</li>
<li>Precision: 37.5% (18/48) - Muchos saltos innecesarios</li>
<li>Recall: 90% (18/20) - Detecta casi todos los peligros</li>
<li>F1: 53.3%</li>
</ul>

<p>
<b><b>Veredicto</b></b>: Mejor que el conservador. Aunque tiene muchos saltos innecesarios, evita casi todas las colisiones.
</p>
</div>
</div>
<div id="outline-container-org3321673" class="outline-4">
<h4 id="org3321673">Escenario 3: Modelo Balanceado (Ideal)</h4>
<div class="outline-text-4" id="text-org3321673">
<p>
Un modelo bien entrenado:
</p>

<pre class="example" id="org47a2be2">
                    Predicción
                 No Salto  Salto
Real  No Salto      70      10
      Salto           3      17
</pre>

<ul class="org-ul">
<li>Accuracy: 87% (87/100)</li>
<li>Precision: 63% (17/27)</li>
<li>Recall: 85% (17/20) - Detecta la mayoría de peligros</li>
<li>F1: 72.3%</li>
</ul>

<p>
<b><b>Veredicto</b></b>: Buen modelo. Balance entre evitar colisiones y no saltar innecesariamente.
</p>
</div>
</div>
</div>
<div id="outline-container-org126a5fb" class="outline-3">
<h3 id="org126a5fb">Implementación en Python</h3>
<div class="outline-text-3" id="text-org126a5fb">
</div>
<div id="outline-container-orgde87149" class="outline-4">
<h4 id="orgde87149">Cálculo manual de métricas</h4>
<div class="outline-text-4" id="text-orgde87149">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #fb4934;">from</span> sklearn.metrics <span style="color: #fb4934;">import</span> confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

<span style="color: #928374;"># </span><span style="color: #928374;">Ejemplo con predicciones y valores reales
</span><span style="color: #83a598;">y_real</span> = <span style="color: #fe8019;">[</span>0, 0, 0, 1, 1, 1, 0, 1, 0, 1<span style="color: #fe8019;">]</span>
<span style="color: #83a598;">y_pred</span> = <span style="color: #fe8019;">[</span>0, 0, 1, 1, 1, 0, 0, 1, 0, 1<span style="color: #fe8019;">]</span>

<span style="color: #928374;"># </span><span style="color: #928374;">Matriz de confusi&#243;n
</span><span style="color: #83a598;">cm</span> = confusion_matrix<span style="color: #fe8019;">(</span>y_real, y_pred<span style="color: #fe8019;">)</span>
<span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span><span style="color: #b8bb26;">"Matriz de confusi&#243;n:"</span><span style="color: #fe8019;">)</span>
<span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>cm<span style="color: #fe8019;">)</span>
<span style="color: #928374;"># </span><span style="color: #928374;">[[4 1]   # TN=4, FP=1
</span><span style="color: #928374;">#  </span><span style="color: #928374;">[1 4]]  # FN=1, TP=4
</span>
<span style="color: #928374;"># </span><span style="color: #928374;">M&#233;tricas
</span><span style="color: #83a598;">accuracy</span> = accuracy_score<span style="color: #fe8019;">(</span>y_real, y_pred<span style="color: #fe8019;">)</span>
<span style="color: #83a598;">precision</span> = precision_score<span style="color: #fe8019;">(</span>y_real, y_pred<span style="color: #fe8019;">)</span>
<span style="color: #83a598;">recall</span> = recall_score<span style="color: #fe8019;">(</span>y_real, y_pred<span style="color: #fe8019;">)</span>
<span style="color: #83a598;">f1</span> = f1_score<span style="color: #fe8019;">(</span>y_real, y_pred<span style="color: #fe8019;">)</span>

<span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>f<span style="color: #b8bb26;">"Accuracy: </span>{accuracy:.2%}<span style="color: #b8bb26;">"</span><span style="color: #fe8019;">)</span>
<span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>f<span style="color: #b8bb26;">"Precision: </span>{precision:.2%}<span style="color: #b8bb26;">"</span><span style="color: #fe8019;">)</span>
<span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>f<span style="color: #b8bb26;">"Recall: </span>{recall:.2%}<span style="color: #b8bb26;">"</span><span style="color: #fe8019;">)</span>
<span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>f<span style="color: #b8bb26;">"F1-Score: </span>{f1:.2%}<span style="color: #b8bb26;">"</span><span style="color: #fe8019;">)</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-org4e64726" class="outline-4">
<h4 id="org4e64726">Reporte completo de clasificación</h4>
<div class="outline-text-4" id="text-org4e64726">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #fb4934;">from</span> sklearn.metrics <span style="color: #fb4934;">import</span> classification_report

<span style="color: #83a598;">y_real</span> = <span style="color: #fe8019;">[</span>0, 0, 0, 1, 1, 1, 0, 1, 0, 1<span style="color: #fe8019;">]</span>
<span style="color: #83a598;">y_pred</span> = <span style="color: #fe8019;">[</span>0, 0, 1, 1, 1, 0, 0, 1, 0, 1<span style="color: #fe8019;">]</span>

<span style="color: #83a598;">report</span> = classification_report<span style="color: #fe8019;">(</span>y_real, y_pred, target_names=<span style="color: #b16286;">[</span><span style="color: #b8bb26;">'No Salto'</span>, <span style="color: #b8bb26;">'Salto'</span><span style="color: #b16286;">]</span><span style="color: #fe8019;">)</span>
<span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>report<span style="color: #fe8019;">)</span>
</pre>
</div>

<p>
Salida esperada:
</p>
<pre class="example" id="org4253d64">
              precision    recall  f1-score   support

    No Salto       0.80      0.80      0.80         5
       Salto       0.80      0.80      0.80         5

    accuracy                           0.80        10
   macro avg       0.80      0.80      0.80        10
weighted avg       0.80      0.80      0.80        10
</pre>
</div>
</div>
<div id="outline-container-orgaeff331" class="outline-4">
<h4 id="orgaeff331">Visualización de la matriz de confusión</h4>
<div class="outline-text-4" id="text-orgaeff331">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #fb4934;">import</span> matplotlib.pyplot <span style="color: #fb4934;">as</span> plt
<span style="color: #fb4934;">import</span> seaborn <span style="color: #fb4934;">as</span> sns
<span style="color: #fb4934;">from</span> sklearn.metrics <span style="color: #fb4934;">import</span> confusion_matrix

<span style="color: #83a598;">y_real</span> = <span style="color: #fe8019;">[</span>0, 0, 0, 1, 1, 1, 0, 1, 0, 1<span style="color: #fe8019;">]</span>
<span style="color: #83a598;">y_pred</span> = <span style="color: #fe8019;">[</span>0, 0, 1, 1, 1, 0, 0, 1, 0, 1<span style="color: #fe8019;">]</span>

<span style="color: #83a598;">cm</span> = confusion_matrix<span style="color: #fe8019;">(</span>y_real, y_pred<span style="color: #fe8019;">)</span>
sns.heatmap<span style="color: #fe8019;">(</span>cm, annot=<span style="color: #d3869b;">True</span>, fmt=<span style="color: #b8bb26;">'d'</span>, cmap=<span style="color: #b8bb26;">'Blues'</span>, 
<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   xticklabels=<span style="color: #b16286;">[</span><span style="color: #b8bb26;">'No Salto'</span>, <span style="color: #b8bb26;">'Salto'</span><span style="color: #b16286;">]</span>,
<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   yticklabels=<span style="color: #b16286;">[</span><span style="color: #b8bb26;">'No Salto'</span>, <span style="color: #b8bb26;">'Salto'</span><span style="color: #b16286;">]</span><span style="color: #fe8019;">)</span>
plt.ylabel<span style="color: #fe8019;">(</span><span style="color: #b8bb26;">'Real'</span><span style="color: #fe8019;">)</span>
plt.xlabel<span style="color: #fe8019;">(</span><span style="color: #b8bb26;">'Predicci&#243;n'</span><span style="color: #fe8019;">)</span>
plt.title<span style="color: #fe8019;">(</span><span style="color: #b8bb26;">'Matriz de Confusi&#243;n'</span><span style="color: #fe8019;">)</span>
plt.show<span style="color: #fe8019;">()</span>
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org16a5b04" class="outline-3">
<h3 id="org16a5b04">Mejorando el Juego con Métricas</h3>
<div class="outline-text-3" id="text-org16a5b04">
</div>
<div id="outline-container-org69b6001" class="outline-4">
<h4 id="org69b6001">Añadir métricas al entrenamiento</h4>
<div class="outline-text-4" id="text-org69b6001">
<p>
Podemos modificar <code>entrenar_modelo()</code> para mostrar más métricas:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #fb4934;">from</span> sklearn.metrics <span style="color: #fb4934;">import</span> classification_report, confusion_matrix

<span style="color: #fb4934;">def</span> <span style="color: #b8bb26;">entrenar_modelo_mejorado</span><span style="color: #fe8019;">(</span><span style="color: #fb4934;">self</span><span style="color: #fe8019;">)</span> -&gt; Tuple<span style="color: #fe8019;">[</span><span style="color: #fe8019;">bool</span>, <span style="color: #fe8019;">str</span><span style="color: #fe8019;">]</span>:
<span style="background-color: #3c3836;"> </span>   <span style="color: #928374;"># </span><span style="color: #928374;">... c&#243;digo de entrenamiento existente ...
</span><span style="background-color: #3c3836;"> </span>   
<span style="background-color: #3c3836;"> </span>   <span style="color: #928374;"># </span><span style="color: #928374;">Predicciones en el conjunto de prueba
</span><span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">y_pred</span> = clf.predict<span style="color: #fe8019;">(</span>X_test<span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   
<span style="background-color: #3c3836;"> </span>   <span style="color: #928374;"># </span><span style="color: #928374;">Matriz de confusi&#243;n
</span><span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">cm</span> = confusion_matrix<span style="color: #fe8019;">(</span>y_test, y_pred<span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   <span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span><span style="color: #b8bb26;">"</span><span style="color: #d3869b;">\n</span><span style="color: #b8bb26;">Matriz de Confusi&#243;n:"</span><span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   <span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>cm<span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   
<span style="background-color: #3c3836;"> </span>   <span style="color: #928374;"># </span><span style="color: #928374;">Reporte completo
</span><span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">report</span> = classification_report<span style="color: #fe8019;">(</span>y_test, y_pred, 
<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>  target_names=<span style="color: #b16286;">[</span><span style="color: #b8bb26;">'No Salto'</span>, <span style="color: #b8bb26;">'Salto'</span><span style="color: #b16286;">]</span><span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   <span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span><span style="color: #b8bb26;">"</span><span style="color: #d3869b;">\n</span><span style="color: #b8bb26;">Reporte de Clasificaci&#243;n:"</span><span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   <span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>report<span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   
<span style="background-color: #3c3836;"> </span>   <span style="color: #928374;"># </span><span style="color: #928374;">M&#233;tricas individuales
</span><span style="background-color: #3c3836;"> </span>   <span style="color: #fb4934;">from</span> sklearn.metrics <span style="color: #fb4934;">import</span> precision_score, recall_score, f1_score
<span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">precision</span> = precision_score<span style="color: #fe8019;">(</span>y_test, y_pred<span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">recall</span> = recall_score<span style="color: #fe8019;">(</span>y_test, y_pred<span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   <span style="color: #83a598;">f1</span> = f1_score<span style="color: #fe8019;">(</span>y_test, y_pred<span style="color: #fe8019;">)</span>
<span style="background-color: #3c3836;"> </span>   
<span style="background-color: #3c3836;"> </span>   <span style="color: #fb4934;">return</span> <span style="color: #d3869b;">True</span>, <span style="color: #fe8019;">(</span>f<span style="color: #b8bb26;">"MLP entrenado. Accuracy: </span>{acc:.3f}<span style="color: #b8bb26;">, "</span>
<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span> f<span style="color: #b8bb26;">"Precision: </span>{precision:.3f}<span style="color: #b8bb26;">, "</span>
<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span> f<span style="color: #b8bb26;">"Recall: </span>{recall:.3f}<span style="color: #b8bb26;">, "</span>
<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span> f<span style="color: #b8bb26;">"F1: </span>{f1:.3f}<span style="color: #b8bb26;">"</span><span style="color: #fe8019;">)</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-orga705a19" class="outline-4">
<h4 id="orga705a19">Interpretar resultados en el juego</h4>
<div class="outline-text-4" id="text-orga705a19">
<p>
Si el modelo tiene:
</p>
<ul class="org-ul">
<li>Recall bajo (&lt; 0.6): Muchas colisiones → recolecta más datos donde saltas</li>
<li>Precision bajo (&lt; 0.5): Muchos saltos innecesarios → recolecta más datos donde NO saltas</li>
<li>F1 bajo (&lt; 0.6): El modelo no está aprendiendo bien → más datos variados</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org74cdf0d" class="outline-3">
<h3 id="org74cdf0d">Curvas ROC y AUC</h3>
<div class="outline-text-3" id="text-org74cdf0d">
</div>
<div id="outline-container-org1dad432" class="outline-4">
<h4 id="org1dad432">¿Qué es la curva ROC?</h4>
<div class="outline-text-4" id="text-org1dad432">
<p>
ROC (Receiver Operating Characteristic) muestra el trade-off entre:
</p>
<ul class="org-ul">
<li>Tasa de Verdaderos Positivos (Recall/TPR) en el eje Y</li>
<li>Tasa de Falsos Positivos (FPR = FP/(FP+TN)) en el eje X</li>
</ul>
</div>
</div>
<div id="outline-container-orgc7255e5" class="outline-4">
<h4 id="orgc7255e5">¿Qué es AUC?</h4>
<div class="outline-text-4" id="text-orgc7255e5">
<p>
AUC (Area Under Curve) es el área bajo la curva ROC:
</p>
<ul class="org-ul">
<li>AUC = 1.0: Clasificador perfecto</li>
<li>AUC = 0.5: Clasificador aleatorio (no mejor que adivinar)</li>
<li>AUC &gt; 0.7: Buen clasificador</li>
<li>AUC &gt; 0.9: Excelente clasificador</li>
</ul>
</div>
</div>
<div id="outline-container-orgc66e912" class="outline-4">
<h4 id="orgc66e912">Código para generar curva ROC</h4>
<div class="outline-text-4" id="text-orgc66e912">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #fb4934;">from</span> sklearn.metrics <span style="color: #fb4934;">import</span> roc_curve, roc_auc_score
<span style="color: #fb4934;">import</span> matplotlib.pyplot <span style="color: #fb4934;">as</span> plt

<span style="color: #928374;"># </span><span style="color: #928374;">Obtener probabilidades (no solo predicciones)
</span><span style="color: #83a598;">y_proba</span> = modelo.predict_proba<span style="color: #fe8019;">(</span>X_test<span style="color: #fe8019;">)[</span>:, 1<span style="color: #fe8019;">]</span>

<span style="color: #928374;"># </span><span style="color: #928374;">Calcular curva ROC
</span><span style="color: #83a598;">fpr</span>, <span style="color: #83a598;">tpr</span>, <span style="color: #83a598;">thresholds</span> = roc_curve<span style="color: #fe8019;">(</span>y_test, y_proba<span style="color: #fe8019;">)</span>
<span style="color: #83a598;">auc</span> = roc_auc_score<span style="color: #fe8019;">(</span>y_test, y_proba<span style="color: #fe8019;">)</span>

<span style="color: #928374;"># </span><span style="color: #928374;">Graficar
</span>plt.figure<span style="color: #fe8019;">(</span>figsize=<span style="color: #b16286;">(</span>8, 6<span style="color: #b16286;">)</span><span style="color: #fe8019;">)</span>
plt.plot<span style="color: #fe8019;">(</span>fpr, tpr, label=f<span style="color: #b8bb26;">'ROC curve (AUC = </span>{auc:.2f}<span style="color: #b8bb26;">)'</span><span style="color: #fe8019;">)</span>
plt.plot<span style="color: #fe8019;">(</span><span style="color: #b16286;">[</span>0, 1<span style="color: #b16286;">]</span>, <span style="color: #b16286;">[</span>0, 1<span style="color: #b16286;">]</span>, <span style="color: #b8bb26;">'k--'</span>, label=<span style="color: #b8bb26;">'Random classifier'</span><span style="color: #fe8019;">)</span>
plt.xlabel<span style="color: #fe8019;">(</span><span style="color: #b8bb26;">'False Positive Rate'</span><span style="color: #fe8019;">)</span>
plt.ylabel<span style="color: #fe8019;">(</span><span style="color: #b8bb26;">'True Positive Rate'</span><span style="color: #fe8019;">)</span>
plt.title<span style="color: #fe8019;">(</span><span style="color: #b8bb26;">'ROC Curve'</span><span style="color: #fe8019;">)</span>
plt.legend<span style="color: #fe8019;">()</span>
plt.grid<span style="color: #fe8019;">(</span><span style="color: #d3869b;">True</span><span style="color: #fe8019;">)</span>
plt.show<span style="color: #fe8019;">()</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-org087fd99" class="outline-4">
<h4 id="org087fd99">Interpretación</h4>
<div class="outline-text-4" id="text-org087fd99">
<ul class="org-ul">
<li>Curva cerca de la esquina superior izquierda → mejor modelo</li>
<li>AUC alto → modelo puede distinguir bien entre clases</li>
<li>Útil para comparar diferentes modelos o umbrales</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org9bbcf26" class="outline-3">
<h3 id="org9bbcf26">Métricas para Clases Desbalanceadas</h3>
<div class="outline-text-3" id="text-org9bbcf26">
</div>
<div id="outline-container-org0632c0e" class="outline-4">
<h4 id="org0632c0e">El problema</h4>
<div class="outline-text-4" id="text-org0632c0e">
<p>
Cuando una clase es mucho más frecuente que la otra:
</p>
<ul class="org-ul">
<li>Accuracy puede ser engañoso</li>
<li>Un modelo que siempre predice la clase mayoritaria puede tener buen accuracy</li>
</ul>
</div>
</div>
<div id="outline-container-org8054b83" class="outline-4">
<h4 id="org8054b83">Soluciones</h4>
<div class="outline-text-4" id="text-org8054b83">
<ol class="org-ol">
<li><b><b>F1-Score</b></b>: Ya lo vimos, balancea Precision y Recall</li>

<li><b><b>Precision-Recall Curve</b></b>: Similar a ROC pero mejor para clases desbalanceadas</li>
</ol>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #fb4934;">from</span> sklearn.metrics <span style="color: #fb4934;">import</span> precision_recall_curve, auc

<span style="color: #83a598;">precision_vals</span>, <span style="color: #83a598;">recall_vals</span>, <span style="color: #83a598;">thresholds</span> = precision_recall_curve<span style="color: #fe8019;">(</span>y_test, y_proba<span style="color: #fe8019;">)</span>
<span style="color: #83a598;">pr_auc</span> = auc<span style="color: #fe8019;">(</span>recall_vals, precision_vals<span style="color: #fe8019;">)</span>

plt.plot<span style="color: #fe8019;">(</span>recall_vals, precision_vals<span style="color: #fe8019;">)</span>
plt.xlabel<span style="color: #fe8019;">(</span><span style="color: #b8bb26;">'Recall'</span><span style="color: #fe8019;">)</span>
plt.ylabel<span style="color: #fe8019;">(</span><span style="color: #b8bb26;">'Precision'</span><span style="color: #fe8019;">)</span>
plt.title<span style="color: #fe8019;">(</span>f<span style="color: #b8bb26;">'Precision-Recall Curve (AUC = </span>{pr_auc:.2f}<span style="color: #b8bb26;">)'</span><span style="color: #fe8019;">)</span>
plt.show<span style="color: #fe8019;">()</span>
</pre>
</div>

<ol class="org-ol">
<li><b><b>Matthews Correlation Coefficient (MCC)</b></b>:</li>
</ol>

<p>
\[
MCC = \frac{TP \times TN - FP \times FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}
\]
</p>

<ul class="org-ul">
<li>Rango: -1 a +1</li>
<li>+1: Predicción perfecta</li>
<li>0: Predicción aleatoria</li>
<li>-1: Predicción inversa perfecta</li>
<li>Útil para clases desbalanceadas</li>
</ul>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #fb4934;">from</span> sklearn.metrics <span style="color: #fb4934;">import</span> matthews_corrcoef

<span style="color: #83a598;">mcc</span> = matthews_corrcoef<span style="color: #fe8019;">(</span>y_test, y_pred<span style="color: #fe8019;">)</span>
<span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>f<span style="color: #b8bb26;">"MCC: </span>{mcc:.3f}<span style="color: #b8bb26;">"</span><span style="color: #fe8019;">)</span>
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-orgaa82ca2" class="outline-3">
<h3 id="orgaa82ca2">Resumen: Guía Rápida</h3>
<div class="outline-text-3" id="text-orgaa82ca2">
</div>
<div id="outline-container-org7787ea3" class="outline-4">
<h4 id="org7787ea3">Tabla de métricas clave</h4>
<div class="outline-text-4" id="text-org7787ea3">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Métrica</th>
<th scope="col" class="org-left">Fórmula</th>
<th scope="col" class="org-right">Rango</th>
<th scope="col" class="org-left">¿Qué mide?</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Accuracy</td>
<td class="org-left">(TP+TN)/(TP+TN+FP+FN)</td>
<td class="org-right">0-1</td>
<td class="org-left">Proporción de aciertos totales</td>
</tr>

<tr>
<td class="org-left">Precision</td>
<td class="org-left">TP/(TP+FP)</td>
<td class="org-right">0-1</td>
<td class="org-left">Confiabilidad de predicciones positivas</td>
</tr>

<tr>
<td class="org-left">Recall</td>
<td class="org-left">TP/(TP+FN)</td>
<td class="org-right">0-1</td>
<td class="org-left">Detección de casos positivos reales</td>
</tr>

<tr>
<td class="org-left">F1-Score</td>
<td class="org-left">2×(P×R)/(P+R)</td>
<td class="org-right">0-1</td>
<td class="org-left">Balance Precision/Recall</td>
</tr>

<tr>
<td class="org-left">Especificidad</td>
<td class="org-left">TN/(TN+FP)</td>
<td class="org-right">0-1</td>
<td class="org-left">Detección de casos negativos reales</td>
</tr>

<tr>
<td class="org-left">AUC-ROC</td>
<td class="org-left">Área bajo curva ROC</td>
<td class="org-right">0-1</td>
<td class="org-left">Capacidad de distinguir clases</td>
</tr>

<tr>
<td class="org-left">MCC</td>
<td class="org-left">(TP×TN-FP×FN)/√(&#x2026;)</td>
<td class="org-right">-1 a +1</td>
<td class="org-left">Correlación general (balanceada)</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-org348356c" class="outline-4">
<h4 id="org348356c">Decisión rápida: ¿Qué métrica usar?</h4>
<div class="outline-text-4" id="text-org348356c">
<pre class="example" id="orgcd6e06c">
¿Las clases están balanceadas?
├─ SÍ → Accuracy es útil
└─ NO → Usa F1-Score o MCC

¿Los falsos negativos son críticos?
├─ SÍ → Recall es la más importante
└─ NO → Precision puede ser más importante

¿Necesitas una métrica única?
├─ SÍ → F1-Score o MCC
└─ NO → Revisa Precision, Recall y Accuracy juntas
</pre>
</div>
</div>
<div id="outline-container-org4e42d27" class="outline-4">
<h4 id="org4e42d27">Checklist de evaluación</h4>
<div class="outline-text-4" id="text-org4e42d27">
<p>
Antes de aceptar un modelo, verifica:
</p>

<ul class="org-ul">
<li class="off"><code>[&#xa0;]</code> Accuracy &gt; umbral mínimo (ej: 0.7)</li>
<li class="off"><code>[&#xa0;]</code> Recall &gt; umbral mínimo (ej: 0.7) - especialmente si FN son críticos</li>
<li class="off"><code>[&#xa0;]</code> Precision &gt; umbral mínimo (ej: 0.6) - si FP son costosos</li>
<li class="off"><code>[&#xa0;]</code> F1-Score muestra buen balance</li>
<li class="off"><code>[&#xa0;]</code> Matriz de confusión no muestra patrones extraños</li>
<li class="off"><code>[&#xa0;]</code> Métricas en test son similares a entrenamiento (no overfitting)</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org1a759ff" class="outline-3">
<h3 id="org1a759ff">Ejercicios Prácticos</h3>
<div class="outline-text-3" id="text-org1a759ff">
</div>
<div id="outline-container-org4c6b10d" class="outline-4">
<h4 id="org4c6b10d">Ejercicio 1: Calcular métricas manualmente</h4>
<div class="outline-text-4" id="text-org4c6b10d">
<p>
Dada esta matriz de confusión:
</p>

<pre class="example" id="org771e83b">
                    Predicción
                 No Salto  Salto
Real  No Salto      60      20
      Salto         10      10
</pre>

<p>
Calcula:
</p>
<ul class="org-ul">
<li>TP, TN, FP, FN</li>
<li>Accuracy</li>
<li>Precision</li>
<li>Recall</li>
<li>F1-Score</li>
<li>Especificidad</li>
</ul>

<p>
<b><b>Solución</b></b>:
</p>
<ul class="org-ul">
<li>TP = 10, TN = 60, FP = 20, FN = 10</li>
<li>Accuracy = 70/100 = 0.70</li>
<li>Precision = 10/30 = 0.33</li>
<li>Recall = 10/20 = 0.50</li>
<li>F1 = 2×(0.33×0.50)/(0.33+0.50) = 0.40</li>
<li>Especificidad = 60/80 = 0.75</li>
</ul>
</div>
</div>
<div id="outline-container-org85d1624" class="outline-4">
<h4 id="org85d1624">Ejercicio 2: Interpretar resultados</h4>
<div class="outline-text-4" id="text-org85d1624">
<p>
Un modelo tiene:
</p>
<ul class="org-ul">
<li>Accuracy: 85%</li>
<li>Precision: 40%</li>
<li>Recall: 90%</li>
</ul>

<p>
¿Es un buen modelo para el juego? ¿Por qué?
</p>

<p>
<b><b>Solución</b></b>: 
</p>
<ul class="org-ul">
<li>Recall alto (90%) es bueno → detecta casi todos los peligros</li>
<li>Precision bajo (40%) → muchos saltos innecesarios</li>
<li>Para el juego: Es aceptable porque evitar colisiones (Recall) es más importante que saltos innecesarios (Precision)</li>
</ul>
</div>
</div>
<div id="outline-container-org172a045" class="outline-4">
<h4 id="org172a045">Ejercicio 3: Mejorar un modelo</h4>
<div class="outline-text-4" id="text-org172a045">
<p>
Un modelo tiene Recall = 0.5. ¿Qué puedes hacer para mejorarlo?
</p>

<p>
<b><b>Solución</b></b>:
</p>
<ul class="org-ul">
<li>Recolectar más datos donde realmente debes saltar</li>
<li>Disminuir el umbral de decisión (de 0.5 a 0.3, por ejemplo)</li>
<li>Ajustar hiperparámetros del MLP</li>
<li>Añadir más características al modelo</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgc38b790" class="outline-3">
<h3 id="orgc38b790">Referencias y Recursos</h3>
<div class="outline-text-3" id="text-orgc38b790">
</div>
<div id="outline-container-orge4fa7e9" class="outline-4">
<h4 id="orge4fa7e9">Documentación oficial</h4>
<div class="outline-text-4" id="text-orge4fa7e9">
<ul class="org-ul">
<li>scikit-learn metrics: <a href="https://scikit-learn.org/stable/modules/model_evaluation.html">https://scikit-learn.org/stable/modules/model_evaluation.html</a></li>
<li>Confusion matrix: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html</a></li>
</ul>
</div>
</div>
<div id="outline-container-org9cedc72" class="outline-4">
<h4 id="org9cedc72">Lecturas recomendadas</h4>
<div class="outline-text-4" id="text-org9cedc72">
<ul class="org-ul">
<li>"Hands-On Machine Learning" - Aurélien Géron (Capítulo sobre evaluación)</li>
<li>"Pattern Recognition and Machine Learning" - Christopher Bishop</li>
<li>"The Elements of Statistical Learning" - Hastie, Tibshirani, Friedman</li>
</ul>
</div>
</div>
<div id="outline-container-orgeb319c5" class="outline-4">
<h4 id="orgeb319c5">Herramientas útiles</h4>
<div class="outline-text-4" id="text-orgeb319c5">
<ul class="org-ul">
<li>scikit-learn: Métricas implementadas</li>
<li>Yellowbrick: Visualizaciones de métricas</li>
<li>Weka: Suite completa de ML con evaluación</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgb2e3501" class="outline-3">
<h3 id="orgb2e3501">Conclusión</h3>
<div class="outline-text-3" id="text-orgb2e3501">
<p>
Las métricas de evaluación son esenciales para entender y mejorar modelos de Machine Learning. No hay una métrica "perfecta" - la elección depende de:
</p>
<ul class="org-ul">
<li>Tu problema específico</li>
<li>El costo de diferentes tipos de errores</li>
<li>El balance de clases</li>
<li>Tus objetivos</li>
</ul>

<p>
Para el juego de salto:
</p>
<ul class="org-ul">
<li><b><b>Recall es crítico</b></b> (evitar colisiones)</li>
<li><b><b>Precision es importante</b></b> (no saltar innecesariamente)</li>
<li><b><b>F1-Score</b></b> da un buen balance general</li>
<li><b><b>Accuracy</b></b> puede ser engañoso si las clases están desbalanceadas</li>
</ul>

<p>
Recuerda: Un modelo con 90% accuracy puede ser inútil si tiene 10% Recall (muchas colisiones). Siempre revisa múltiples métricas.
</p>
</div>
</div>
</div>
<div id="outline-container-org23c386f" class="outline-2">
<h2 id="org23c386f">Redes Neuronales LSTM (Long Short-Term Memory)</h2>
<div class="outline-text-2" id="text-org23c386f">
<p>
Las redes neuronales LSTM (<b>Long Short-Term Memory</b>) son un tipo especial
de <b><b>Red Neuronal Recurrente (RNN)</b></b> diseñada para trabajar con datos
<b><b>secuenciales</b></b> y, en particular, para <b><b>recordar dependencias a largo
plazo</b></b>.
</p>

<p>
Se utilizan cuando el <b>orden</b> de los datos importa:
</p>

<ul class="org-ul">
<li>Texto (oraciones, párrafos, documentos).</li>
<li>Series temporales (precios, temperatura, señales).</li>
<li>Audio y voz.</li>
<li>Secuencias de eventos (logs, clics, acciones de usuario, etc.).</li>
</ul>

<p>
Su principal aporte es resolver (o al menos mitigar fuertemente) el
problema del <b><b>desvanecimiento del gradiente (vanishing gradient)</b></b> que
afecta a las RNN simples cuando manejan secuencias largas.
</p>
</div>
<div id="outline-container-org07e416b" class="outline-3">
<h3 id="org07e416b">Estructura general de una RNN</h3>
<div class="outline-text-3" id="text-org07e416b">
<p>
En una RNN “simple”, en cada paso temporal \(t\):
</p>

<ul class="org-ul">
<li>Se recibe una entrada \(x_t\).</li>
<li>Se tiene un estado oculto anterior \(h_{t-1}\).</li>
<li>Se actualiza el estado oculto actual \(h_t\) con:</li>
</ul>

<div class="latex" id="org72fa874">
<p>
h<sub>t</sub> = &phi;(W<sub>xh</sub> x<sub>t</sub> + W<sub>hh</sub> h<sub>t-1</sub> + b<sub>h</sub>)
</p>

</div>

<p>
donde:
</p>

<ul class="org-ul">
<li>\(W_{xh}\) y \(W_{hh}\) son matrices de pesos,</li>
<li>\(b_h\) es el sesgo,</li>
<li>\(\phi\) es una función de activación (típicamente <code>tanh</code> o <code>ReLU</code>).</li>
</ul>

<p>
El estado final (o todos los estados) se usan para hacer predicciones.
</p>
</div>
<div id="outline-container-orgcd6e749" class="outline-4">
<h4 id="orgcd6e749">Problema: vanishing y exploding gradients</h4>
<div class="outline-text-4" id="text-orgcd6e749">
<p>
Durante el entrenamiento, se aplica <b>Backpropagation Through Time (BPTT)</b>,
es decir, se retropropagan los gradientes a través de todos los pasos
temporales.
</p>

<p>
En secuencias largas:
</p>

<ul class="org-ul">
<li>Los gradientes pueden volverse <b><b>muy pequeños</b></b> → la red “olvida” las
dependencias lejanas (problema de <b>vanishing gradient</b>).</li>
<li>O volverse <b><b>muy grandes</b></b> → inestabilidad numérica (problema de
<b>exploding gradient</b>).</li>
</ul>

<p>
Consecuencia: las RNN simples tienen dificultades para <b><b>aprender
relaciones de largo plazo</b></b>, por ejemplo, la dependencia entre una
palabra al principio de una oración y otra al final.
</p>
</div>
</div>
</div>
<div id="outline-container-orge603905" class="outline-3">
<h3 id="orge603905">Motivación de las LSTM</h3>
<div class="outline-text-3" id="text-orge603905">
<p>
Las LSTM fueron propuestas para permitir que la red:
</p>

<ul class="org-ul">
<li><b><b>Mantenga información relevante durante muchos pasos temporales</b></b>.</li>
<li><b><b>Controle qué recordar y qué olvidar</b></b>.</li>
<li>Facilite el flujo de gradiente a través del tiempo, reduciendo el
problema del desvanecimiento.</li>
</ul>

<p>
La idea principal es introducir una <b><b>celda de memoria</b></b> y un sistema de
<b><b>puertas (gates)</b></b> que regulan el flujo de información.
</p>
</div>
</div>
<div id="outline-container-org21fc0f5" class="outline-3">
<h3 id="org21fc0f5">Arquitectura interna de una celda LSTM</h3>
<div class="outline-text-3" id="text-org21fc0f5">
<p>
En lugar de solo tener el estado oculto \(h_t\), la LSTM mantiene:
</p>

<ul class="org-ul">
<li>un <b><b>estado de memoria</b></b> \(c_t\) (a veces llamado <b>cell state</b>),</li>
<li>un <b><b>estado oculto</b></b> \(h_t\) (la “salida” en ese paso).</li>
</ul>

<p>
En cada paso temporal \(t\), la LSTM procesa:
</p>

<ul class="org-ul">
<li>la entrada actual \(x_t\),</li>
<li>el estado oculto anterior \(h_{t-1}\),</li>
<li>el estado de memoria anterior \(c_{t-1}\).</li>
</ul>

<p>
Y calcula:
</p>

<ul class="org-ul">
<li>una <b>puerta de olvido</b> \(f_t\),</li>
<li>una <b>puerta de entrada</b> \(i_t\),</li>
<li>una <b>candidata de memoria</b> \(\tilde{c}_t\),</li>
<li>una <b>puerta de salida</b> \(o_t\),</li>
<li>el nuevo estado de memoria \(c_t\),</li>
<li>el nuevo estado oculto \(h_t\).</li>
</ul>
</div>
<div id="outline-container-org01725c3" class="outline-4">
<h4 id="org01725c3">Ecuaciones de la LSTM</h4>
<div class="outline-text-4" id="text-org01725c3">
<p>
Típicamente, las ecuaciones de una LSTM (versión “estándar”) son:
</p>

<div class="latex" id="org9bcc280">
\begin{aligned}
f_t &= \sigma(W_f [h_{t-1}, x_t] + b_f) \\
i_t &= \sigma(W_i [h_{t-1}, x_t] + b_i) \\
\tilde{c}_t &= \tanh(W_c [h_{t-1}, x_t] + b_c) \\
c_t &= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t \\
o_t &= \sigma(W_o [h_{t-1}, x_t] + b_o) \\
h_t &= o_t \odot \tanh(c_t)
\end{aligned}

</div>

<p>
donde:
</p>

<ul class="org-ul">
<li>\(\sigma\) es la función sigmoide,</li>
<li>\(\tanh\) es la tangente hiperbólica,</li>
<li>\(\odot\) es el producto elemento a elemento,</li>
<li>\([h_{t-1}, x_t]\) indica la concatenación de vectores.</li>
</ul>
</div>
</div>
<div id="outline-container-orgb05eb1c" class="outline-4">
<h4 id="orgb05eb1c">Interpretación de las puertas</h4>
<div class="outline-text-4" id="text-orgb05eb1c">
<ul class="org-ul">
<li>\(f_t\) (<b>forget gate</b> o puerta de olvido):  
<ul class="org-ul">
<li>Toma valores en \([0,1]\) (por la sigmoide).</li>
<li>Decide cuánto de la memoria anterior \(c_{t-1}\) se conserva.</li>
<li>Si \(f_t \approx 1\) → se mantiene casi toda la memoria anterior.</li>
<li>Si \(f_t \approx 0\) → se olvida casi todo.</li>
</ul></li>

<li>\(i_t\) (<b>input gate</b> o puerta de entrada):  
<ul class="org-ul">
<li>Controla cuánta de la nueva información candidata \(\tilde{c}_t\) se
incorpora a la memoria.</li>
<li>Valores cercanos a 1 permiten “escribir” nueva información; valores
cercanos a 0 bloquean escritura.</li>
</ul></li>

<li>\(\tilde{c}_t\) (candidata de memoria):  
<ul class="org-ul">
<li>Es el nuevo contenido potencial para la memoria.</li>
<li>Se combina con \(i_t\) para actualizar \(c_t\).</li>
</ul></li>

<li>\(c_t\) (<b>cell state</b>):  
<ul class="org-ul">
<li>Es la memoria principal de la LSTM.</li>
<li>Se actualiza como:
\[
      c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t
    \]</li>
<li>Puede transportar información durante muchos pasos, ya que el
gradiente puede fluir a lo largo de \(c_t\) con menos atenuación.</li>
</ul></li>

<li>\(o_t\) (<b>output gate</b> o puerta de salida):  
<ul class="org-ul">
<li>Controla qué parte de la memoria \(c_t\) se expone como salida \(h_t\).</li>
</ul></li>

<li>\(h_t\) (estado oculto / salida):  
<ul class="org-ul">
<li>Es la “representación” que se usa para producir salidas en cada
paso, o para alimentar capas posteriores.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgb4f8d5e" class="outline-3">
<h3 id="orgb4f8d5e">Intuición conceptual</h3>
<div class="outline-text-3" id="text-orgb4f8d5e">
<p>
Una forma intuitiva de entender una LSTM es verla como una <b><b>celda de
memoria con una “llave de escritura”, una “llave de borrado” y una
“llave de lectura”</b></b>:
</p>

<ul class="org-ul">
<li><b>Olvido</b>: la red decide qué partes de la memoria vieja ya no son
relevantes y las borra parcialmente (puerta \(f_t\)).</li>
<li><b>Escritura</b>: la red decide qué nueva información vale la pena guardar
(puerta \(i_t\) y candidata \(\tilde{c}_t\)).</li>
<li><b>Lectura</b>: la red decide qué parte de la memoria compartir como salida
(puerta \(o_t\)).</li>
</ul>

<p>
Esto permite que la red conserve información importante durante muchos
pasos temporales, por ejemplo:
</p>

<ul class="org-ul">
<li>el tema general de una oración,</li>
<li>la tendencia de una serie temporal,</li>
<li>el contexto de una conversación, etc.</li>
</ul>
</div>
</div>
<div id="outline-container-org2e836ca" class="outline-3">
<h3 id="org2e836ca">Entrenamiento de una LSTM</h3>
<div class="outline-text-3" id="text-org2e836ca">
<p>
El entrenamiento de una LSTM sigue el mismo esquema general que otras
redes neuronales:
</p>

<ol class="org-ol">
<li><b><b>Forward pass</b></b>:
<ul class="org-ul">
<li>La secuencia completa \((x_1, x_2, \dots, x_T)\) se ingresa paso a
paso.</li>
<li>En cada paso se actualizan \(c_t\) y \(h_t\).</li>
<li>Se produce una salida (por ejemplo, en cada paso o solo al final).</li>
</ul></li>

<li><b><b>Cálculo de la pérdida</b></b>:
<ul class="org-ul">
<li>Se compara la salida de la red con la etiqueta real.</li>
<li>Se usa una función de pérdida acorde al problema:
<ul class="org-ul">
<li>Clasificación: entropía cruzada (multiclase o binaria).</li>
<li>Regresión: error cuadrático medio (MSE), etc.</li>
</ul></li>
</ul></li>

<li><b><b>Backpropagation Through Time (BPTT)</b></b>:
<ul class="org-ul">
<li>Se retropropaga el gradiente desde el último paso hacia los
primeros.</li>
<li>Se actualizan los pesos de todas las puertas y conexiones.</li>
</ul></li>

<li><b><b>Actualización de parámetros</b></b>:
<ul class="org-ul">
<li>Se utilizan optimizadores como SGD, Adam, RMSprop.</li>
<li>En la práctica, Adam y RMSprop son muy populares en LSTM.</li>
</ul></li>
</ol>

<p>
Gracias a la estructura de memoria, los gradientes a través de \(c_t\)
tienden a <b><b>no desaparecer tan rápido</b></b> como en una RNN simple.
</p>
</div>
</div>
<div id="outline-container-orgc089635" class="outline-3">
<h3 id="orgc089635">Tipos de tareas típicas con LSTM</h3>
<div class="outline-text-3" id="text-orgc089635">
<p>
Las LSTM se aplican a muchas tareas de secuencia:
</p>

<ul class="org-ul">
<li><b>Modelado de lenguaje</b>:
<ul class="org-ul">
<li>Predecir la siguiente palabra de una oración.</li>
<li>Asignar probabilidad a una secuencia de palabras.</li>
</ul></li>

<li><b>Traducción automática</b>:
<ul class="org-ul">
<li>Modelos encoder–decoder (LSTM para codificar una oración, otra LSTM
para decodificar en otro idioma).</li>
</ul></li>

<li><b>Etiquetado secuencial</b>:
<ul class="org-ul">
<li>Etiquetado de partes del habla (POS tagging).</li>
<li>Reconocimiento de entidades nombradas (NER).</li>
</ul></li>

<li><b>Series temporales</b>:
<ul class="org-ul">
<li>Predicción de valores futuros (ej. precios de acciones).</li>
<li>Detección de anomalías en señales.</li>
</ul></li>

<li><b>Procesamiento de audio / voz</b>:
<ul class="org-ul">
<li>Reconocimiento de voz.</li>
<li>Síntesis de voz (en combinación con otras arquitecturas).</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgca91fd2" class="outline-3">
<h3 id="orgca91fd2">Variantes y extensiones de LSTM</h3>
<div class="outline-text-3" id="text-orgca91fd2">
<p>
A partir de la LSTM estándar, han surgido múltiples variantes:
</p>

<ul class="org-ul">
<li><b>LSTM bidireccional (Bidirectional LSTM)</b>:
<ul class="org-ul">
<li>Procesa la secuencia en dos direcciones:
<ul class="org-ul">
<li>de izquierda a derecha (forward),</li>
<li>de derecha a izquierda (backward).</li>
</ul></li>
<li>Se concatena la información de ambos sentidos.</li>
<li>Útil cuando se tiene acceso a la secuencia completa (texto, frases).</li>
</ul></li>

<li><b>Pilado de LSTMs (Stacked LSTM)</b>:
<ul class="org-ul">
<li>Varias capas LSTM apiladas una sobre otra.</li>
<li>Las salidas de una capa sirven de entradas a la siguiente.</li>
<li>Permite aprender representaciones más complejas.</li>
</ul></li>

<li><b>LSTM con atención (Attention)</b>:
<ul class="org-ul">
<li>Combinación de LSTM con mecanismos de atención.</li>
<li>El modelo decide a qué partes de la secuencia “prestar más atención”
al hacer una predicción.</li>
</ul></li>

<li><b>GRU (Gated Recurrent Unit)</b>:
<ul class="org-ul">
<li>Variante simplificada de LSTM (menos puertas, menos parámetros).</li>
<li>Suele tener rendimiento comparable en muchos problemas.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org66df1da" class="outline-3">
<h3 id="org66df1da">Implementación básica en Python (Keras)</h3>
<div class="outline-text-3" id="text-org66df1da">
<p>
A modo de ejemplo muy simple (no ligado a un dataset específico):
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #fb4934;">import</span> numpy <span style="color: #fb4934;">as</span> np
<span style="color: #fb4934;">import</span> tensorflow <span style="color: #fb4934;">as</span> tf
<span style="color: #fb4934;">from</span> tensorflow.keras.models <span style="color: #fb4934;">import</span> Sequential
<span style="color: #fb4934;">from</span> tensorflow.keras.layers <span style="color: #fb4934;">import</span> LSTM, Dense

<span style="color: #928374;"># </span><span style="color: #928374;">Supongamos una secuencia de longitud 10 con 5 features por timestep
</span><span style="color: #83a598;">timesteps</span> = 10
<span style="color: #83a598;">features</span> = 5
<span style="color: #83a598;">num_samples</span> = 100
<span style="color: #83a598;">num_classes</span> = 3

<span style="color: #928374;"># </span><span style="color: #928374;">Datos aleatorios de ejemplo (solo para ilustrar la forma)
</span><span style="color: #83a598;">X</span> = np.random.randn<span style="color: #fe8019;">(</span>num_samples, timesteps, features<span style="color: #fe8019;">)</span>
<span style="color: #83a598;">y</span> = np.random.randint<span style="color: #fe8019;">(</span>0, num_classes, size=<span style="color: #b16286;">(</span>num_samples,<span style="color: #b16286;">)</span><span style="color: #fe8019;">)</span>

<span style="color: #928374;"># </span><span style="color: #928374;">One-hot encoding
</span><span style="color: #83a598;">y_onehot</span> = tf.keras.utils.to_categorical<span style="color: #fe8019;">(</span>y, num_classes=num_classes<span style="color: #fe8019;">)</span>

<span style="color: #928374;"># </span><span style="color: #928374;">Definir modelo LSTM
</span><span style="color: #83a598;">model</span> = Sequential<span style="color: #fe8019;">(</span><span style="color: #b16286;">[</span>
<span style="background-color: #3c3836;"> </span>   LSTM<span style="color: #b8bb26;">(</span>32, input_shape=<span style="color: #83a598;">(</span>timesteps, features<span style="color: #83a598;">)</span><span style="color: #b8bb26;">)</span>,
<span style="background-color: #3c3836;"> </span>   Dense<span style="color: #b8bb26;">(</span>num_classes, activation=<span style="color: #b8bb26;">'softmax'</span><span style="color: #b8bb26;">)</span>
<span style="color: #b16286;">]</span><span style="color: #fe8019;">)</span>

model.<span style="color: #fe8019;">compile</span><span style="color: #fe8019;">(</span>
<span style="background-color: #3c3836;"> </span>   loss=<span style="color: #b8bb26;">'categorical_crossentropy'</span>,
<span style="background-color: #3c3836;"> </span>   optimizer=<span style="color: #b8bb26;">'adam'</span>,
<span style="background-color: #3c3836;"> </span>   metrics=<span style="color: #b16286;">[</span><span style="color: #b8bb26;">'accuracy'</span><span style="color: #b16286;">]</span>
<span style="color: #fe8019;">)</span>

model.summary<span style="color: #fe8019;">()</span>

<span style="color: #928374;"># </span><span style="color: #928374;">Entrenamiento de ejemplo
</span>model.fit<span style="color: #fe8019;">(</span>X, y_onehot, epochs=10, batch_size=16<span style="color: #fe8019;">)</span>
</pre>
</div>

<p>
En este ejemplo:
</p>

<ul class="org-ul">
<li>La entrada tiene forma <code>(num_samples, timesteps, features)</code>.</li>
<li>La LSTM devuelve un vector (el último estado oculto).</li>
<li>La capa <code>Dense</code> con softmax produce una distribución de probabilidad
sobre las clases.</li>
</ul>
</div>
</div>
<div id="outline-container-org8303ff2" class="outline-3">
<h3 id="org8303ff2">Ventajas y limitaciones de LSTM</h3>
<div class="outline-text-3" id="text-org8303ff2">
</div>
<div id="outline-container-org1018157" class="outline-4">
<h4 id="org1018157">Ventajas</h4>
<div class="outline-text-4" id="text-org1018157">
<ul class="org-ul">
<li>Manejan mejor dependencias de largo plazo que las RNN simples.</li>
<li>Funcionan bien en problemas donde el orden y el contexto importan.</li>
<li>Son muy flexibles y pueden combinarse con otras arquitecturas
(convolucionales, atención, etc.).</li>
</ul>
</div>
</div>
<div id="outline-container-orge2eea2c" class="outline-4">
<h4 id="orge2eea2c">Limitaciones</h4>
<div class="outline-text-4" id="text-orge2eea2c">
<ul class="org-ul">
<li>Entrenamiento relativamente costoso (más parámetros que RNN simple).</li>
<li>Difíciles de paralelizar completamente debido a su naturaleza
secuencial.</li>
<li>En tareas muy complejas y con grandes volúmenes de datos, han sido en
muchos casos superadas por arquitecturas basadas en <b>transformers</b>.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgc7c6fac" class="outline-3">
<h3 id="orgc7c6fac">Conclusión</h3>
<div class="outline-text-3" id="text-orgc7c6fac">
<p>
Las LSTM representan un paso fundamental en la historia de las redes
neuronales para secuencias:
</p>

<ul class="org-ul">
<li>resolvieron en gran medida el problema del desvanecimiento del
gradiente en RNN,</li>
<li>permitieron avances significativos en NLP, series temporales y voz,</li>
<li>siguen siendo una herramienta muy útil y educativa para entender cómo
las redes pueden “recordar” información a lo largo del tiempo.</li>
</ul>

<p>
Aun cuando hoy en día los <b>transformers</b> dominen muchas aplicaciones de
NLP, las LSTM siguen siendo:
</p>

<ul class="org-ul">
<li>una excelente base conceptual,</li>
<li>útiles en problemas con secuencias pequeñas o recursos limitados,</li>
<li>y una parte importante de la “caja de herramientas” de cualquier
persona que trabaja con aprendizaje profundo y datos secuenciales.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org3a04f2e" class="outline-2">
<h2 id="org3a04f2e">Ejemplo Clasificación del dataset Iris con una red LSTM</h2>
<div class="outline-text-2" id="text-org3a04f2e">
<p>
Este apartado muestra un ejemplo completo en Python usando el dataset
clásico Iris y una red neuronal recurrente del tipo <b>LSTM</b> (Long Short-Term Memory)
usando la librería <b>Keras</b> (TensorFlow).
</p>

<p>
El objetivo es clasificar flores Iris en tres clases:
</p>

<ul class="org-ul">
<li>Setosa</li>
<li>Versicolor</li>
<li>Virginica</li>
</ul>

<p>
A diferencia del MLP (Perceptrón Multicapa), la LSTM está diseñada
para procesar datos <b>secuenciales</b>. En este ejemplo, forzamos la
estructura secuencial tratando las 4 características de Iris como una
secuencia de 4 pasos temporales.
</p>
</div>
<div id="outline-container-org5e4703d" class="outline-3">
<h3 id="org5e4703d">Importar librerías</h3>
<div class="outline-text-3" id="text-org5e4703d">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #fb4934;">import</span> numpy <span style="color: #fb4934;">as</span> np

<span style="color: #fb4934;">from</span> sklearn.datasets <span style="color: #fb4934;">import</span> load_iris
<span style="color: #fb4934;">from</span> sklearn.model_selection <span style="color: #fb4934;">import</span> train_test_split
<span style="color: #fb4934;">from</span> sklearn.preprocessing <span style="color: #fb4934;">import</span> StandardScaler
<span style="color: #fb4934;">from</span> sklearn.metrics <span style="color: #fb4934;">import</span> accuracy_score, classification_report

<span style="color: #fb4934;">import</span> tensorflow <span style="color: #fb4934;">as</span> tf
<span style="color: #fb4934;">from</span> tensorflow.keras.models <span style="color: #fb4934;">import</span> Sequential
<span style="color: #fb4934;">from</span> tensorflow.keras.layers <span style="color: #fb4934;">import</span> LSTM, Dense
<span style="color: #fb4934;">from</span> tensorflow.keras.utils <span style="color: #fb4934;">import</span> to_categorical
</pre>
</div>
</div>
</div>
<div id="outline-container-org40ca51f" class="outline-3">
<h3 id="org40ca51f">Cargar el dataset Iris</h3>
<div class="outline-text-3" id="text-org40ca51f">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">iris</span> = load_iris<span style="color: #fe8019;">()</span>
<span style="color: #83a598;">X</span> = iris.data      <span style="color: #928374;"># </span><span style="color: #928374;">Caracter&#237;sticas (150 x 4)
</span><span style="color: #83a598;">y</span> = iris.target    <span style="color: #928374;"># </span><span style="color: #928374;">Clases (150,)
</span>
<span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>X.shape<span style="color: #fe8019;">)</span>     <span style="color: #928374;"># </span><span style="color: #928374;">(150, 4)
</span><span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>y.shape<span style="color: #fe8019;">)</span>     <span style="color: #928374;"># </span><span style="color: #928374;">(150,)</span>
</pre>
</div>

<p>
Las 4 características son:
</p>

<ul class="org-ul">
<li>Largo del sépalo</li>
<li>Ancho del sépalo</li>
<li>Largo del pétalo</li>
<li>Ancho del pétalo</li>
</ul>

<p>
Las clases están codificadas como:
</p>

<ul class="org-ul">
<li>0 → Setosa</li>
<li>1 → Versicolor</li>
<li>2 → Virginica</li>
</ul>
</div>
</div>
<div id="outline-container-org88ba2d6" class="outline-3">
<h3 id="org88ba2d6">Partición entrenamiento / prueba</h3>
<div class="outline-text-3" id="text-org88ba2d6">
<p>
El siguiente fragmento de código realiza la partición del conjunto de
datos original en dos subconjuntos disjuntos: uno destinado al
entrenamiento del modelo y otro a su evaluación.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">X_train</span>, <span style="color: #83a598;">X_test</span>, <span style="color: #83a598;">y_train</span>, <span style="color: #83a598;">y_test</span> = train_test_split<span style="color: #fe8019;">(</span>
<span style="background-color: #3c3836;"> </span>   X,
<span style="background-color: #3c3836;"> </span>   y,
<span style="background-color: #3c3836;"> </span>   test_size=0.2,
<span style="background-color: #3c3836;"> </span>   random_state=42,
<span style="background-color: #3c3836;"> </span>   stratify=y
<span style="color: #fe8019;">)</span>
</pre>
</div>
</div>
<div id="outline-container-orgcf3c5bf" class="outline-4">
<h4 id="orgcf3c5bf">Propósito de la partición</h4>
<div class="outline-text-4" id="text-orgcf3c5bf">
<p>
En aprendizaje automático supervisado, es fundamental evaluar la
capacidad de generalización de un modelo. Para ello, los datos
disponibles se dividen en:
</p>

<ul class="org-ul">
<li><b>Conjunto de entrenamiento</b>: utilizado para ajustar los parámetros del modelo (pesos y sesgos).</li>
<li><b>Conjunto de prueba</b>: utilizado exclusivamente para medir el desempeño del modelo sobre datos no vistos durante el entrenamiento.</li>
</ul>

<p>
Esta separación permite detectar fenómenos como overfitting y
underfitting, y proporciona una estimación más realista del
rendimiento esperado en producción.
</p>
</div>
</div>
<div id="outline-container-org4f9ef89" class="outline-4">
<h4 id="org4f9ef89">Parámetro test<sub>size</sub></h4>
<div class="outline-text-4" id="text-org4f9ef89">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">test_size</span> = 0.2
</pre>
</div>

<p>
indica que el 20 % del conjunto total de datos se reserva para el
conjunto de prueba, mientras que el 80 % restante se utiliza para
entrenamiento.
</p>
</div>
</div>
<div id="outline-container-org6ae8ec9" class="outline-4">
<h4 id="org6ae8ec9">Reproducibilidad del experimento</h4>
<div class="outline-text-4" id="text-org6ae8ec9">
<p>
El parámetro:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">random_state</span> = 42
</pre>
</div>

<p>
fija la semilla del generador de números aleatorios utilizado durante
la partición. Esto garantiza que la división de los datos sea
reproducible.
</p>
</div>
</div>
<div id="outline-container-org6c6c743" class="outline-4">
<h4 id="org6c6c743">Estratificación de clases</h4>
<div class="outline-text-4" id="text-org6c6c743">
<p>
El parámetro:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">stratify</span> = y
</pre>
</div>

<p>
indica que la división debe realizarse de forma estratificada,
conservando la proporción original de cada clase en ambos
subconjuntos.
</p>
</div>
</div>
</div>
<div id="outline-container-org1d9c6e9" class="outline-3">
<h3 id="org1d9c6e9">Normalización de los datos</h3>
<div class="outline-text-3" id="text-org1d9c6e9">
<p>
La normalización de los datos de entrada es un paso crítico en el
entrenamiento de redes neuronales (incluyendo LSTM), ya que influye
directamente en la estabilidad numérica, la velocidad de convergencia
y la eficacia del aprendizaje.
</p>

<p>
Usaremos estandarización con <code>StandardScaler</code>:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">scaler</span> = StandardScaler<span style="color: #fe8019;">()</span>
<span style="color: #83a598;">X_train</span> = scaler.fit_transform<span style="color: #fe8019;">(</span>X_train<span style="color: #fe8019;">)</span>
<span style="color: #83a598;">X_test</span> = scaler.transform<span style="color: #fe8019;">(</span>X_test<span style="color: #fe8019;">)</span>
</pre>
</div>

<p>
Cada característica \(x\) se transforma según:
</p>

<div class="latex" id="org9703777">
<p>
x' = \frac{x - \mu}{\sigma}
</p>

</div>

<p>
donde:
</p>

<ul class="org-ul">
<li>\( \mu \) es la media de la característica,</li>
<li>\( \sigma \) es la desviación estándar.</li>
</ul>

<p>
El resultado es:
</p>

<ul class="org-ul">
<li>media aproximadamente 0,</li>
<li>desviación estándar aproximadamente 1.</li>
</ul>
</div>
</div>
<div id="outline-container-orgc4f8d9e" class="outline-3">
<h3 id="orgc4f8d9e">Preparar los datos para LSTM</h3>
<div class="outline-text-3" id="text-orgc4f8d9e">
<p>
Las LSTM en Keras esperan entradas con forma:
</p>

<ul class="org-ul">
<li>\((n\_muestras, n\_timesteps, n\_features)\)</li>
</ul>

<p>
En Iris, tenemos 4 características. Una forma simple de usar LSTM es:
</p>

<ul class="org-ul">
<li>considerar una secuencia de longitud 4 (<code>timesteps=4</code>),</li>
<li>con 1 característica por paso (<code>features=1</code>).</li>
</ul>

<p>
Esto implica reestructurar cada muestra de forma (4,) a (4, 1):
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #928374;"># </span><span style="color: #928374;">Reshape para LSTM: (n_muestras, timesteps=4, features=1)
</span><span style="color: #83a598;">X_train_lstm</span> = X_train.reshape<span style="color: #fe8019;">(</span><span style="color: #b16286;">(</span>X_train.shape<span style="color: #b8bb26;">[</span>0<span style="color: #b8bb26;">]</span>, 4, 1<span style="color: #b16286;">)</span><span style="color: #fe8019;">)</span>
<span style="color: #83a598;">X_test_lstm</span> = X_test.reshape<span style="color: #fe8019;">(</span><span style="color: #b16286;">(</span>X_test.shape<span style="color: #b8bb26;">[</span>0<span style="color: #b8bb26;">]</span>, 4, 1<span style="color: #b16286;">)</span><span style="color: #fe8019;">)</span>

<span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>X_train_lstm.shape<span style="color: #fe8019;">)</span>  <span style="color: #928374;"># </span><span style="color: #928374;">(120, 4, 1)
</span><span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>X_test_lstm.shape<span style="color: #fe8019;">)</span>   <span style="color: #928374;"># </span><span style="color: #928374;">(30, 4, 1)</span>
</pre>
</div>

<p>
Además, para clasificación multiclase con Keras, se suele usar
one-hot encoding de las etiquetas:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">num_classes</span> = <span style="color: #fe8019;">len</span><span style="color: #fe8019;">(</span>np.unique<span style="color: #b16286;">(</span>y<span style="color: #b16286;">)</span><span style="color: #fe8019;">)</span>
<span style="color: #83a598;">y_train_cat</span> = to_categorical<span style="color: #fe8019;">(</span>y_train, num_classes=num_classes<span style="color: #fe8019;">)</span>
<span style="color: #83a598;">y_test_cat</span> = to_categorical<span style="color: #fe8019;">(</span>y_test, num_classes=num_classes<span style="color: #fe8019;">)</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-org1d83e72" class="outline-3">
<h3 id="org1d83e72">Definir la red LSTM</h3>
<div class="outline-text-3" id="text-org1d83e72">
<p>
Definimos un modelo secuencial con una capa LSTM y una capa densa de
salida con softmax:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">model</span> = Sequential<span style="color: #fe8019;">(</span><span style="color: #b16286;">[</span>
<span style="background-color: #3c3836;"> </span>   LSTM<span style="color: #b8bb26;">(</span>
<span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   units=16,           <span style="color: #928374;"># </span><span style="color: #928374;">n&#250;mero de unidades LSTM
</span><span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   activation=<span style="color: #b8bb26;">'tanh'</span>,  <span style="color: #928374;"># </span><span style="color: #928374;">activaci&#243;n interna
</span><span style="background-color: #3c3836;"> </span>   <span style="background-color: #3c3836;"> </span>   input_shape=<span style="color: #83a598;">(</span>4, 1<span style="color: #83a598;">)</span>  <span style="color: #928374;"># </span><span style="color: #928374;">(timesteps=4, features=1)
</span><span style="background-color: #3c3836;"> </span>   <span style="color: #b8bb26;">)</span>,
<span style="background-color: #3c3836;"> </span>   Dense<span style="color: #b8bb26;">(</span>num_classes, activation=<span style="color: #b8bb26;">'softmax'</span><span style="color: #b8bb26;">)</span>
<span style="color: #b16286;">]</span><span style="color: #fe8019;">)</span>
</pre>
</div>
</div>
<div id="outline-container-org3b03d1f" class="outline-4">
<h4 id="org3b03d1f">Arquitectura de la red</h4>
<div class="outline-text-4" id="text-org3b03d1f">
<ul class="org-ul">
<li><b>Capa de entrada</b>:
<ul class="org-ul">
<li>Forma: (4, 1)</li>
<li>4 pasos "temporales" (cada uno con 1 valor), derivados de las 4 características.</li>
</ul></li>

<li><b>Capa LSTM</b>:
<ul class="org-ul">
<li><code>units=16</code>: 16 unidades de memoria.</li>
<li>Procesa la secuencia \((x_1, x_2, x_3, x_4)\) y genera una representación que captura
dependencias entre pasos.</li>
</ul></li>

<li><b>Capa de salida (Dense + softmax)</b>:
<ul class="org-ul">
<li>3 neuronas (una por clase).</li>
<li>Activación softmax para obtener probabilidades.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org1d194bf" class="outline-3">
<h3 id="org1d194bf">Compilación del modelo</h3>
<div class="outline-text-3" id="text-org1d194bf">
<p>
Escogemos una función de pérdida apropiada para clasificación
multiclase y un optimizador:
</p>

<div class="org-src-container">
<pre class="src src-python">model.<span style="color: #fe8019;">compile</span><span style="color: #fe8019;">(</span>
<span style="background-color: #3c3836;"> </span>   loss=<span style="color: #b8bb26;">'categorical_crossentropy'</span>,
<span style="background-color: #3c3836;"> </span>   optimizer=tf.keras.optimizers.Adam<span style="color: #b16286;">(</span>learning_rate=0.001<span style="color: #b16286;">)</span>,
<span style="background-color: #3c3836;"> </span>   metrics=<span style="color: #b16286;">[</span><span style="color: #b8bb26;">'accuracy'</span><span style="color: #b16286;">]</span>
<span style="color: #fe8019;">)</span>
</pre>
</div>

<ul class="org-ul">
<li><code>categorical_crossentropy</code>: log-loss multiclase.</li>
<li><b>Adam</b>: optimizador basado en descenso de gradiente con momento y tasas
de aprendizaje adaptativas.</li>
<li>Tasa de aprendizaje inicial \(\eta = 0.001\).</li>
</ul>
</div>
</div>
<div id="outline-container-org930bdb9" class="outline-3">
<h3 id="org930bdb9">Entrenamiento del modelo LSTM</h3>
<div class="outline-text-3" id="text-org930bdb9">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #83a598;">history</span> = model.fit<span style="color: #fe8019;">(</span>
<span style="background-color: #3c3836;"> </span>   X_train_lstm,
<span style="background-color: #3c3836;"> </span>   y_train_cat,
<span style="background-color: #3c3836;"> </span>   epochs=200,         <span style="color: #928374;"># </span><span style="color: #928374;">n&#250;mero de &#233;pocas
</span><span style="background-color: #3c3836;"> </span>   batch_size=16,
<span style="background-color: #3c3836;"> </span>   validation_split=0.1,
<span style="background-color: #3c3836;"> </span>   verbose=1
<span style="color: #fe8019;">)</span>
</pre>
</div>

<p>
Durante el entrenamiento:
</p>
</div>
<div id="outline-container-orgf4cc0ac" class="outline-4">
<h4 id="orgf4cc0ac">Propagación hacia adelante (forward pass)</h4>
<div class="outline-text-4" id="text-orgf4cc0ac">
<ul class="org-ul">
<li>Cada muestra de entrada se considera como una secuencia de 4 pasos.</li>
<li>En cada paso, la LSTM actualiza su:
<ul class="org-ul">
<li>estado oculto \(h_t\),</li>
<li>estado de memoria \(c_t\).</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org5b28343" class="outline-4">
<h4 id="org5b28343">Cálculo del error</h4>
<div class="outline-text-4" id="text-org5b28343">
<ul class="org-ul">
<li>La salida softmax se compara con la etiqueta real codificada en one-hot.</li>
<li>Se calcula la pérdida mediante entropía cruzada categórica.</li>
</ul>
</div>
</div>
<div id="outline-container-org94beedc" class="outline-4">
<h4 id="org94beedc">Backpropagation Through Time (BPTT)</h4>
<div class="outline-text-4" id="text-org94beedc">
<ul class="org-ul">
<li>El error se propaga hacia atrás a lo largo del tiempo (sobre los 4 pasos).</li>
<li>Se calculan los gradientes de los pesos de la LSTM y de la capa de salida.</li>
</ul>
</div>
</div>
<div id="outline-container-org24a3254" class="outline-4">
<h4 id="org24a3254">Descenso de gradiente con Adam</h4>
<div class="outline-text-4" id="text-org24a3254">
<ul class="org-ul">
<li>Adam combina Momentum y RMSProp.</li>
<li>Ajusta automáticamente la tasa de aprendizaje para cada parámetro.</li>
<li>Proporciona convergencia rápida y estable.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org6ffc5fb" class="outline-3">
<h3 id="org6ffc5fb">Evaluación del modelo</h3>
<div class="outline-text-3" id="text-org6ffc5fb">
<p>
Una vez entrenado el modelo, se evalúa su desempeño usando datos no
vistos (conjunto de prueba).
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #928374;"># </span><span style="color: #928374;">Probabilidades por clase
</span><span style="color: #83a598;">y_pred_proba</span> = model.predict<span style="color: #fe8019;">(</span>X_test_lstm<span style="color: #fe8019;">)</span>

<span style="color: #928374;"># </span><span style="color: #928374;">Clase predicha = &#237;ndice de la probabilidad m&#225;xima
</span><span style="color: #83a598;">y_pred</span> = np.argmax<span style="color: #fe8019;">(</span>y_pred_proba, axis=1<span style="color: #fe8019;">)</span>

<span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span><span style="color: #b8bb26;">"Accuracy:"</span>, accuracy_score<span style="color: #b16286;">(</span>y_test, y_pred<span style="color: #b16286;">)</span><span style="color: #fe8019;">)</span>
<span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span><span style="color: #b8bb26;">"</span><span style="color: #d3869b;">\n</span><span style="color: #b8bb26;">Reporte de clasificaci&#243;n:</span><span style="color: #d3869b;">\n</span><span style="color: #b8bb26;">"</span><span style="color: #fe8019;">)</span>
<span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span>classification_report<span style="color: #b16286;">(</span>y_test, y_pred, target_names=iris.target_names<span style="color: #b16286;">)</span><span style="color: #fe8019;">)</span>
</pre>
</div>
</div>
<div id="outline-container-orgd269206" class="outline-4">
<h4 id="orgd269206">Métricas utilizadas</h4>
<div class="outline-text-4" id="text-orgd269206">
<ul class="org-ul">
<li><b>Accuracy</b>: proporción total de predicciones correctas.</li>
<li><b>Precision</b>: qué tan confiables son las predicciones positivas.</li>
<li><b>Recall</b>: capacidad del modelo para encontrar todos los ejemplos de una clase.</li>
<li><b>F1-score</b>: balance entre precisión y recall.</li>
</ul>

<p>
En el dataset Iris, el accuracy también suele estar en el rango del
95 % al 100 %.
</p>
</div>
</div>
</div>
<div id="outline-container-org766bf73" class="outline-3">
<h3 id="org766bf73">Predicción con una nueva flor</h3>
<div class="outline-text-3" id="text-org766bf73">
<p>
El modelo entrenado puede utilizarse para predecir nuevas muestras.
Es importante repetir el mismo preprocesamiento: normalización y
reshape para LSTM.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #928374;"># </span><span style="color: #928374;">[largo_sepalo, ancho_sepalo, largo_petalo, ancho_petalo]
</span><span style="color: #83a598;">flor_nueva</span> = np.array<span style="color: #fe8019;">(</span><span style="color: #b16286;">[</span><span style="color: #b8bb26;">[</span>5.1, 3.5, 1.4, 0.2<span style="color: #b8bb26;">]</span><span style="color: #b16286;">]</span><span style="color: #fe8019;">)</span>

<span style="color: #928374;"># </span><span style="color: #928374;">1) Normalizaci&#243;n con el mismo scaler del entrenamiento
</span><span style="color: #83a598;">flor_nueva_scaled</span> = scaler.transform<span style="color: #fe8019;">(</span>flor_nueva<span style="color: #fe8019;">)</span>

<span style="color: #928374;"># </span><span style="color: #928374;">2) Reshape para LSTM: (1, timesteps=4, features=1)
</span><span style="color: #83a598;">flor_nueva_lstm</span> = flor_nueva_scaled.reshape<span style="color: #fe8019;">(</span><span style="color: #b16286;">(</span>1, 4, 1<span style="color: #b16286;">)</span><span style="color: #fe8019;">)</span>

<span style="color: #928374;"># </span><span style="color: #928374;">3) Predicci&#243;n
</span><span style="color: #83a598;">proba</span> = model.predict<span style="color: #fe8019;">(</span>flor_nueva_lstm<span style="color: #fe8019;">)</span>
<span style="color: #83a598;">prediccion</span> = np.argmax<span style="color: #fe8019;">(</span>proba, axis=1<span style="color: #fe8019;">)</span>

<span style="color: #fe8019;">print</span><span style="color: #fe8019;">(</span><span style="color: #b8bb26;">"Clase predicha:"</span>, iris.target_names<span style="color: #b16286;">[</span>prediccion<span style="color: #b8bb26;">[</span>0<span style="color: #b8bb26;">]</span><span style="color: #b16286;">]</span><span style="color: #fe8019;">)</span>
</pre>
</div>

<p>
Es fundamental aplicar la misma normalización y la misma estructura de
entrada (forma (1, 4, 1)) que se usaron en el entrenamiento.
</p>
</div>
</div>
<div id="outline-container-orgfc975eb" class="outline-3">
<h3 id="orgfc975eb">Interpretación matemática básica de la LSTM</h3>
<div class="outline-text-3" id="text-orgfc975eb">
<p>
En una LSTM, en cada paso temporal \(t\) se procesan:
</p>

<ul class="org-ul">
<li>la entrada \(x_t\),</li>
<li>el estado oculto anterior \(h_{t-1}\),</li>
<li>el estado de memoria anterior \(c_{t-1}\).</li>
</ul>

<p>
La LSTM utiliza <b>puertas</b> que controlan el flujo de información:
</p>

<div class="latex" id="org09effab">
\begin{aligned}
f_t &= \sigma(W_f [h_{t-1}, x_t] + b_f) \\
i_t &= \sigma(W_i [h_{t-1}, x_t] + b_i) \\
\tilde{c}_t &= \tanh(W_c [h_{t-1}, x_t] + b_c) \\
c_t &= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t \\
o_t &= \sigma(W_o [h_{t-1}, x_t] + b_o) \\
h_t &= o_t \odot \tanh(c_t)
\end{aligned}

</div>

<p>
donde:
</p>

<ul class="org-ul">
<li>\(\sigma\) es la función sigmoide,</li>
<li>\(\odot\) denota el producto elemento a elemento.</li>
</ul>

<p>
Al final de la secuencia (tras los 4 pasos), el último estado oculto
\(h_T\) se pasa a la capa densa con función softmax:
</p>

<div class="latex" id="orgd7a28ca">
<p>
\text{softmax}(z<sub>i</sub>) = \frac{e<sup>z<sub>i</sub></sup>}{&sum;<sub>j</sub> e<sup>z<sub>j</sub></sup>}
</p>

</div>

<p>
lo que permite interpretar las salidas como probabilidades para cada
clase (Setosa, Versicolor, Virginica).
</p>
</div>
</div>
</div>
<div id="outline-container-orgf296ad0" class="outline-2">
<h2 id="orgf296ad0">Bibliografía  Deep Learning</h2>
<div class="outline-text-2" id="text-orgf296ad0">
</div>
<div id="outline-container-orgaab7740" class="outline-3">
<h3 id="orgaab7740">Teórica y Académica&#xa0;&#xa0;&#xa0;<span class="tag"><span class="MATEMATICAS">MATEMATICAS</span>&#xa0;<span class="FUNDAMENTOS">FUNDAMENTOS</span></span></h3>
<div class="outline-text-3" id="text-orgaab7740">
<p>
Libros diseñados para entender el "porqué" de los algoritmos.
</p>
</div>
<div id="outline-container-org285668d" class="outline-4">
<h4 id="org285668d">Deep Learning</h4>
<div class="outline-text-4" id="text-org285668d">
<ul class="org-ul">
<li><b>Autores:</b> Ian Goodfellow, Yoshua Bengio y Aaron Courville.</li>
<li><b>Editorial:</b> MIT Press (2016).</li>
<li><b>Nota:</b> Considerada la "Biblia" de la IA. Cubre álgebra lineal, probabilidad y arquitecturas complejas.</li>
<li><b>Web:</b> <a href="https://www.deeplearningbook.org/">Acceso gratuito a la versión web (HTML)</a></li>
</ul>
</div>
</div>
<div id="outline-container-org2dd37ca" class="outline-4">
<h4 id="org2dd37ca">Neural Networks and Deep Learning: A Textbook</h4>
<div class="outline-text-4" id="text-org2dd37ca">
<ul class="org-ul">
<li><b>Autor:</b> Charu C. Aggarwal.</li>
<li><b>Editorial:</b> Springer (2018).</li>
<li><b>Enfoque:</b> Muy estructurado para estudiantes de ingeniería y computación.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org15e7d2b" class="outline-3">
<h3 id="org15e7d2b">Práctica y Programación&#xa0;&#xa0;&#xa0;<span class="tag"><span class="PYTHON">PYTHON</span>&#xa0;<span class="PYTORCH">PYTORCH</span>&#xa0;<span class="KERAS">KERAS</span></span></h3>
<div class="outline-text-3" id="text-org15e7d2b">
<p>
Libros enfocados en la implementación inmediata usando librerías de Python.
</p>
</div>
<div id="outline-container-org97dee16" class="outline-4">
<h4 id="org97dee16">Deep Learning with Python (3rd Edition)</h4>
<div class="outline-text-4" id="text-org97dee16">
<ul class="org-ul">
<li><b>Autor:</b> François Chollet (Creador de Keras).</li>
<li><b>Editorial:</b> Manning Publications (2025).</li>
<li><b>Por qué leerlo:</b> Explicaciones excepcionales sobre la intuición detrás de las redes neuronales.</li>
<li><b>Enlace:</b> <a href="https://www.manning.com/books/deep-learning-with-python-third-edition">Página del libro</a></li>
</ul>
</div>
</div>
<div id="outline-container-orgf0c24bc" class="outline-4">
<h4 id="orgf0c24bc">Hands-On Machine Learning with Scikit-Learn and PyTorch</h4>
<div class="outline-text-4" id="text-orgf0c24bc">
<ul class="org-ul">
<li><b>Autor:</b> Aurélien Géron.</li>
<li><b>Editorial:</b> O'Reilly Media (Edición 2025).</li>
<li><b>Enfoque:</b> Es el manual más completo para pasar de la teoría a la práctica industrial. Cubre desde regresiones simples hasta Transformers.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgc072223" class="outline-3">
<h3 id="orgc072223">Aprendizaje "Desde Cero"&#xa0;&#xa0;&#xa0;<span class="tag"><span class="NUMPY">NUMPY</span>&#xa0;<span class="LOGICA">LOGICA</span></span></h3>
<div class="outline-text-3" id="text-orgc072223">
<p>
Ideales para entender cómo funciona el "motor" de una red neuronal sin usar herramientas automáticas.
</p>
</div>
<div id="outline-container-orgf966900" class="outline-4">
<h4 id="orgf966900">Grokking Deep Learning</h4>
<div class="outline-text-4" id="text-orgf966900">
<ul class="org-ul">
<li><b>Autor:</b> Andrew Trask.</li>
<li><b>Editorial:</b> Manning Publications.</li>
<li><b>Descripción:</b> Aprenderás a programar redes neuronales usando solo <code>NumPy</code>. Ideal si quieres entender la "caja negra".</li>
</ul>
</div>
</div>
<div id="outline-container-org3978b2a" class="outline-4">
<h4 id="org3978b2a">Neural Networks from Scratch (NNFS)</h4>
<div class="outline-text-4" id="text-org3978b2a">
<ul class="org-ul">
<li><b>Autores:</b> Harrison Kinsley (Sentdex) y Daniel Kukieła.</li>
<li><b>Descripción:</b> Basado en código puro de Python. Muy popular entre autodidactas.</li>
<li><b>Web:</b> <a href="https://nnfs.io/">Sitio Oficial de NNFS</a></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org112a6d8" class="outline-3">
<h3 id="org112a6d8">Tabla Comparativa para Elección Rápida</h3>
<div class="outline-text-3" id="text-org112a6d8">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Libro</th>
<th scope="col" class="org-left">Nivel</th>
<th scope="col" class="org-left">Enfoque Principal</th>
<th scope="col" class="org-left">Tecnología</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Goodfellow</td>
<td class="org-left">Avanzado</td>
<td class="org-left">Teoría/Matemáticas</td>
<td class="org-left">Genérico</td>
</tr>

<tr>
<td class="org-left">Chollet</td>
<td class="org-left">Intermedio</td>
<td class="org-left">Intuición</td>
<td class="org-left">Keras/TensorFlow</td>
</tr>

<tr>
<td class="org-left">Géron</td>
<td class="org-left">Todos</td>
<td class="org-left">Práctica/Industria</td>
<td class="org-left">PyTorch/Scikit-Learn</td>
</tr>

<tr>
<td class="org-left">Trask</td>
<td class="org-left">Principiante</td>
<td class="org-left">Lógica Interna</td>
<td class="org-left">NumPy</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Autor: Eduardo Alcaraz</p>
<p class="date">Created: 2026-01-21 mié 08:55</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
